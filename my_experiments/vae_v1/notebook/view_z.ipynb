{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from io import BytesIO\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage\n",
    "\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.models.nets.image.convae import ConvVAE\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from scipy.ndimage import gaussian_gradient_magnitude, laplace\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# seam-ai (parihaka)\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_228917/856989621.py:30: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  return np.array(patches)\n",
      "/tmp/ipykernel_228917/856989621.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(patches)\n"
     ]
    }
   ],
   "source": [
    "def normalize_data(data):\n",
    "    \"\"\"\n",
    "    Normaliza os valores dos pixels para o intervalo [0, 1].\n",
    "    \"\"\"\n",
    "    data_min, data_max = data.min(), data.max()\n",
    "    return (data - data_min) / (data_max - data_min)\n",
    "\n",
    "def extract_patches(data, patch_size=64, stride=32):\n",
    "        patches = []\n",
    "        h, w, _ = data.shape\n",
    "        for i in range(0, h - patch_size + 1, stride):\n",
    "            for j in range(0, w - patch_size + 1, stride):\n",
    "                patch = data[i:i + patch_size, j:j + patch_size]\n",
    "                patch = patch.transpose(2, 0, 1).astype(np.float32)  # Transpõe para (C, H, W)\n",
    "                patch = np.expand_dims(patch, axis=0)  # Adiciona uma dimensão no começo\n",
    "                patch_tensor = torch.from_numpy(patch)  # Converte para tensor PyTorch\n",
    "                patches.append(patch_tensor)\n",
    "        return np.array(patches)\n",
    "\n",
    "train_img_reader = [normalize_data(image) for image in TiffReader(Path(train_path) / \"train\")] # lendo imagens e normalizando\n",
    "# train_label_reader = PNGReader(Path(annotation_path) / \"train\")\n",
    "\n",
    "patches_img = extract_patches(train_img_reader[0])\n",
    "sample_img = patches_img[0]\n",
    "# sample_lab = train_label_reader[0]\n",
    "\n",
    "print(type(sample_img), sample_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvVAE(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (fc_mu): Linear(in_features=4096, out_features=64, bias=True)\n",
       "  (fc_logvar): Linear(in_features=4096, out_features=64, bias=True)\n",
       "  (fc_decode): Linear(in_features=64, out_features=4096, bias=True)\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (7): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"checkpoints/convVAE-sam_model-2024-11-23-epoch=19-val_loss=0.01.ckpt\"\n",
    "model = ConvVAE.load_from_checkpoint(\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    z_size=64\n",
    ").to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(tensor):\n",
    "    tensor = tensor - tensor.min()  # Ajusta para começar em 0\n",
    "    tensor = tensor / tensor.max()  # Ajusta para terminar em 1\n",
    "    return tensor.clamp(0, 1)  # Garante que os valores estejam no intervalo [0, 1]\n",
    "\n",
    "# Função para gerar imagem a partir de z\n",
    "def generate_image(z_values):\n",
    "    z_tensor = torch.tensor([z_values], dtype=torch.float32).to(device)\n",
    "    with torch.no_grad():\n",
    "        reconstructed = model.decode(z_tensor) # decodificar a partir do Z\n",
    "        reconstructed = normalize_image(reconstructed.squeeze(0)) # normalizar\n",
    "        img = ToPILImage()(reconstructed) # converter para PIL\n",
    "    return img\n",
    "\n",
    "def pil_to_bytes(image):\n",
    "    buffer = BytesIO()\n",
    "    image.save(buffer, format=\"PNG\")\n",
    "    return buffer.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar a imagem de entrada\n",
    "original_img_pil = ToPILImage()(sample_img.squeeze(0))  # Converte o tensor original para PIL\n",
    "original_image_bytes = pil_to_bytes(original_img_pil)  # Converte PIL para bytes\n",
    "\n",
    "# Exibir a imagem original à esquerda\n",
    "original_image_display = widgets.Image(value=original_image_bytes, format=\"png\", layout=widgets.Layout(width=\"256px\", height=\"256px\"))\n",
    "\n",
    "# Inicializar sliders com valores iniciais de z\n",
    "mu, logvar = model.encode(sample_img.to(device))  # Obter mu e logvar\n",
    "z_initial = model.reparameterize(mu, logvar).squeeze(0).tolist()  # Amostra inicial de Z\n",
    "\n",
    "latent_dim = model.z_size\n",
    "sliders = [\n",
    "    widgets.FloatSlider(value=z_initial[i], min=-3.0, max=3.0, step=0.1, description=f\"z_{i}\")\n",
    "    for i in range(latent_dim)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7315f27a5c47afb67ecdffae35fde8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00@\\x00\\x00\\x00@\\x08\\x02\\x00\\x00\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Função para atualizar a imagem reconstruída ao alterar os sliders\n",
    "def update_reconstructed_image(*args):\n",
    "    z_values = [slider.value for slider in sliders]\n",
    "    reconstructed_img = generate_image(z_values)\n",
    "    reconstructed_image_display.value = pil_to_bytes(reconstructed_img)\n",
    "\n",
    "# Conectar os sliders à função de atualização\n",
    "for slider in sliders:\n",
    "    slider.observe(update_reconstructed_image, names=\"value\")\n",
    "\n",
    "# Exibir sliders no centro\n",
    "sliders_box = widgets.VBox(sliders)\n",
    "\n",
    "# Exibir a imagem reconstruída à direita\n",
    "reconstructed_image_display = widgets.Image(layout=widgets.Layout(width=\"256px\", height=\"256px\"))\n",
    "\n",
    "# Layout final\n",
    "layout = widgets.HBox([original_image_display, sliders_box, reconstructed_image_display], layout=widgets.Layout(align_items=\"center\"))\n",
    "display(layout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
