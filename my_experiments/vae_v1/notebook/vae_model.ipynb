{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from scipy.ndimage import gaussian_gradient_magnitude, laplace\n",
    "\n",
    "from minerva.data.datasets.supervised_dataset import SupervisedReconstructionDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# seam-ai (parihaka)\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "model_name = \"vae\"\n",
    "height, width = 255, 701 # f3\n",
    "# height, width = 1006, 590 # parihaka\n",
    "num_epochs = 1000\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return self.normalize_data(padded)\n",
    "    \n",
    "    def normalize_data(self, data, target_min=-1, target_max=1):\n",
    "        \"\"\"\n",
    "        Função responsável por normalizar as imagens no intervalo (-1,1)\n",
    "        \"\"\"\n",
    "        data_min, data_max = data.min(), data.max()\n",
    "        return target_min + (data - data_min) * (target_max - target_min) / (data_max - data_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variational encoder model, used as a visual model\n",
    "for our model of the world.\n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\" VAE decoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.fc1 = nn.Linear(latent_size, 14 * 41 * 256)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(32, img_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = x.view(-1, 256, 14, 41)\n",
    "        x = F.relu(self.deconv1(x))\n",
    "        x = F.relu(self.deconv2(x))\n",
    "        x = F.relu(self.deconv3(x))\n",
    "        reconstruction = F.sigmoid(self.deconv4(x))\n",
    "        reconstruction = F.interpolate(reconstruction, size=(255, 701), mode=\"bilinear\", align_corners=False)\n",
    "        return reconstruction\n",
    "\n",
    "class Encoder(nn.Module): # pylint: disable=too-many-instance-attributes\n",
    "    \"\"\" VAE encoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "        #self.img_size = img_size\n",
    "        self.img_channels = img_channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(img_channels, 32, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4, stride=2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 4, stride=2)\n",
    "\n",
    "        self.fc_mu = nn.Linear(14*41*256, latent_size) # 2*2*256\n",
    "        self.fc_logsigma = nn.Linear(14*41*256, latent_size) # 2*2*256\n",
    "\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        mu = self.fc_mu(x)\n",
    "        logsigma = self.fc_logsigma(x)\n",
    "\n",
    "        return mu, logsigma\n",
    "\n",
    "class VAE(L.LightningModule):\n",
    "    \"\"\" Variational Autoencoder \"\"\"\n",
    "    def __init__(self, img_channels, latent_size, lr=1e-3):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(img_channels, latent_size)\n",
    "        self.decoder = Decoder(img_channels, latent_size)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x): # pylint: disable=arguments-differ\n",
    "        mu, logsigma = self.encoder(x)\n",
    "        sigma = logsigma.exp()\n",
    "        eps = torch.randn_like(sigma)\n",
    "        z = eps.mul(sigma).add_(mu)\n",
    "\n",
    "        recon_x = self.decoder(z)\n",
    "        return recon_x, mu, logsigma\n",
    "    \n",
    "    def _loss_function(self, recon_x, x, mu, logsigma):\n",
    "        BCE = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        KLD = -0.5 * torch.sum(1 + 2 * logsigma - mu.pow(2) - (2 * logsigma).exp())\n",
    "        return BCE + KLD\n",
    "    \n",
    "    def _step(self, batch, batch_idx:int, step_name:str):\n",
    "        data = batch[0] # image -> torch.Size([B, C, H, W])\n",
    "        # label = batch[1] # label \n",
    "        recon_batch, mu, logsigma = self.forward(data)\n",
    "        loss = self._loss_function(recon_batch, data, mu, logsigma)\n",
    "        self.log(f\"{step_name}_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx, \"val\")\n",
    "    \n",
    "    def test_step(self, batch: torch.Tensor, batch_idx: int):\n",
    "        return self._step(batch, batch_idx, \"test\")\n",
    "\n",
    "    def predict_step(self, batch: torch.Tensor, batch_idx: int, dataloader_idx: Optional[int] = None):\n",
    "        data = batch\n",
    "        outputs = self.forward(data)\n",
    "        return outputs\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(3, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "            train_dataset = SupervisedReconstructionDataset(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = SupervisedReconstructionDataset(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = SupervisedReconstructionDataset(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying transforms in data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_train = transforms.Compose([\n",
    "#     transforms.ToPILImage(),\n",
    "#     Padding(height, width),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "# ])\n",
    "\n",
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    batch_size=batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches:  248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch_x shape:  torch.Size([4, 3, 255, 701])\n"
     ]
    }
   ],
   "source": [
    "def get_train_dataloader(data_module):\n",
    "    data_module.setup(\"fit\")\n",
    "    return data_module.train_dataloader()\n",
    "\n",
    "print(\"Total batches: \", len(get_train_dataloader(data_module)))\n",
    "\n",
    "train_batch_x, train_batch_y = next(iter(get_train_dataloader(data_module)))\n",
    "print(\"train_batch_x shape: \", train_batch_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/vae_v1/notebook/lightning_logs/version_12\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/vae_v1/notebook/lightning_logs/version_12/run_2024-12-21-17-06-1859a3bb9cbe2342d490e006b98f774adb.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | encoder | Encoder | 151 M  | train\n",
      "1 | decoder | Decoder | 76.1 M | train\n",
      "--------------------------------------------\n",
      "227 M     Trainable params\n",
      "0         Non-trainable params\n",
      "227 M     Total params\n",
      "908.936   Total estimated model params size (MB)\n",
      "13        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 248/248 [00:09<00:00, 26.82it/s, v_num=12, train_loss=5.21e+4, val_loss=5.22e+4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999: 100%|██████████| 248/248 [00:15<00:00, 15.82it/s, v_num=12, train_loss=5.21e+4, val_loss=5.22e+4]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/vae_v1/notebook/lightning_logs/version_12/run_2024-12-21-17-06-1859a3bb9cbe2342d490e006b98f774adb.yaml\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "\n",
    "# current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# # Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "# checkpoint_callback = ModelCheckpoint(\n",
    "#     monitor=\"val_loss\", # Métrica para monitorar\n",
    "#     dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "#     filename=f\"convVAE-sam_model-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "#     save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "#     mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    "# )\n",
    "\n",
    "# from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# # Defina o logger do TensorBoard\n",
    "# logger = TensorBoardLogger(\"logs\", name=\"sam_model\")\n",
    "\n",
    "# from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "# logger = CSVLogger(\"logs\", name=\"conv_vae\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    # logger=logger,\n",
    "    # callbacks=[checkpoint_callback],\n",
    ")\n",
    "# trainer.fit(model, data_module)\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sem normalize = 230 epocas deu val_loss=5.27e+4\n",
    "# com normalize = 230 epocas deu val_loss=5.24e+4\n",
    "# com normalize = 230 epocas e espaco latente 512, deu val_loss="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
