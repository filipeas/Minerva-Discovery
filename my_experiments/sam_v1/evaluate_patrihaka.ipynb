{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial para usar SAMLoRA (modelo do SAM com LoRA)\n",
    "- No Minerva-Dev, mude para a branch \"141-feature-request-add-sam-segment-anything-model-to-minerva\"\n",
    "- Execute o código abaixo que é pra funcionar :v\n",
    "\n",
    "### Modificações nesse notebook\n",
    "- Os caminhos (dataset e weights) tirei o Path, pois uso a pathlib direto nas classes onde precisa\n",
    "- Não uso transforms, então comentei\n",
    "- Não uso ParihakaModule, então comentei\n",
    "- Adicionei meu próprio Dataset e Module\n",
    "- No evaluate model, mudei para pegar corretamente o retorno das masks que o SAM retorna e apliquei o softmax (como sempre fiz)\n",
    "    - OBS: nessa célula comentei a parte que gera prints e plots por 2 motivos: primeiro que eu faço patches, então todas as imagens que aparecem plotadas são de patches e não de seções do volume. Segundo, como os patches são 255x255, há muitas amostras e o processo é demorado, e se eu botar pra plotar tudo trava o vscode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import lightning as L\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from minerva.models.nets.image.segment_anything.sam_lora import SAMLoRA\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform, TransformPipeline\n",
    "from minerva.data.readers.reader import _Reader\n",
    "\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory: logs/seam_ai/SAM-ViT_B\n"
     ]
    }
   ],
   "source": [
    "model_name = \"SAM-ViT_B\"\n",
    "dataset_name = \"seam_ai\"\n",
    "image_dir = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotations_dir = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "image_height = 255\n",
    "image_width = 255\n",
    "image_channels = 1\n",
    "batch_size = 1      # Plase set this to 1\n",
    "num_classes = 6\n",
    "predict_on_partition = \"test\"\n",
    "ckpt_path = \"/workspaces/Minerva-Discovery/my_experiments/sam_v1/checkpoints/final_train-raio-1.0-2024-11-27-epoch=11-val_loss=0.41.ckpt\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print_crosslines_every = 50\n",
    "output_dir = Path(f\"./logs/{dataset_name}/{model_name}/\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(\n",
    "    images,\n",
    "    plot_title=None,\n",
    "    subplot_titles=None,\n",
    "    cmaps=None,\n",
    "    filename=None,\n",
    "    x_label=None,\n",
    "    y_label=None,\n",
    "    height=5,\n",
    "    width=5,\n",
    "    show=False\n",
    "):\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Create a figure with subplots (1 row, num_images columns), adjusting size based on height and width parameters\n",
    "    fig, axs = plt.subplots(1, num_images, figsize=(width * num_images, height))\n",
    "\n",
    "    # Set overall plot title if provided\n",
    "    if plot_title is not None:\n",
    "        fig.suptitle(plot_title, fontsize=16)\n",
    "\n",
    "    # Ensure subplot_titles and cmaps are lists with correct lengths\n",
    "    if subplot_titles is None:\n",
    "        subplot_titles = [None] * num_images\n",
    "    if cmaps is None:\n",
    "        cmaps = [\"gray\"] * num_images\n",
    "\n",
    "    # Plot each image in its respective subplot\n",
    "    for i, (img, ax, title, cmap) in enumerate(\n",
    "        zip(images, axs, subplot_titles, cmaps)\n",
    "    ):\n",
    "        im = ax.imshow(img, cmap=cmap)\n",
    "\n",
    "        # Set title for each subplot if provided\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "\n",
    "        # Add a colorbar for each subplot\n",
    "        fig.colorbar(im, ax=ax)\n",
    "\n",
    "        # Set x and y labels if provided\n",
    "        if x_label:\n",
    "            ax.set_xlabel(x_label)\n",
    "        if y_label:\n",
    "            ax.set_ylabel(y_label)\n",
    "\n",
    "    # Adjust layout to fit titles, labels, and colorbars\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure if filename is provided\n",
    "    if filename is not None:\n",
    "        plt.savefig(filename, bbox_inches=\"tight\")\n",
    "        print(f\"Figure saved as '{filename}'\")\n",
    "\n",
    "    # Show the plot\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "def hash_file(filepath):\n",
    "    \"\"\"Generate a hash for a file.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with filepath.open('rb') as file:\n",
    "        while chunk := file.read(8192):  # Read in 8 KB chunks\n",
    "            hasher.update(chunk)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def hash_folder(folder_path):\n",
    "    \"\"\"Generate a hash for a folder by hashing its files and structure.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    folder = Path(folder_path)\n",
    "    \n",
    "    files = list(sorted(folder.rglob('*')))  # Get all files and directories\n",
    "    \n",
    "    for path in tqdm.tqdm(files, desc=\"Hashing files...\"):  # Recursively iterate over all files and directories\n",
    "        if path.is_file():\n",
    "            hasher.update(hash_file(path).encode('utf-8'))  # Hash file content\n",
    "        hasher.update(str(path.relative_to(folder)).encode('utf-8'))  # Hash relative path for structure\n",
    "    \n",
    "    return hasher.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hashing files...: 100%|██████████| 1375/1375 [00:06<00:00, 223.38it/s]\n",
      "Hashing files...: 100%|██████████| 1375/1375 [00:00<00:00, 1675.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image folder hash: 9799486b and Annotations folder hash: 2566b002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_folder_hash = hash_folder(image_dir)\n",
    "annotations_folder_hash = hash_folder(annotations_dir)\n",
    "print(f\"Image folder hash: {image_folder_hash[:8]} and Annotations folder hash: {annotations_folder_hash[:8]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms (NÃO USADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PadCrop(_Transform):\n",
    "#     \"\"\"Transforms image and pads or crops it to the target size.\n",
    "#     If the axis is larger than the target size, it will crop the image.\n",
    "#     If the axis is smaller than the target size, it will pad the image.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         target_h_size: int,\n",
    "#         target_w_size: int,\n",
    "#         padding_mode: str = \"reflect\",\n",
    "#         seed: int | None = None,\n",
    "#         constant_values: int = 0,\n",
    "#     ):\n",
    "#         \"\"\"\n",
    "#         Initializes the transformation with target sizes, padding mode, and RNG seed.\n",
    "\n",
    "#         Parameters:\n",
    "#         - target_h_size (int): The target height size.\n",
    "#         - target_w_size (int): The target width size.\n",
    "#         - padding_mode (str): The padding mode to use (default is \"reflect\").\n",
    "#         - seed (int): Seed for random number generator to make cropping reproducible.\n",
    "#         \"\"\"\n",
    "#         self.target_h_size = target_h_size\n",
    "#         self.target_w_size = target_w_size\n",
    "#         self.padding_mode = padding_mode\n",
    "#         self.rng = np.random.default_rng(\n",
    "#             seed\n",
    "#         )  # Random number generator with the provided seed\n",
    "#         self.constant_values = constant_values\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         h, w = x.shape[:2]\n",
    "#         # print(f\"-> [{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "\n",
    "#         # Handle height dimension independently: pad if target_h_size > h, else crop\n",
    "#         if self.target_h_size > h:\n",
    "#             pad_h = self.target_h_size - h\n",
    "#             pad_top = pad_h // 2\n",
    "#             pad_bottom = pad_h - pad_top\n",
    "#             pad_args = {\n",
    "#                 \"array\": x,\n",
    "#                 \"pad_width\": (\n",
    "#                     ((pad_top, pad_bottom), (0, 0), (0, 0))\n",
    "#                     if len(x.shape) == 3\n",
    "#                     else ((pad_top, pad_bottom), (0, 0))\n",
    "#                 ),\n",
    "#                 \"mode\": self.padding_mode,\n",
    "#             }\n",
    "#             if self.padding_mode == \"constant\":\n",
    "#                 pad_args[\"constant_values\"] = self.constant_values\n",
    "\n",
    "#             x = np.pad(**pad_args)\n",
    "\n",
    "#         elif self.target_h_size < h:\n",
    "#             crop_h_start = self.rng.integers(0, h - self.target_h_size + 1)\n",
    "#             x = x[crop_h_start : crop_h_start + self.target_h_size, ...]\n",
    "\n",
    "#         # Handle width dimension independently: pad if target_w_size > w, else crop\n",
    "#         if self.target_w_size > w:\n",
    "#             pad_w = self.target_w_size - w\n",
    "#             pad_left = pad_w // 2\n",
    "#             pad_right = pad_w - pad_left\n",
    "\n",
    "#             pad_args = {\n",
    "#                 \"array\": x,\n",
    "#                 \"pad_width\": (\n",
    "#                     ((0, 0), (pad_left, pad_right), (0, 0))\n",
    "#                     if len(x.shape) == 3\n",
    "#                     else ((0, 0), (pad_left, pad_right))\n",
    "#                 ),\n",
    "#                 \"mode\": self.padding_mode,\n",
    "#             }\n",
    "\n",
    "#             if self.padding_mode == \"constant\":\n",
    "#                 pad_args[\"constant_values\"] = self.constant_values\n",
    "\n",
    "#             x = np.pad(**pad_args)\n",
    "\n",
    "#         elif self.target_w_size < w:\n",
    "#             crop_w_start = self.rng.integers(0, w - self.target_w_size + 1)\n",
    "#             x = x[:, crop_w_start : crop_w_start + self.target_w_size, ...]\n",
    "\n",
    "#         # Ensure channel dimension consistency\n",
    "#         if len(x.shape) == 2:  # For grayscale, add a channel dimension\n",
    "#             x = np.expand_dims(x, axis=2)\n",
    "\n",
    "#         # Convert to torch tensor with format C x H x W\n",
    "#         # output = torch.from_numpy(x).float()\n",
    "#         x = np.transpose(x, (2, 0, 1))  # Convert to C x H x W format\n",
    "#         # print(f\"[{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         # print(f\"<- [{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         return x\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(target_h_size={self.target_h_size}, target_w_size={self.target_w_size})\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)\n",
    "\n",
    "\n",
    "# class SelectChannel(_Transform):\n",
    "#     \"\"\"Perform a channel selection on the input image.\"\"\"\n",
    "\n",
    "#     def __init__(self, channel: int, expand_channels: int = None):\n",
    "#         \"\"\"\n",
    "#         Initializes the transformation with the channel to select.\n",
    "\n",
    "#         Parameters:\n",
    "#         - channel (int): The channel to select.\n",
    "#         \"\"\"\n",
    "#         self.channel = channel\n",
    "#         self.expand_channels = expand_channels\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         x = x[self.channel, ...]\n",
    "#         if self.expand_channels is not None:\n",
    "#             x = np.expand_dims(x, axis=self.expand_channels)\n",
    "#         # print(f\"[{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         return x\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(channel={self.channel})\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)\n",
    "\n",
    "\n",
    "# class CastTo(_Transform):\n",
    "#     def __init__(self, dtype: type):\n",
    "#         \"\"\"\n",
    "#         Initializes the transformation with the target data type.\n",
    "\n",
    "#         Parameters:\n",
    "#         - dtype (type): The target data type.\n",
    "#         \"\"\"\n",
    "#         self.dtype = dtype\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         # print(f\"[{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         return x.astype(self.dtype)\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(dtype={self.dtype})\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)\n",
    "\n",
    "\n",
    "# class SwapAxes(_Transform):\n",
    "#     def __init__(self, source_axis: int, target_axis: int):\n",
    "#         \"\"\"\n",
    "#         Initializes the transformation with the source and target axes.\n",
    "\n",
    "#         Parameters:\n",
    "#         - source_axis (int): The source axis to swap.\n",
    "#         - target_axis (int): The target axis to swap.\n",
    "#         \"\"\"\n",
    "#         self.source_axis = source_axis\n",
    "#         self.target_axis = target_axis\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         x = np.swapaxes(x, self.source_axis, self.target_axis)\n",
    "#         # print(f\"[{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         return x\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(source_axis={self.source_axis}, target_axis={self.target_axis})\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)\n",
    "\n",
    "\n",
    "# class RepeatChannel(_Transform):\n",
    "#     def __init__(self, repeats: int, axis: int):\n",
    "#         \"\"\"\n",
    "#         Initializes the transformation with the number of repeats.\n",
    "\n",
    "#         Parameters:\n",
    "#         - repeats (int): The number of repeats.\n",
    "#         - axis (int): The axis to repeat.\n",
    "#         \"\"\"\n",
    "#         self.repeats = repeats\n",
    "#         self.axis = axis\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         x = np.repeat(x, self.repeats, axis=self.axis)\n",
    "#         # print(f\"[{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         return x\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(repeats={self.repeats}, axis={self.axis})\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)\n",
    "\n",
    "\n",
    "# class ExpandDims(_Transform):\n",
    "#     def __init__(self, axis: int):\n",
    "#         \"\"\"\n",
    "#         Initializes the transformation with the axis to expand.\n",
    "\n",
    "#         Parameters:\n",
    "#         - axis (int): The axis to expand.\n",
    "#         \"\"\"\n",
    "#         self.axis = axis\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         x = np.expand_dims(x, axis=self.axis)\n",
    "#         # print(f\"[{self.__class__.__name__}] x.shape={x.shape}\")\n",
    "#         return x\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"{self.__class__.__name__}(axis={self.axis})\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset (Filipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedDatasetPatches(SimpleDataset):\n",
    "    def __init__(self, readers: List[_Reader], transforms: Optional[_Transform] = None, patch_size: int = 255, stride: int = 32):\n",
    "        \"\"\"Adds support for splitting images into patches.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        readers: List[_Reader]\n",
    "            List of data readers. It must contain exactly 2 readers.\n",
    "            The first reader for the input data and the second reader for the\n",
    "            target data.\n",
    "        transforms: Optional[_Transform]\n",
    "            Optional data transformation pipeline.\n",
    "        patch_size: int\n",
    "            Size of the patches into which the images will be divided.\n",
    "        stride: int\n",
    "            Stride used to extract patches from images.\n",
    "        Raises\n",
    "        -------\n",
    "            AssertionError: If the number of readers is not exactly 2.\n",
    "        \"\"\"\n",
    "        super().__init__(readers, transforms)\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self._patch_indices = []\n",
    "        self._precompute_patch_indices()\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"SupervisedReconstructionDataset requires exactly 2 readers\"\n",
    "    \n",
    "    def _precompute_patch_indices(self):\n",
    "        \"\"\"Precomputes patch indices for all images.\"\"\"\n",
    "        for img_idx in range(len(self.readers[0])):\n",
    "            # Obtem a dimensão da imagem para calcular o número de patches\n",
    "            image = self.readers[0][img_idx]\n",
    "            h, w = image.shape[:2]\n",
    "            num_patches_h = (h - self.patch_size) // self.stride + 1\n",
    "            num_patches_w = (w - self.patch_size) // self.stride + 1\n",
    "            for patch_idx in range(num_patches_h * num_patches_w):\n",
    "                self._patch_indices.append((img_idx, patch_idx))\n",
    "    \n",
    "    def _extract_single_patch(self, data, patch_idx, patch_size=255, stride=32, img_type='image'):\n",
    "        if img_type == 'image': # caso seja imagens de entrada (h, w, c)\n",
    "            h, w, _ = data.shape\n",
    "        else: # caso seja labels de entrada (h, w)\n",
    "            h, w = data.shape\n",
    "        num_patches_w = (w - patch_size) // stride + 1\n",
    "        row = patch_idx // num_patches_w # numero da linha do patch\n",
    "        col = patch_idx % num_patches_w # numero da coluna do patch\n",
    "        i, j = row * stride, col * stride # coordenada do patch no grid\n",
    "        patch = data[i:i + patch_size, j:j + patch_size]\n",
    "        if img_type == 'image':\n",
    "            return patch.transpose(2, 0, 1).astype(np.float32) # (C H W)\n",
    "        else:\n",
    "            return patch.astype(np.int64)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of patches.\"\"\"\n",
    "        return len(self._patch_indices)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load data and return a single patch.\"\"\"\n",
    "        img_idx, patch_idx = self._patch_indices[index]\n",
    "        input_data = self.readers[0][img_idx]\n",
    "        target_data = self.readers[1][img_idx]\n",
    "\n",
    "        input_patch = self._extract_single_patch(input_data, patch_idx, img_type='image')\n",
    "        target_patch = self._extract_single_patch(target_data, patch_idx, img_type='label')\n",
    "        return input_patch, target_patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parihaka DataModule (Filipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchingModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        patch_size: int = 255,\n",
    "        stride: int = 32,\n",
    "        batch_size: int = 8,\n",
    "        transforms: _Transform = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.num_workers = num_workers if num_workers else os.cpu_count()\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    # função útil\n",
    "    def normalize_data(self, data, target_min=-1, target_max=1):\n",
    "        \"\"\"Function responsible for normalizing images in the range (-1,1)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : np.ndarray\n",
    "            Sample (image), with 3 channels\n",
    "        target_min : int\n",
    "            Min value of target to normalize data.\n",
    "        target_max : int\n",
    "            Max value of target to normalize data.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Sample (image) normalized.\n",
    "        \"\"\"\n",
    "        data_min, data_max = data.min(), data.max()\n",
    "        return target_min + (data - data_min) * (target_max - target_min) / (data_max - data_min)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = [self.normalize_data(image) for image in TiffReader(self.train_path / \"train\")] # lendo imagens e normalizando\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "            \n",
    "            # Criar dataset para treinamento\n",
    "            self.datasets['train'] = SupervisedDatasetPatches(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                patch_size=self.patch_size,\n",
    "                stride=self.stride\n",
    "            )\n",
    "            del train_img_reader, train_label_reader\n",
    "            gc.collect()\n",
    "\n",
    "            val_img_reader = [self.normalize_data(image) for image in TiffReader(self.train_path / \"val\")]\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "\n",
    "            self.datasets[\"val\"] = SupervisedDatasetPatches(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                patch_size=self.patch_size,\n",
    "                stride=self.stride\n",
    "            )\n",
    "            del val_img_reader, val_label_reader\n",
    "            gc.collect()\n",
    "        \n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = [self.normalize_data(image) for image in TiffReader(self.train_path / \"test\")]\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "\n",
    "            test_dataset = SupervisedDatasetPatches(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                patch_size=self.patch_size,\n",
    "                stride=self.stride\n",
    "            )\n",
    "            del test_img_reader, test_label_reader\n",
    "            gc.collect()\n",
    "\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True, \n",
    "            drop_last=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True, \n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True, \n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True, \n",
    "            drop_last=False\n",
    "        )\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"\"\"DataModule\n",
    "        Data: {self.train_path}\n",
    "        Annotations: {self.annotations_path}\n",
    "        Batch size: {self.batch_size}\"\"\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parihaka DataModule Definition (NÃO USADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GenericParihakaDataModule(L.LightningDataModule):\n",
    "#     class Identity(_Transform):\n",
    "#         def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#             return x\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         root_data_dir: str,\n",
    "#         root_annotation_dir: str,\n",
    "#         image_transforms: TransformPipeline,\n",
    "#         label_transforms: TransformPipeline,\n",
    "#         batch_size: int = 1,\n",
    "#         num_workers: Optional[int] = None,\n",
    "#         predict_on: str = \"test\",\n",
    "#     ):\n",
    "#         assert predict_on in [\"test\", \"val\", \"train\"]\n",
    "        \n",
    "#         super().__init__()\n",
    "#         self.root_data_dir = Path(root_data_dir)\n",
    "#         self.root_annotation_dir = Path(root_annotation_dir)\n",
    "#         self.image_transforms = image_transforms or self.Identity()\n",
    "#         self.label_transforms = label_transforms or self.Identity()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = (\n",
    "#             num_workers if num_workers is not None else os.cpu_count()\n",
    "#         )\n",
    "#         self.predict_on = predict_on\n",
    "#         self.datasets = {}\n",
    "\n",
    "#     def _create_dataset(self, partition: str):\n",
    "#         img_reader = TiffReader(str(self.root_data_dir / partition))\n",
    "#         label_reader = PNGReader(str(self.root_annotation_dir / partition))\n",
    "#         return SimpleDataset(\n",
    "#             readers=[img_reader, label_reader],\n",
    "#             transforms=[self.image_transforms, self.label_transforms],\n",
    "#         )\n",
    "        \n",
    "#     def _get_dataloader(self, partition: str, shuffle: bool):\n",
    "#         return DataLoader(\n",
    "#             self.datasets[partition],\n",
    "#             batch_size=self.batch_size,\n",
    "#             num_workers=self.num_workers, # type: ignore\n",
    "#             shuffle=shuffle,\n",
    "#         )\n",
    "\n",
    "#     def setup(self, stage=None):        \n",
    "#         if stage == \"fit\":\n",
    "#             self.datasets[\"train\"] = self._create_dataset(\"train\")\n",
    "#             self.datasets[\"val\"] = self._create_dataset(\"val\")\n",
    "#         elif stage == \"test\":\n",
    "#             self.datasets[\"test\"] = self._create_dataset(\"test\")\n",
    "#         elif stage == \"predict\":\n",
    "#             self.datasets[\"predict\"] = self._create_dataset(self.predict_on)\n",
    "#         else:\n",
    "#             raise ValueError(f\"Invalid stage: {stage}\")\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return self._get_dataloader(\"train\", shuffle=True)\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return self._get_dataloader(\"val\", shuffle=False)\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return self._get_dataloader(\"test\", shuffle=False)\n",
    "\n",
    "#     def predict_dataloader(self):\n",
    "#         return self._get_dataloader(\"predict\", shuffle=False)\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return f\"\"\"DataModule\n",
    "#     Data: {self.root_data_dir}\n",
    "#     Annotations: {self.root_annotation_dir}\n",
    "#     Batch size: {self.batch_size}\"\"\"\n",
    "\n",
    "#     def __repr__(self) -> str:\n",
    "#         return str(self)\n",
    "    \n",
    "\n",
    "# Helper functions (if needed)\n",
    "def get_train_dataloader(data_module):\n",
    "    data_module.setup(\"fit\")\n",
    "    return data_module.train_dataloader()\n",
    "\n",
    "def get_val_dataloader(data_module):\n",
    "    data_module.setup(\"fit\")\n",
    "    return data_module.val_dataloader()\n",
    "\n",
    "def get_test_dataloader(data_module):\n",
    "    data_module.setup(\"test\")\n",
    "    return data_module.test_dataloader()\n",
    "\n",
    "def get_predict_dataloader(data_module):\n",
    "    data_module.setup(\"predict\")\n",
    "    return data_module.predict_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms instantiation (NÃO USADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_transforms = []\n",
    "# image_transforms.append(SwapAxes(0, -1))\n",
    "# image_transforms.append(SelectChannel(0))\n",
    "# image_transforms.append(SwapAxes(0, 1))\n",
    "# image_transforms.append(PadCrop(image_height, image_width, padding_mode=\"reflect\", seed=42))\n",
    "# if image_channels > 1:\n",
    "#     image_transforms.append(RepeatChannel(image_channels, axis=0))\n",
    "# image_transforms.append(CastTo(np.float32))\n",
    "\n",
    "\n",
    "# label_transforms = []\n",
    "# label_transforms.append(PadCrop(image_height, image_width, padding_mode=\"reflect\", seed=42))\n",
    "# label_transforms.append(CastTo(np.float32))\n",
    "\n",
    "# print(f\"Image transforms: {image_transforms}\")\n",
    "# print(f\"Label transforms: {label_transforms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataModule\n",
       "        Data: /workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\n",
       "        Annotations: /workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\n",
       "        Batch size: 1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_module = GenericParihakaDataModule(\n",
    "#     root_data_dir=str(image_dir),\n",
    "#     root_annotation_dir=str(annotations_dir),\n",
    "#     image_transforms=TransformPipeline(image_transforms),\n",
    "#     label_transforms=TransformPipeline(label_transforms),\n",
    "#     batch_size=batch_size,\n",
    "# )\n",
    "\n",
    "data_module = PatchingModule(\n",
    "    train_path=image_dir,\n",
    "    annotations_path=annotations_dir,\n",
    "    patch_size=image_height,\n",
    "    stride=32,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch X shape: torch.Size([1, 3, 255, 255])\n",
      "Train batch Y shape: torch.Size([1, 255, 255])\n"
     ]
    }
   ],
   "source": [
    "train_batch_x, train_batch_y = next(iter(get_train_dataloader(data_module)))\n",
    "print(f\"Train batch X shape: {train_batch_x.shape}\")\n",
    "print(f\"Train batch Y shape: {train_batch_y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Batch (de tamanho 1) possui: 3 canais, 255 altura e 255 largura.\n"
     ]
    }
   ],
   "source": [
    "print(f\"O Batch (de tamanho {train_batch_x.shape[0]}) possui: {train_batch_x.shape[1]} canais, {train_batch_x.shape[2]} altura e {train_batch_x.shape[3]} largura.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and load model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal loss alpha=0.25, will shrink the impact in background\n"
     ]
    }
   ],
   "source": [
    "# class DummyModel(torch.nn.Module):\n",
    "#     def __init__(self, num_classes: int):\n",
    "#         super().__init__()\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         y_hat = torch.randint_like(x, 0, self.num_classes)\n",
    "#         y_hat = torch.randn(x.shape[0], self.num_classes, x.shape[2], x.shape[3], device=x.device)\n",
    "#         return y_hat\n",
    "\n",
    "def get_model(ckpt_path) -> torch.nn.Module:\n",
    "    \"\"\"Create and load a model from a checkpoint.\"\"\"\n",
    "    # ckpt = torch.load(ckpt_path)\n",
    "    # return DummyModel(num_classes)\n",
    "    return SAMLoRA.load_from_checkpoint(\n",
    "        checkpoint_path=ckpt_path,\n",
    "        image_size=image_height,\n",
    "        num_classes=num_classes-1, # considera 6 pois internamente o sam faz +1 pro background\n",
    "        alpha=1,\n",
    "        rank=4,\n",
    "    )\n",
    "\n",
    "model = get_model(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 52800it [14:44, 59.72it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module.setup(\"test\")\n",
    "\n",
    "miou_metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes).to(\n",
    "    \"cuda\"\n",
    ")\n",
    "metric_values = []\n",
    "\n",
    "model.eval()\n",
    "model = model.to(\"cuda\")\n",
    "\n",
    "curent_index = 0\n",
    "\n",
    "for batch_idx, (batch_x, batch_y) in tqdm.tqdm(\n",
    "    enumerate(data_module.test_dataloader()),\n",
    "    desc=\"Testing\",\n",
    "    leave=True,\n",
    "):\n",
    "    batch_x, batch_y = batch_x.to(\"cuda\"), batch_y.to(\"cuda\")\n",
    "    batch_y_hat = model.forward(batch_x, multimask_output=True, image_size=model.image_size)\n",
    "    probs = torch.softmax(batch_y_hat['masks'], dim=1)\n",
    "    batch_y_hat = torch.argmax(probs, dim=1).squeeze(1)\n",
    "    # print(batch_x.shape, batch_y.shape, batch_y_hat.shape)\n",
    "\n",
    "    for i, (x, y, y_hat) in enumerate(zip(batch_x, batch_y, batch_y_hat)):\n",
    "        # print(i, x.shape, y.shape, y_hat.shape)\n",
    "        curent_index += 1\n",
    "        res = miou_metric(\n",
    "            y_hat.unsqueeze(0), y.unsqueeze(0)\n",
    "        ).item()  # re-add batch dimension (unsqueeze(0))\n",
    "        metric_values.append(res)\n",
    "\n",
    "        # if curent_index % print_crosslines_every == 0:\n",
    "        #     x = x.permute(1, 2, 0).squeeze(0).cpu().numpy()\n",
    "        #     y = y.squeeze(0).cpu().numpy()\n",
    "        #     y_hat = y_hat.squeeze(0).cpu().numpy()\n",
    "        #     diff = (y != y_hat).astype(np.int32)\n",
    "            # print(f\"Crossline {curent_index} MIOU: {res}\")\n",
    "            # print(\n",
    "            #     f\"x.shape={x.shape}, y.shape={y.shape}, y_hat.shape={y_hat.shape}, diff.shape={diff.shape}\"\n",
    "            # )\n",
    "\n",
    "            # plot_images(\n",
    "            #     images=[x, y, y_hat, diff],\n",
    "            #     subplot_titles=[\n",
    "            #         \"Input\",\n",
    "            #         \"Ground Truth\",\n",
    "            #         \"Prediction\",\n",
    "            #         \"Difference\",\n",
    "            #     ],\n",
    "            #     cmaps=[\"seismic\", \"Accent\", \"Accent\", \"gray\"],\n",
    "            #     plot_title=f\"{model_name} Parihaka Segmentation (crossline {curent_index}). MIOU={res:.3f}\",\n",
    "            #     filename=f\"{output_dir}/segmentation_{curent_index}.png\",\n",
    "            #     show=True,\n",
    "            # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean IoU: 0.5579\n"
     ]
    }
   ],
   "source": [
    "mean_iou = np.mean(metric_values)\n",
    "print(f\"Mean IoU: {mean_iou:.4f}\")\n",
    "with open(f\"{output_dir}/mean_iou.txt\", \"w\") as f:\n",
    "    f.write(f\"{mean_iou:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
