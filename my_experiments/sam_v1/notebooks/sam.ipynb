{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:54: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from minerva.data.datasets.supervised_dataset import SupervisedReconstructionDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.models.nets.image.segment_anything.sam_lora import SAMLoRA\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f13109804d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "# train_path = \"/workspaces/Minerva-Dev-Container/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Dev-Container/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# seam-ai (parihaka)\n",
    "train_path = \"/workspaces/Minerva-Dev-Container/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Dev-Container/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "checkpoint_path = \"/workspaces/Minerva-Dev-Container/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2826603/3997028444.py:51: DeprecationWarning: Please use `zoom` from the `scipy.ndimage` namespace, the `scipy.ndimage.interpolation` namespace is deprecated.\n",
      "  from scipy.ndimage.interpolation import zoom\n"
     ]
    }
   ],
   "source": [
    "# class Padding(_Transform):\n",
    "#     def __init__(self, target_h_size: int, target_w_size: int):\n",
    "#         self.target_h_size = target_h_size\n",
    "#         self.target_w_size = target_w_size\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         h, w = x.shape[:2]\n",
    "#         pad_h = max(0, self.target_h_size - h)\n",
    "#         pad_w = max(0, self.target_w_size - w)\n",
    "#         if len(x.shape) == 2:\n",
    "#             padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "#             padded = np.expand_dims(padded, axis=2)\n",
    "#             padded = torch.from_numpy(padded).float()\n",
    "#         else:\n",
    "#             padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "#             padded = torch.from_numpy(padded).float()\n",
    "\n",
    "#         padded = np.transpose(padded, (2, 0, 1))\n",
    "#         return padded\n",
    "\n",
    "# class Padding(_Transform):\n",
    "#     def __init__(self, target_h_size: int, target_w_size: int):\n",
    "#         self.target_h_size = target_h_size\n",
    "#         self.target_w_size = target_w_size\n",
    "\n",
    "#     def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "#         h, w = x.shape[:2]\n",
    "#         pad_h = max(0, self.target_h_size - h)\n",
    "#         pad_w = max(0, self.target_w_size - w)\n",
    "\n",
    "#         # Pad the image\n",
    "#         if len(x.shape) == 2:  # 2D image (grayscale)\n",
    "#             padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "#             padded = np.expand_dims(padded, axis=2)  # Add channel dimension\n",
    "#         else:  # 3D image (color)\n",
    "#             padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "\n",
    "#         # Convert to tensor and ensure shape is (C, H, W)\n",
    "#         padded = torch.from_numpy(padded).float()\n",
    "\n",
    "#         # Transpose to (C, H, W) format\n",
    "#         padded = padded.permute(2, 0, 1) if len(padded.shape) == 3 else padded.unsqueeze(0)\n",
    "\n",
    "#         # Ensure output size is (C, target_h_size, target_w_size)\n",
    "#         padded = F.pad(padded, (0, self.target_w_size - padded.shape[2], 0, self.target_h_size - padded.shape[1]), \"reflect\")\n",
    "\n",
    "#         return padded\n",
    "\n",
    "# transform = Padding(512, 512)\n",
    "\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "class RandomGeneratorForImage(_Transform):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def __call__(self, sample: np.ndarray) -> np.ndarray:\n",
    "        channels, x, y = sample.shape\n",
    "        \n",
    "        # Redimensiona cada canal individualmente para evitar o erro de dimensão\n",
    "        resized_channels = [\n",
    "            zoom(sample[c], (self.output_size[0] / x, self.output_size[1] / y), order=3)\n",
    "            for c in range(channels)\n",
    "        ]\n",
    "        \n",
    "        # Converte a lista de canais redimensionados de volta para um array numpy e, em seguida, para um tensor PyTorch\n",
    "        image = np.stack(resized_channels, axis=0).astype(np.float32)\n",
    "        image = torch.from_numpy(image)\n",
    "        return image\n",
    "\n",
    "class RandomGeneratorForLabel(_Transform):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "        # self.low_res = low_res\n",
    "\n",
    "    def __call__(self, sample: np.ndarray) -> np.ndarray:\n",
    "        label = sample\n",
    "\n",
    "        x, y = label.shape\n",
    "        if x != self.output_size[0] or y != self.output_size[1]:\n",
    "            label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)\n",
    "        label = torch.from_numpy(label.astype(np.float32))\n",
    "        return label.long()\n",
    "\n",
    "transformImage = RandomGeneratorForImage((512, 512))\n",
    "transformLabel = RandomGeneratorForLabel((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from patchify import patchify\n",
    "\n",
    "def rotate_and_flip_patch(patch_img, patch_label):\n",
    "    rotate = cv2.ROTATE_90_CLOCKWISE\n",
    "    # Rotaciona -90 graus (equivalente a transpor e depois inverter verticalmente)\n",
    "    patch_img_rotated = cv2.rotate(patch_img, rotate)\n",
    "    patch_label_rotated = cv2.rotate(patch_label, rotate)\n",
    "    # Espelha horizontalmente\n",
    "    patch_img_flipped = cv2.flip(patch_img_rotated, 1)\n",
    "    patch_label_flipped = cv2.flip(patch_label_rotated, 1)\n",
    "    return patch_img_flipped, patch_label_flipped\n",
    "\n",
    "def resize_image(image, target_shape):\n",
    "    return cv2.resize(image, target_shape, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def cria_patchs_sismicas(images, labels, patch_size=(512,512), step=512):\n",
    "    all_img_patches = []\n",
    "    all_label_patches = []\n",
    "    for i, large_image in enumerate(images): # iterando as imagens\n",
    "        # plt.figure(figsize=(8, 8))\n",
    "        # plt.imshow(cv2.cvtColor(np.array(large_image), cv2.COLOR_BGR2RGB))  # Convertendo BGR para RGB para visualização correta\n",
    "        # plt.title(\"large_image\")\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        # Garantir que a imagem está deitada\n",
    "        # if large_image.shape[0] > large_image.shape[1]:\n",
    "        #     large_image = cv2.rotate(np.array(large_image), cv2.ROTATE_90_CLOCKWISE)\n",
    "        #     label = cv2.rotate(np.array(labels[i]), cv2.ROTATE_90_CLOCKWISE)\n",
    "        \n",
    "        target_shape = ((large_image.shape[1] // patch_size[1] + 1) * patch_size[1],\n",
    "                        (large_image.shape[0] // patch_size[0] + 1) * patch_size[0])\n",
    "        \n",
    "        large_image_resized = resize_image(np.array(large_image), target_shape)\n",
    "        # print(\"large_image shape:\", large_image.shape)\n",
    "        # print(\"large_image type:\", type(large_image))\n",
    "        # print(\"Target shape:\", target_shape)\n",
    "        # print(\"large_image_resized shape:\", large_image_resized.shape)\n",
    "        # print(\"large_image_resized type:\", type(large_image_resized))\n",
    "        # print(\"patch_size shape:\", patch_size)\n",
    "        # print(\"step shape:\", step)\n",
    "\n",
    "        # # Plotando a imagem redimensionada\n",
    "        # plt.figure(figsize=(8, 8))\n",
    "        # plt.imshow(cv2.cvtColor(np.array(large_image), cv2.COLOR_BGR2RGB))  # Convertendo BGR para RGB para visualização correta\n",
    "        # plt.title(\"large_image\")\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "\n",
    "        # Garantir que os rótulos também tenham o tipo correto\n",
    "        label = np.array(labels[i]).astype(np.uint8)\n",
    "        \n",
    "        # plt.figure(figsize=(8, 8))\n",
    "        # plt.imshow(cv2.cvtColor(label, cv2.COLOR_BGR2RGB))  # Convertendo BGR para RGB para visualização correta\n",
    "        # plt.title(\"labels\")\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        \n",
    "        large_label_resized = resize_image(label, target_shape)\n",
    "        # print(\"label\")\n",
    "        # print(\"Original label shape:\", label.shape)\n",
    "        # print(\"Target shape:\", target_shape)\n",
    "        # print(\"Resized shape:\", large_label_resized.shape)\n",
    "        # print(\"patch_size shape:\", patch_size)\n",
    "        # print(\"step shape:\", step)\n",
    "        \n",
    "        # Verifica se as dimensões redimensionadas são múltiplos exatos do patch_size\n",
    "        assert large_image_resized.shape[0] % patch_size[0] == 0, \"Altura da imagem não é múltipla do patch_size.\"\n",
    "        assert large_image_resized.shape[1] % patch_size[1] == 0, \"Largura da imagem não é múltipla do patch_size.\"\n",
    "        assert large_label_resized.shape[0] % patch_size[0] == 0, \"Altura da label não é múltipla do patch_size.\"\n",
    "        assert large_label_resized.shape[1] % patch_size[1] == 0, \"Largura da label não é múltipla do patch_size.\"\n",
    "        # plt.imshow(large_image)\n",
    "        # plt.show()\n",
    "        # plt.imshow(large_image_resized)\n",
    "        # plt.show()\n",
    "\n",
    "        patches_img = patchify(large_image_resized, patch_size=patch_size + (3,), step=step)\n",
    "        # print(len(patches_img), patches_img.shape)\n",
    "        patches_label = patchify(large_label_resized, patch_size=patch_size, step=step)\n",
    "        # print(len(patches_label), patches_label.shape)\n",
    "\n",
    "        for i in range(patches_img.shape[0]):\n",
    "            for j in range(patches_img.shape[1]):\n",
    "                single_patch_img = patches_img[i, j, 0]  # Remove a dimensão extra da Imagem\n",
    "                single_patch_label = patches_label[i, j]  # Remove a dimensão extra da Label\n",
    "                \n",
    "                # Converte o patch da imagem para float32\n",
    "                single_patch_img = np.transpose(single_patch_img, (2, 0, 1)).astype(np.float32)\n",
    "                # Converte o patch do rótulo para int64 (long)\n",
    "                single_patch_label = single_patch_label.astype(np.int64)\n",
    "                \n",
    "                # single_patch_img, single_patch_label = rotate_and_flip_patch(single_patch_img, single_patch_label) # Aplica a rotação e espelhamento\n",
    "                # print(single_patch_img.shape, single_patch_label.shape)\n",
    "                # plt.imshow(single_patch_img)\n",
    "                # plt.title(f\"Patch ({i}, {j})\")\n",
    "                # plt.axis('off')\n",
    "                # plt.show()\n",
    "                # plt.imshow(single_patch_label)\n",
    "                # plt.title(f\"Patch ({i}, {j})\")\n",
    "                # plt.axis('off')\n",
    "                # plt.show()\n",
    "                # single_patch_img = np.transpose(single_patch_img, (2, 0, 1))\n",
    "                # single_patch_label = np.transpose(single_patch_label, (2, 0, 1))\n",
    "                \n",
    "                all_img_patches.append(single_patch_img)\n",
    "                all_label_patches.append(single_patch_label)\n",
    "        # break\n",
    "    return np.array(all_img_patches), np.array(all_label_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F3DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "            \n",
    "            # TODO aplicar patchify, e pra cada (img, label), aplicar o RandomTransform()\n",
    "            # e só depois jogar no SupervisedReconstructionDataset().\n",
    "            # Ou então ajeitar o RandomTransform() para enviar diretamente ao SupervisedReconstructionDataset()\n",
    "            train_imgs, train_labels = cria_patchs_sismicas(train_img_reader, train_label_reader)\n",
    "            # print(train_imgs.shape)\n",
    "            # print(train_labels.shape)\n",
    "\n",
    "            train_dataset = SupervisedReconstructionDataset(\n",
    "                readers=[train_imgs, train_labels],\n",
    "                transforms=self.transforms,\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "\n",
    "            # TODO aplicar patchify, e pra cada (img, label), aplicar o RandomTransform()\n",
    "            # e só depois jogar no SupervisedReconstructionDataset().\n",
    "            # Ou então ajeitar o RandomTransform() para enviar diretamente ao SupervisedReconstructionDataset()\n",
    "            val_imgs, val_labels = cria_patchs_sismicas(val_img_reader, val_label_reader)\n",
    "\n",
    "            val_dataset = SupervisedReconstructionDataset(\n",
    "                readers=[val_imgs, val_labels],\n",
    "                transforms=self.transforms,\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "\n",
    "            # TODO aplicar patchify, e pra cada (img, label), aplicar o RandomTransform()\n",
    "            # e só depois jogar no SupervisedReconstructionDataset().\n",
    "            # Ou então ajeitar o RandomTransform() para enviar diretamente ao SupervisedReconstructionDataset()\n",
    "            test_imgs, test_labels = cria_patchs_sismicas(test_img_reader, test_label_reader)\n",
    "\n",
    "            test_dataset = SupervisedReconstructionDataset(\n",
    "                readers=[test_imgs, test_labels],\n",
    "                transforms=self.transforms,\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True, \n",
    "            worker_init_fn=self.worker_init_fn,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True, \n",
    "            worker_init_fn=self.worker_init_fn,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True, \n",
    "            worker_init_fn=self.worker_init_fn,\n",
    "            drop_last=False\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            pin_memory=True, \n",
    "            worker_init_fn=self.worker_init_fn,\n",
    "            drop_last=False\n",
    "        )\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        random.seed(18 + worker_id)\n",
    "\n",
    "data_module = F3DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    # transforms=transform,\n",
    "    transforms=[transformImage, transformLabel],\n",
    "    batch_size=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mO Kernel deu pane ao executar o código na célula atual ou em uma célula anterior. \n",
      "\u001b[1;31mAnalise o código nas células para identificar uma possível causa da pane. \n",
      "\u001b[1;31mClique <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqui</a> para obter mais informações. \n",
      "\u001b[1;31mConsulte Jupyter <a href='command:jupyter.viewOutput'>log</a> para obter mais detalhes."
     ]
    }
   ],
   "source": [
    "# Configura o DataModule para o estágio de treinamento\n",
    "data_module.setup(stage='fit')\n",
    "\n",
    "# Obtenha o DataLoader para os dados de treinamento\n",
    "train_loader = data_module.train_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# # Define o número de amostras que queremos exibir\n",
    "# num_amostras = 6\n",
    "# amostras = []\n",
    "\n",
    "# # Itera pelos batches para coletar as amostras\n",
    "# print(len(train_loader))\n",
    "# for batch in train_loader:\n",
    "#     image, label = batch\n",
    "#     print(image.shape, label.shape)\n",
    "#     image_np = image.permute(0, 2, 3, 1).detach().cpu().numpy()  # Ajusta para HWC (batch, height, width, channels)\n",
    "#     label_np = label.detach().cpu().numpy()  # Sem permutação, pois já está em formato HW\n",
    "\n",
    "#     # Normaliza a imagem se necessário\n",
    "#     if image_np.max() > 1:\n",
    "#         image_np = image_np / 255.0\n",
    "\n",
    "#     # Adiciona cada imagem e rótulo ao conjunto de amostras até atingir o número desejado\n",
    "#     for i in range(image_np.shape[0]):\n",
    "#         amostras.append((image_np[i], label_np[i]))\n",
    "#         if len(amostras) >= num_amostras:\n",
    "#             break\n",
    "#     if len(amostras) >= num_amostras:\n",
    "#         break\n",
    "\n",
    "# # Plota as 6 amostras em uma grade 3x2\n",
    "# for idx, (img, lbl) in enumerate(amostras):\n",
    "#     fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "#     # Plota a imagem\n",
    "#     axs[0].imshow(img)\n",
    "#     axs[0].set_title(\"Imagem\")\n",
    "#     axs[0].axis('off')\n",
    "\n",
    "#     # Plota o rótulo\n",
    "#     axs[1].imshow(lbl, cmap='gray')\n",
    "#     axs[1].set_title(\"Rótulo\")\n",
    "#     axs[1].axis('off')\n",
    "\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instancia o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/minerva/models/nets/image/segment_anything/build_sam.py:131: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    }
   ],
   "source": [
    "model = SAMLoRA(\n",
    "    image_size=512,\n",
    "    num_classes=5, # considera 6 pois internamente o sam faz +1 pro background\n",
    "    # pixel_mean=pixel_mean,\n",
    "    # pixel_std=pixel_std,\n",
    "    alpha=1,\n",
    "    rank=4,\n",
    "    # apply_lora_vision_encoder=apply_lora_vision_encoder,\n",
    "    # apply_lora_mask_decoder=apply_lora_mask_decoder,\n",
    "    # frozen_vision_encoder=frozen_vision_encoder,\n",
    "    # frozen_prompt_encoder=frozen_prompt_encoder,\n",
    "    # frozen_mask_decoder=frozen_mask_decoder,\n",
    "    # vit_model=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    # train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=6)},\n",
    "    # val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=6)},\n",
    "    # test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=6)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Dev-Container/notebooks/logs/sam_model/version_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | ce_loss   | CrossEntropyLoss | 0      | train\n",
      "1 | dice_loss | DiceLoss         | 0      | train\n",
      "2 | model     | Sam              | 91.8 M | train\n",
      "-------------------------------------------------------\n",
      "191 K     Trainable params\n",
      "91.6 M    Non-trainable params\n",
      "91.8 M    Total params\n",
      "367.260   Total estimated model params size (MB)\n",
      "310       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 339/339 [00:41<00:00,  8.09it/s, v_num=3, train_loss_step=0.144, val_loss_step=0.144, val_pixel_accuracy=0.984, val_mean_class_accuracy=0.976, val_dice_score=0.984, val_mIoU=0.950, val_loss_epoch=0.0989, train_pixel_accuracy=0.984, train_mean_class_accuracy=0.976, train_dice_score=0.984, train_mIoU=0.949, train_loss_epoch=0.067]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 339/339 [00:42<00:00,  7.95it/s, v_num=3, train_loss_step=0.144, val_loss_step=0.144, val_pixel_accuracy=0.984, val_mean_class_accuracy=0.976, val_dice_score=0.984, val_mIoU=0.950, val_loss_epoch=0.0989, train_pixel_accuracy=0.984, train_mean_class_accuracy=0.976, train_dice_score=0.984, train_mIoU=0.949, train_loss_epoch=0.067]\n",
      "Pipeline info saved at: /workspaces/Minerva-Dev-Container/notebooks/logs/sam_model/version_3/run_2024-11-02-15-43-164bcf6f4ad3864e4e93ae0370dc01484c.yaml\n"
     ]
    }
   ],
   "source": [
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "# Defina o logger do TensorBoard\n",
    "logger = TensorBoardLogger(\"logs\", name=\"sam_model\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger\n",
    ")\n",
    "# trainer.fit(model, data_module)\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 200/200 [00:10<00:00, 18.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_dice_score      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9221548438072205     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.2616531252861023     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_mIoU         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7319445610046387     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> test_mean_class_accuracy  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8064001202583313     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_pixel_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9221548438072205     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_dice_score     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9221548438072205    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.2616531252861023    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_mIoU        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7319445610046387    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36mtest_mean_class_accuracy \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8064001202583313    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_pixel_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9221548438072205    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Dev-Container/notebooks/logs/sam_model/version_3/run_2024-11-02-15-43-164bcf6f4ad3864e4e93ae0370dc01484c.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_pixel_accuracy': 0.9221548438072205,\n",
       "  'test_mean_class_accuracy': 0.8064001202583313,\n",
       "  'test_dice_score': 0.9221548438072205,\n",
       "  'test_mIoU': 0.7319445610046387,\n",
       "  'test_loss_epoch': 0.2616531252861023}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.test(model, data_module)\n",
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 200/200 [00:11<00:00, 17.92it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Dev-Container/notebooks/logs/sam_model/version_3/run_2024-11-02-15-43-164bcf6f4ad3864e4e93ae0370dc01484c.yaml\n"
     ]
    }
   ],
   "source": [
    "# preds = trainer.predict(model, data_module)\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.argmax(preds[108]['masks'], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (4, 512, 512) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_cmap\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py:2695\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   2690\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   2691\u001b[0m         X, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, aspect\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, interpolation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2692\u001b[0m         alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, vmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, extent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2693\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, filternorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, filterrad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4.0\u001b[39m,\n\u001b[1;32m   2694\u001b[0m         resample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2695\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maspect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maspect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2697\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2699\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolation_stage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilternorm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilternorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilterrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilterrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2701\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2702\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2703\u001b[0m     sci(__ret)\n\u001b[1;32m   2704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1444\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py:5665\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5657\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[1;32m   5658\u001b[0m im \u001b[38;5;241m=\u001b[39m mimage\u001b[38;5;241m.\u001b[39mAxesImage(\u001b[38;5;28mself\u001b[39m, cmap\u001b[38;5;241m=\u001b[39mcmap, norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   5659\u001b[0m                       interpolation\u001b[38;5;241m=\u001b[39minterpolation, origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   5660\u001b[0m                       extent\u001b[38;5;241m=\u001b[39mextent, filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   5661\u001b[0m                       filterrad\u001b[38;5;241m=\u001b[39mfilterrad, resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   5662\u001b[0m                       interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   5663\u001b[0m                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 5665\u001b[0m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5666\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5668\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to axes patch\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py:710\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A[:, :, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    709\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    711\u001b[0m                     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mshape))\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    714\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (4, 512, 512) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze().numpy(), cmap=label_cmap)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
