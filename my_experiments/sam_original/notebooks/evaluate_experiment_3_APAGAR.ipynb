{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for apply methodology of evaluate by Otávio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "from pathlib import Path\n",
    "import math\n",
    "import lightning as L\n",
    "import traceback\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchmetrics import Accuracy, JaccardIndex, F1Score, AUROC, Dice\n",
    "from minerva.models.loaders import FromPretrained\n",
    "from minerva.engines.patch_inferencer_engine import PatchInferencer\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.data.data_modules.parihaka import (\n",
    "    default_train_transforms,\n",
    "    default_test_transforms,\n",
    ")\n",
    "from minerva.utils.typing import PathLike\n",
    "from minerva.transforms.transform import Indexer, Unsqueeze, Squeeze\n",
    "from lightning.pytorch.loggers.csv_logs import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from typing import List, Tuple, Optional, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking CUDA on node\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking CUDA on node\")\n",
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "debug_load_weights = True\n",
    "\n",
    "root_data_dir = Path(\"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\")\n",
    "root_annotation_dir = Path(\"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\")\n",
    "single_channel = True\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial functions for the TiffReader and PNGReader with numeric sort\n",
    "# and delimiter \"_\" for the Parihaka dataset.\n",
    "TiffReaderWithNumericSort = partial(\n",
    "    TiffReader, sort_method=[\"text\", \"numeric\"], delimiter=\"_\", key_index=[0, 1]\n",
    ")\n",
    "PNGReaderWithNumericSort = partial(\n",
    "    PNGReader, sort_method=[\"text\", \"numeric\"], delimiter=\"_\", key_index=[0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets for SAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset for experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels (refatored for numpy)\n",
    "            image = data_readers[0]\n",
    "            \n",
    "            if image.shape[0] == 1:\n",
    "                image = np.repeat(image, 3, axis=0)\n",
    "            image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ParihakaDataModule (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParihakaDataModule(L.LightningDataModule):\n",
    "    \"\"\"Default data module for the Parihaka dataset. This data module creates a\n",
    "    supervised reconstruction dataset for training, validation, testing, and\n",
    "    prediction with default transforms to read the images and labels.\n",
    "\n",
    "    The parihaka dataset is organized as follows:\n",
    "    root_data_dir\n",
    "    ├── train\n",
    "    │   ├── il_1.tif\n",
    "    |   ├── il_2.tif\n",
    "    |   ├── ...\n",
    "    ├── val\n",
    "    │   ├── il_1.tif\n",
    "    |   ├── il_2.tif\n",
    "    |   ├── ...\n",
    "    ├── test\n",
    "    │   ├── il_1.tif\n",
    "    |   ├── il_2.tif\n",
    "    |   ├── ...\n",
    "    root_annotation_dir\n",
    "    ├── train\n",
    "    │   ├── il_1.png\n",
    "    |   ├── il_2.png\n",
    "    |   ├── ...\n",
    "    ├── val\n",
    "    │   ├── il_1.png\n",
    "    |   ├── il_2.png\n",
    "    |   ├── ...\n",
    "    ├── test\n",
    "    │   ├── il_1.png\n",
    "    |   ├── il_2.png\n",
    "    |   ├── ...\n",
    "\n",
    "    The `root_data_dir` contains the seismic images and the\n",
    "    `root_annotation_dir` contains the corresponding labels. Files with the\n",
    "    same name in the same directory are assumed to be pairs of seismic images\n",
    "    and labels. For instance `root_data_dir/train/il_1.tif` and\n",
    "    `root_annotation_dir/train/il_1.png` are assumed to be a pair of seismic\n",
    "    image and label.\n",
    "\n",
    "    Original parihaka dataset contains inlines and crosslines in train and val\n",
    "    directories. Inlines have dimensions (1006, 590, 3) and crosslines have\n",
    "    dimensions (1006, 531, 3). By default, crosslines are padded to (1006, 590)\n",
    "    and all images are transposed to (3, 1006, 590) format. Labels are also\n",
    "    padded to (1, 1006, 590) and are not transposed. Finally, images are cast to\n",
    "    float32 and labels are cast to int32.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_data_dir: PathLike,\n",
    "        root_annotation_dir: PathLike,\n",
    "        train_transforms: Optional[Union[_Transform, List[_Transform]]] = None,\n",
    "        valid_transforms: Optional[Union[_Transform, List[_Transform]]] = None,\n",
    "        test_transforms: Optional[Union[_Transform, List[_Transform]]] = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        num_workers: Optional[int] = None,\n",
    "        drop_last: bool = True,\n",
    "        data_loader_kwargs: Optional[dict] = None,\n",
    "    ):\n",
    "        \"\"\"Initialize the ParihakaDataModule with the root data and annotation\n",
    "        directories. The data module is initialized with default training and\n",
    "        testing transforms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        root_data_dir : str\n",
    "            Root directory containing the seismic images. Inside this directory\n",
    "            should be subdirectories `train`, `val`, and `test` containing the\n",
    "            training, validation, and testing TIFF images.\n",
    "        root_annotation_dir : str\n",
    "            Root directory containing the annotations. Inside this directory\n",
    "            should be subdirectories `train`, `val`, and `test` containing the\n",
    "            training, validation, and testing PNG annotations. Files with the\n",
    "            same name in the same directory are assumed to be pairs of seismic\n",
    "            images and labels.\n",
    "        train_transforms : Optional[Union[_Transform, List[_Transform]]], optional\n",
    "            2-element list of transform pipelines for the image and label reader.\n",
    "            Transforms to apply to the training and validation datasets. If\n",
    "            None, default training transforms are used, which pads images to\n",
    "            (1, 1006, 590) and transposes them to (3, 1006, 590) format. Labels\n",
    "            are also padded to (1006, 590). By default None\n",
    "        valid_transforms: Optional[Union[_Transform, List[_Transform]]], optional\n",
    "            2-element list of transform pipelines for the image and label reader.\n",
    "            Transforms to apply to the validation datasets. If None, default\n",
    "            training transforms are used, which pads images to (1006, 590) and\n",
    "            transposes them to (3, 1006, 590) format. Labels are also padded to\n",
    "            (1, 1006, 590). By default None\n",
    "        test_transforms : Optional[Union[_Transform, List[_Transform]]], optional\n",
    "            2-element list of transform pipelines for the image and label reader.\n",
    "            Transforms to apply to the testing and prediction datasets. If None,\n",
    "            default testing transforms are used, which transposes images to\n",
    "            CxHxW format. Labels are untouched. By default None\n",
    "        num_points : int, optional\n",
    "            Number of points will be added in sample. Default: 3 points.\n",
    "        batch_size : int, optional\n",
    "            Default batch size for the dataloaders, by default 1\n",
    "        num_workers : Optional[int], optional\n",
    "            Number of workers for the dataloaders, by default None. If None,\n",
    "            the number of workers is set to the number of CPUs on the system.\n",
    "        drop_last : bool, optional\n",
    "            Whether to drop the last batch if it is smaller than the batch size,\n",
    "            by default True.\n",
    "        data_loader_kwargs : Optional[dict], optional\n",
    "            Aditional keyword arguments to pass to the DataLoader instantiation,\n",
    "            for training, validation, testing, and prediction dataloaders.\n",
    "            By default None. Note that, `batch_size`, `num_workers`, and \n",
    "            `drop_last` are ignored if passed in this dictionary, as they are\n",
    "            already presented in the ParihakaDataModule constructor.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.root_data_dir = Path(root_data_dir)\n",
    "        self.root_annotation_dir = Path(root_annotation_dir)\n",
    "        self.train_transforms = train_transforms or default_train_transforms()\n",
    "        self.valid_transforms = valid_transforms or default_train_transforms()\n",
    "        self.test_transforms = test_transforms or default_test_transforms()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_points = num_points\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "        self.num_workers = num_workers or 1\n",
    "        self.drop_last = drop_last\n",
    "        self.datasets = {}\n",
    "        \n",
    "        self.data_loader_kwargs = data_loader_kwargs or {}\n",
    "        # Update the data loader kwargs with the batch size, num workers, and\n",
    "        # drop last parameters, passed to the ParihakaDataModule.\n",
    "        self.data_loader_kwargs.update(\n",
    "            {\n",
    "                \"batch_size\": self.batch_size,\n",
    "                \"num_workers\": self.num_workers,\n",
    "                \"drop_last\": self.drop_last,\n",
    "            }\n",
    "        )\n",
    "        # Remove the shuffle parameter from the data loader kwargs, as it is\n",
    "        # handled by the dataloaders.\n",
    "        self.data_loader_kwargs.pop(\"shuffle\", None)\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReaderWithNumericSort(\n",
    "                self.root_data_dir / \"train\"\n",
    "            )\n",
    "            train_label_reader = PNGReaderWithNumericSort(\n",
    "                self.root_annotation_dir / \"train\",\n",
    "            )\n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.train_transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReaderWithNumericSort(\n",
    "                self.root_data_dir / \"val\",\n",
    "            )\n",
    "            val_label_reader = PNGReaderWithNumericSort(\n",
    "                self.root_annotation_dir / \"val\",\n",
    "            )\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.valid_transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReaderWithNumericSort(\n",
    "                self.root_data_dir / \"test\",\n",
    "            )\n",
    "            test_label_reader = PNGReaderWithNumericSort(\n",
    "                self.root_annotation_dir / \"test\",\n",
    "            )\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.test_transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "\n",
    "    def _get_dataloader(self, partition: str, shuffle: bool):\n",
    "        return DataLoader(\n",
    "            self.datasets[partition],\n",
    "            shuffle=shuffle,\n",
    "            **self.data_loader_kwargs\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self._get_dataloader(\"train\", shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return self._get_dataloader(\"val\", shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self._get_dataloader(\"test\", shuffle=False)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return self._get_dataloader(\"predict\", shuffle=False)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"\"\"DataModule\n",
    "    Data: {self.root_data_dir}\n",
    "    Annotations: {self.root_annotation_dir}\n",
    "    Batch size: {self.batch_size}\"\"\"\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_data_module (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_module(\n",
    "    root_data_dir: PathLike,\n",
    "    root_annotation_dir: PathLike,\n",
    "    img_size: Optional[Tuple[int, int]] = (1006, 590),\n",
    "    num_points: int = 3,\n",
    "    batch_size: int = 1,\n",
    "    num_workers: Optional[int] = None,\n",
    "    seed: int = 42,\n",
    "    single_channel: bool = False,\n",
    ") -> L.LightningDataModule:\n",
    "    train_transforms = default_train_transforms(img_size=img_size, seed=seed)\n",
    "    if single_channel:\n",
    "        train_transforms[0] += Indexer(0)\n",
    "        train_transforms[0] += Unsqueeze(0)\n",
    "        train_transforms[1] += Indexer(0)\n",
    "        train_transforms[1] += Unsqueeze(0)\n",
    "\n",
    "    test_transforms = default_test_transforms(img_size=img_size, seed=seed)\n",
    "    if single_channel:\n",
    "        test_transforms[0] += Indexer(0)\n",
    "        test_transforms[0] += Unsqueeze(0)\n",
    "        test_transforms[1] += Indexer(0)\n",
    "        test_transforms[1] += Unsqueeze(0)\n",
    "\n",
    "    test_transforms[1] += Squeeze(0)\n",
    "\n",
    "    return ParihakaDataModule(\n",
    "        root_data_dir=root_data_dir,\n",
    "        root_annotation_dir=root_annotation_dir,\n",
    "        train_transforms=train_transforms,\n",
    "        valid_transforms=train_transforms,\n",
    "        test_transforms=test_transforms,\n",
    "        num_points=num_points,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug: get_data_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    data_module = get_data_module(\n",
    "        root_data_dir=root_data_dir,\n",
    "        root_annotation_dir=root_annotation_dir,\n",
    "        img_size=None,  # Uses original image size (no resize)\n",
    "        single_channel=single_channel,  # 1 or 3 channels\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        num_workers=os.cpu_count() # originally is 12\n",
    "    )\n",
    "\n",
    "    data_module.setup(\"fit\")\n",
    "    test_train_dataloader = data_module.train_dataloader()\n",
    "    print(\"Total batches: \", len(test_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    sample_index = 50  # Escolha o índice desejado\n",
    "    train_batch = [test_train_dataloader.dataset[sample_index]]\n",
    "\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape} - type: {type(train_batch[0]['image'])}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape} - type: {type(train_batch[0]['label'])}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']} - type: {type(train_batch[0]['original_size'])}\")\n",
    "    print(f\"Train batch point_coords shape: {train_batch[0]['point_coords'].shape} - type: {type(train_batch[0]['point_coords'])}\")\n",
    "    print(f\"Train batch point_labels shape: {train_batch[0]['point_labels'].shape} - type: {type(train_batch[0]['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    image_tensor = torch.from_numpy(train_batch[0]['image'])\n",
    "\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    print(\"shape da image: \", image_tensor.shape)\n",
    "    print(\"intervalo da image: \", torch.min(image_tensor), torch.max(image_tensor))\n",
    "    print(\"shape da label: \", train_batch[0]['label'].shape)\n",
    "    # image = image_tensor.squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    # label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    points = train_batch[0]['point_coords'] # Lista de coordenadas (x, y)\n",
    "    print(points)\n",
    "    image = image_tensor.permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plotando os pontos na imagem\n",
    "    for point in points:\n",
    "        for y, x in point:\n",
    "            axes[0].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # plotando os pontos na label\n",
    "    for point in points:\n",
    "        for y, x in point:\n",
    "            axes[1].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## finetuned_models (custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sam(\n",
    "        directory:str, \n",
    "        single_channel:bool = False,\n",
    "        num_classes:int = 6,\n",
    "        multimask_output:bool = True\n",
    "    ):\n",
    "    default_ckpt_dir = Path.cwd() / \"checkpoints\"\n",
    "    \n",
    "    img_size = (1006, 590)\n",
    "    vit_model = \"vit-b\"\n",
    "    ckpt_file = default_ckpt_dir / Path(directory)\n",
    "\n",
    "    model = Sam(\n",
    "        vit_type=vit_model,\n",
    "        num_multimask_outputs=num_classes,\n",
    "        iou_head_depth=num_classes,\n",
    "        apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "        # train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "        # val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "        # test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "        # multimask_output=multimask_output,\n",
    "        return_prediction_only=True,\n",
    "        # checkpoint=str(ckpt_file),\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"name\": \"sam\",\n",
    "        \"model\": model,\n",
    "        \"ckpt_file\": ckpt_file,\n",
    "        \"img_size\": img_size,\n",
    "        \"single_channel\": single_channel,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug: debug_load_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Encoder freeze!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sam:\n\tMissing key(s) in state_dict: \"model.mask_decoder.output_hypernetworks_mlps.0.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.4.bias\", \"model.mask_decoder.iou_prediction_head.0.weight\", \"model.mask_decoder.iou_prediction_head.0.bias\", \"model.mask_decoder.iou_prediction_head.2.weight\", \"model.mask_decoder.iou_prediction_head.2.bias\", \"model.mask_decoder.iou_prediction_head.4.weight\", \"model.mask_decoder.iou_prediction_head.4.bias\", \"model.mask_decoder.iou_prediction_head.6.weight\", \"model.mask_decoder.iou_prediction_head.6.bias\", \"model.mask_decoder.iou_prediction_head.8.weight\", \"model.mask_decoder.iou_prediction_head.8.bias\", \"model.mask_decoder.iou_prediction_head.10.weight\", \"model.mask_decoder.iou_prediction_head.10.bias\". \n\tUnexpected key(s) in state_dict: \"model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.2.bias\", \"model.mask_decoder.iou_prediction_head.layers.0.weight\", \"model.mask_decoder.iou_prediction_head.layers.0.bias\", \"model.mask_decoder.iou_prediction_head.layers.1.weight\", \"model.mask_decoder.iou_prediction_head.layers.1.bias\", \"model.mask_decoder.iou_prediction_head.layers.2.weight\", \"model.mask_decoder.iou_prediction_head.layers.2.bias\", \"model.mask_decoder.iou_prediction_head.layers.3.weight\", \"model.mask_decoder.iou_prediction_head.layers.3.bias\", \"model.mask_decoder.iou_prediction_head.layers.4.weight\", \"model.mask_decoder.iou_prediction_head.layers.4.bias\", \"model.mask_decoder.iou_prediction_head.layers.5.weight\", \"model.mask_decoder.iou_prediction_head.layers.5.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_load_weights:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# tmp_model = sam(directory=\"parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt\")\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# tmp_loaded_model = FromPretrained(tmp_model['model'], tmp_model['ckpt_file'], strict=False)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     tmp_loaded_model \u001b[38;5;241m=\u001b[39m \u001b[43mSam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_1_1.0_SAM_ViT_B_parihaka_fine_tuning_&_fine_tuning_1.0-2025-02-16-epoch=04-val_loss=0.11.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvit-b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_multimask_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43miou_head_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tmp_loaded_model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/module.py:1581\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1492\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[1;32m   1493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1494\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1500\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \n\u001b[1;32m   1580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1581\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1587\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1588\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/core/saving.py:187\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    184\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Sam:\n\tMissing key(s) in state_dict: \"model.mask_decoder.output_hypernetworks_mlps.0.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.4.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.4.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.4.bias\", \"model.mask_decoder.iou_prediction_head.0.weight\", \"model.mask_decoder.iou_prediction_head.0.bias\", \"model.mask_decoder.iou_prediction_head.2.weight\", \"model.mask_decoder.iou_prediction_head.2.bias\", \"model.mask_decoder.iou_prediction_head.4.weight\", \"model.mask_decoder.iou_prediction_head.4.bias\", \"model.mask_decoder.iou_prediction_head.6.weight\", \"model.mask_decoder.iou_prediction_head.6.bias\", \"model.mask_decoder.iou_prediction_head.8.weight\", \"model.mask_decoder.iou_prediction_head.8.bias\", \"model.mask_decoder.iou_prediction_head.10.weight\", \"model.mask_decoder.iou_prediction_head.10.bias\". \n\tUnexpected key(s) in state_dict: \"model.mask_decoder.output_hypernetworks_mlps.0.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.0.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.1.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.2.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.3.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.4.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.5.layers.2.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.0.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.0.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.1.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.1.bias\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.2.weight\", \"model.mask_decoder.output_hypernetworks_mlps.6.layers.2.bias\", \"model.mask_decoder.iou_prediction_head.layers.0.weight\", \"model.mask_decoder.iou_prediction_head.layers.0.bias\", \"model.mask_decoder.iou_prediction_head.layers.1.weight\", \"model.mask_decoder.iou_prediction_head.layers.1.bias\", \"model.mask_decoder.iou_prediction_head.layers.2.weight\", \"model.mask_decoder.iou_prediction_head.layers.2.bias\", \"model.mask_decoder.iou_prediction_head.layers.3.weight\", \"model.mask_decoder.iou_prediction_head.layers.3.bias\", \"model.mask_decoder.iou_prediction_head.layers.4.weight\", \"model.mask_decoder.iou_prediction_head.layers.4.bias\", \"model.mask_decoder.iou_prediction_head.layers.5.weight\", \"model.mask_decoder.iou_prediction_head.layers.5.bias\". "
     ]
    }
   ],
   "source": [
    "if debug_load_weights:\n",
    "    # tmp_model = sam(directory=\"parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt\")\n",
    "    # tmp_loaded_model = FromPretrained(tmp_model['model'], tmp_model['ckpt_file'], strict=False)\n",
    "\n",
    "    tmp_loaded_model = Sam.load_from_checkpoint(\n",
    "        checkpoint_path=\"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_1_1.0_SAM_ViT_B_parihaka_fine_tuning_&_fine_tuning_1.0-2025-02-16-epoch=04-val_loss=0.11.ckpt\",\n",
    "        vit_type=\"vit-b\",\n",
    "        num_multimask_outputs=6,\n",
    "        iou_head_depth=6,\n",
    "        # apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    )\n",
    "    print(tmp_loaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictorWrapper(L.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Adds a dimension after logits to match PatchInferencer requirements\n",
    "        # 6x1006x590 -> 6x1x1006x590\n",
    "        res = self.model(x)\n",
    "        # For SET-R model, the output is a tuple (y_hat and more 3 aux outputs)\n",
    "        if isinstance(res, tuple):\n",
    "            res = res[0]\n",
    "        res = res.unsqueeze(2)\n",
    "        return res\n",
    "\n",
    "\n",
    "class PatchedPredictorWrapper(L.LightningModule):\n",
    "    def __init__(self, model: PatchInferencer):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        if not isinstance(batch, torch.Tensor) and len(batch) == 2:\n",
    "            # print(\"::Batch is a tuple, unpacking it and passing to patch inferencer\")\n",
    "            batch_x, batch_y = batch\n",
    "            outputs = self.forward(batch_x)\n",
    "            # return outputs, batch_y\n",
    "            return outputs\n",
    "        else:\n",
    "            # print(\"::Batch is not a tuple, passing to patch inferencer normal\")\n",
    "            return self.forward(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, ckpt):\n",
    "    return FromPretrained(model, ckpt, strict=False)\n",
    "\n",
    "def load_model_from_info(model_info):\n",
    "    model = model_info[\"model\"]\n",
    "    ckpt_file = model_info[\"ckpt_file\"]\n",
    "    return load_model(model, ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_data_module(\n",
    "    model_instantiator_func,\n",
    "    img_shape: Tuple[int, int] = (1006, 590),\n",
    "    num_points: int = 3,\n",
    "    n_classes: int = 3,\n",
    "    batch_size: int = 1,\n",
    "):\n",
    "    # Model Info is a dictionary containing information about the model:\n",
    "    #   name: str\n",
    "    #   model: L.LightningModule\n",
    "    #   ckpt_file: Path\n",
    "    #   img_size: Tuple[int, int]\n",
    "    #   single_channel: bool\n",
    "    model_info = model_instantiator_func\n",
    "\n",
    "    # ---- 1. Data ----\n",
    "    data_module = get_data_module(\n",
    "        root_data_dir=root_data_dir,\n",
    "        root_annotation_dir=root_annotation_dir,\n",
    "        img_size=None,  # Uses original image size (no resize)\n",
    "        single_channel=model_info[\"single_channel\"],  # 1 or 3 channels\n",
    "        num_points=num_points,\n",
    "        batch_size=batch_size,\n",
    "        seed=42,\n",
    "        num_workers=os.cpu_count()#12\n",
    "    )\n",
    "\n",
    "    # ---- 2. Model and wrapper ----\n",
    "\n",
    "    # Let's check if padding is needed.\n",
    "    # If shape of model and data is the same, no padding is needed\n",
    "    if img_shape == model_info[\"img_size\"]:\n",
    "        pad_dict = None\n",
    "    else:\n",
    "        model_h, model_w = model_info[\"img_size\"]\n",
    "        h_ratio = math.ceil(img_shape[0] / model_h)\n",
    "        w_ratio = math.ceil(img_shape[1] / model_w)\n",
    "        pad_dict = {\n",
    "            \"mode\": \"constant\",\n",
    "            \"value\": 0,\n",
    "            \"pad\": (0, h_ratio * model_h, w_ratio * model_w),\n",
    "        }\n",
    "\n",
    "    model_input_shape = (\n",
    "        1 if model_info[\"single_channel\"] else 3,\n",
    "        *model_info[\"img_size\"],\n",
    "    )\n",
    "    model_output_shape = (n_classes, 1, *model_info[\"img_size\"])\n",
    "\n",
    "    # Load model\n",
    "    model = load_model_from_info(model_info)\n",
    "    model = PredictorWrapper(model)\n",
    "    model = PatchInferencer(\n",
    "        model=model,  # type: ignore (as used only for inferencing)\n",
    "        input_shape=model_input_shape,\n",
    "        output_shape=model_output_shape,\n",
    "        padding=pad_dict.copy() if pad_dict else None,\n",
    "    )\n",
    "    model = PatchedPredictorWrapper(model)\n",
    "    model = model.eval()\n",
    "\n",
    "    # ---- 3. Return ----\n",
    "    return {\n",
    "        \"model\": model,\n",
    "        \"name\": model_info[\"name\"],\n",
    "        \"data_module\": data_module,\n",
    "        \"ckpt_file\": model_info[\"ckpt_file\"],\n",
    "        \"model_input_shape\": model_input_shape,\n",
    "        \"model_output_shape\": model_output_shape,\n",
    "        \"pad\": pad_dict,\n",
    "        \"single_channel\": model_info[\"single_channel\"],\n",
    "        \"n_classes\": n_classes,\n",
    "        \"batch_size\": batch_size,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_inference(\n",
    "    model_instantiator_func,\n",
    "    predictions_path: Path,\n",
    "    batch_size=1,\n",
    "    n_classes=3,\n",
    "    num_points=3,\n",
    "    img_shape: Tuple[int, int] = (1006, 590),\n",
    "    accelerator: str = \"gpu\",\n",
    "    devices: int = 1,\n",
    "):\n",
    "    model_info = load_model_and_data_module(\n",
    "        model_instantiator_func=model_instantiator_func,\n",
    "        img_shape=img_shape,\n",
    "        n_classes=n_classes,\n",
    "        num_points=num_points,\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    print(f\"Loading model from ckpt at: {model_info['ckpt_file']}\")\n",
    "    predictions_file = predictions_path / f\"{model_info['name']}.npy\"\n",
    "    if predictions_file.exists():\n",
    "        print(\n",
    "            f\"Predictions already exist at {predictions_file}. Skipping inference.\"\n",
    "        )\n",
    "        return None\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        accelerator=accelerator,\n",
    "        devices=devices,\n",
    "        logger=False,\n",
    "        max_epochs=1,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "    predictions = trainer.predict(\n",
    "        model_info[\"model\"], model_info[\"data_module\"]\n",
    "    )\n",
    "    predictions = torch.stack(predictions, dim=0) # type: ignore\n",
    "    predictions = predictions.squeeze()\n",
    "    predictions = predictions.float().cpu().numpy()\n",
    "    np.save(predictions_file, predictions)\n",
    "    \n",
    "    print(f\"Predictions saved at {predictions_file}. Shape: {predictions.shape}\")\n",
    "    return predictions_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_default_ckpt_dir(ckpt_dir: Path):\n",
    "    global default_ckpt_dir\n",
    "    default_ckpt_dir = ckpt_dir\n",
    "    print(f\"Set default checkpoint directory to {default_ckpt_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set default checkpoint directory to /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints\n",
      "--------------------------------------------------------------------------------\n",
      "********************\n",
      "Model: sam\n",
      "********************\n",
      "Sam(\n",
      "  (loss_fn): CrossEntropyLoss()\n",
      "  (model): _SAM(\n",
      "    (image_encoder): ImageEncoderViT(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "      )\n",
      "      (blocks): ModuleList(\n",
      "        (0-11): 12 x Block(\n",
      "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (attn): Attention(\n",
      "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "          (mlp): MLPBlock(\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (neck): Sequential(\n",
      "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): LayerNorm2d()\n",
      "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (3): LayerNorm2d()\n",
      "      )\n",
      "    )\n",
      "    (prompt_encoder): PromptEncoder(\n",
      "      (pe_layer): PositionEmbeddingRandom()\n",
      "      (point_embeddings): ModuleList(\n",
      "        (0-3): 4 x Embedding(1, 256)\n",
      "      )\n",
      "      (not_a_point_embed): Embedding(1, 256)\n",
      "      (mask_downscaling): Sequential(\n",
      "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (1): LayerNorm2d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (4): LayerNorm2d()\n",
      "        (5): GELU(approximate='none')\n",
      "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "      (no_mask_embed): Embedding(1, 256)\n",
      "    )\n",
      "    (mask_decoder): MaskDecoder(\n",
      "      (transformer): TwoWayTransformer(\n",
      "        (layers): ModuleList(\n",
      "          (0-1): 2 x TwoWayAttentionBlock(\n",
      "            (self_attn): AttentionMaskDecoder(\n",
      "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
      "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "            )\n",
      "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLPBlock(\n",
      "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
      "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
      "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
      "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
      "        )\n",
      "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (iou_token): Embedding(1, 256)\n",
      "      (mask_tokens): Embedding(4, 256)\n",
      "      (output_upscaling): Sequential(\n",
      "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (1): LayerNorm2d()\n",
      "        (2): GELU(approximate='none')\n",
      "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "        (4): GELU(approximate='none')\n",
      "      )\n",
      "      (output_hypernetworks_mlps): ModuleList(\n",
      "        (0-3): 4 x MLP(\n",
      "          (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=256, out_features=32, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (iou_prediction_head): MLP(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=256, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt\n",
      "Error executing model: sam\n",
      "-------------------------------------------------------------------------------- \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2938714/2236796148.py\", line 22, in <module>\n",
      "    perform_inference(model_instantiator_func, predictions_path)\n",
      "  File \"/tmp/ipykernel_2938714/1164291490.py\", line 11, in perform_inference\n",
      "    model_info = load_model_and_data_module(\n",
      "  File \"/tmp/ipykernel_2938714/1897333358.py\", line 51, in load_model_and_data_module\n",
      "    model = load_model_from_info(model_info)\n",
      "  File \"/tmp/ipykernel_2938714/2866089643.py\", line 9, in load_model_from_info\n",
      "    return load_model(model, ckpt_file)\n",
      "  File \"/tmp/ipykernel_2938714/2866089643.py\", line 2, in load_model\n",
      "    return FromPretrained(model, ckpt, strict=False)\n",
      "  File \"/home/vscode/.local/lib/python3.10/site-packages/minerva/models/loaders.py\", line 296, in __init__\n",
      "    raise ValueError(f\"Missing keys: {missing_keys}\")\n",
      "ValueError: Missing keys: ['model.mask_decoder.output_hypernetworks_mlps.0.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.0.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.0.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.0.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.0.4.weight', 'model.mask_decoder.output_hypernetworks_mlps.0.4.bias', 'model.mask_decoder.output_hypernetworks_mlps.1.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.1.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.1.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.1.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.1.4.weight', 'model.mask_decoder.output_hypernetworks_mlps.1.4.bias', 'model.mask_decoder.output_hypernetworks_mlps.2.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.2.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.2.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.2.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.2.4.weight', 'model.mask_decoder.output_hypernetworks_mlps.2.4.bias', 'model.mask_decoder.output_hypernetworks_mlps.3.0.weight', 'model.mask_decoder.output_hypernetworks_mlps.3.0.bias', 'model.mask_decoder.output_hypernetworks_mlps.3.2.weight', 'model.mask_decoder.output_hypernetworks_mlps.3.2.bias', 'model.mask_decoder.output_hypernetworks_mlps.3.4.weight', 'model.mask_decoder.output_hypernetworks_mlps.3.4.bias', 'model.mask_decoder.iou_prediction_head.0.weight', 'model.mask_decoder.iou_prediction_head.0.bias', 'model.mask_decoder.iou_prediction_head.2.weight', 'model.mask_decoder.iou_prediction_head.2.bias', 'model.mask_decoder.iou_prediction_head.4.weight', 'model.mask_decoder.iou_prediction_head.4.bias']\n"
     ]
    }
   ],
   "source": [
    "root_predictions_path = Path.cwd() / \"predictions\"\n",
    "finetuned_models_path = [\n",
    "    Path.cwd() / \"checkpoints\"\n",
    "]\n",
    "\n",
    "for path in finetuned_models_path:\n",
    "    set_default_ckpt_dir(path)\n",
    "\n",
    "    predictions_path = root_predictions_path / path.name\n",
    "    predictions_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for model_instantiator_func in [\n",
    "        sam(directory=\"parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt\")\n",
    "    ]:\n",
    "        model_name = model_instantiator_func['name']\n",
    "        print(\"-\"*80)\n",
    "\n",
    "        try:\n",
    "            print(\"*\"*20)\n",
    "            print(f\"Model: {model_name}\")\n",
    "            print(\"*\"*20)\n",
    "            perform_inference(model_instantiator_func, predictions_path)\n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "            print(f\"Error executing model: {model_name}\")\n",
    "        print(\"-\"*80, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (2953317919.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[38], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt()\u001b[0m\n\u001b[0m                                                                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
