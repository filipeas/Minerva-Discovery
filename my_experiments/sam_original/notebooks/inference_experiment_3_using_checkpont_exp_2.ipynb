{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Notebook\n",
    "- This notebook implements the experiment 3.\n",
    "- In the experiment 3, we use SAM model in your original version and:\n",
    "    - train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is promptable, i.e, we use 3 points for learn model where to segment.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/last.ckpt\"\n",
    "\n",
    "# checkpoints SAM\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 20\n",
    "ratio = 1.0\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels\n",
    "            image = data_readers[0]\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam.load_from_checkpoint(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"evaluate_sam_experiment_3_{ratio}_{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    # save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    # mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-23-16-00-01d1ce58a5f20c4cfb936633aa02b32a04.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_9 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 2/1197 [00:01<11:04,  1.80it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1197/1197 [01:49<00:00, 10.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.3381786644458771     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7761608958244324     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.3381786644458771    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7761608958244324    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-23-16-00-01d1ce58a5f20c4cfb936633aa02b32a04.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.3381786644458771,\n",
       "  'test_mIoU_epoch': 0.7761608958244324}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-23-16-00-01d1ce58a5f20c4cfb936633aa02b32a04.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1197/1197 [01:49<00:00, 10.96it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-23-16-00-01d1ce58a5f20c4cfb936633aa02b32a04.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
