{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Notebook\n",
    "- This notebook implements the experiment 3.\n",
    "- In the experiment 3, we use SAM model in your original version and:\n",
    "    - train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is promptable, i.e, we use 3 points for learn model where to segment.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/logs/sam_vit_b_experiment_4/seam_ai/checkpoints/last.ckpt\"\n",
    "\n",
    "# checkpoints SAM\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 20\n",
    "ratio = 1.0\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels\n",
    "            image = data_readers[0]\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam.load_from_checkpoint(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_10\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"evaluate_sam_experiment_3_{ratio}_{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    # save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    # mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_10/run_2025-02-26-02-55-59cd77d6e15b544fb7bfedd17b0bfe91ef.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_10 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 3/1197 [00:01<06:38,  2.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1197/1197 [01:48<00:00, 11.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5000007748603821     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7815079689025879     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5000007748603821    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7815079689025879    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_10/run_2025-02-26-02-55-59cd77d6e15b544fb7bfedd17b0bfe91ef.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.5000007748603821,\n",
       "  'test_mIoU_epoch': 0.7815079689025879}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_10/run_2025-02-26-02-55-59cd77d6e15b544fb7bfedd17b0bfe91ef.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1197/1197 [01:46<00:00, 11.20it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_10/run_2025-02-26-02-55-59cd77d6e15b544fb7bfedd17b0bfe91ef.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
