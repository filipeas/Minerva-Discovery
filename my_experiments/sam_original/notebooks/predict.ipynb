{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchvision.transforms.functional import resize, to_pil_image\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "model_name_experiment = \"SAM-ViT_B-original-weights_f3_\"\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "vit_model = 'vit-b'\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "# vit_model = 'vit-h'\n",
    "\n",
    "USE_ORIGINAL_WEIGHTS = False\n",
    "multimask_output=False # if True, return num_classes, else, return the class with mostly iou\n",
    "num_classes = 3 # num of classes for original Sam\n",
    "num_points = 10 # num of prompt points\n",
    "\n",
    "# fine_tuning & adapter\n",
    "if not USE_ORIGINAL_WEIGHTS:\n",
    "    num_classes = 6 # num of classes for Sam fine tuned\n",
    "    vit_model = 'vit-b'\n",
    "    checkpoint = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints_f3/sam-fine_tuning_&_adapter_1.0-2024-12-17-epoch=98-val_loss=0.01.ckpt\"\n",
    "    # checkpoint = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints_parihaka/sam-fine_tuning_&_adapter_1.0-2024-12-18-epoch=10-val_loss=0.13.ckpt\"\n",
    "    model_name_experiment = f\"SAM-ViT_B_fine_tuning_&_adapter_f3_\"\n",
    "    apply_freeze={\"prompt_encoder\": True, \"image_encoder\": False, \"mask_decoder\": True}\n",
    "    apply_adapter={\"mask_decoder\": LoRA}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeLongestSide:\n",
    "    \"\"\"\n",
    "    Resizes images to the longest side 'target_length', as well as provides\n",
    "    methods for resizing coordinates and boxes. Provides methods for\n",
    "    transforming both numpy array and batched torch tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, target_length: int) -> None:\n",
    "        self.target_length = target_length\n",
    "\n",
    "    def apply_image(self, image: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array with shape HxWxC in uint8 format.\n",
    "        \"\"\"\n",
    "        target_size = self.get_preprocess_shape(image.shape[0], image.shape[1], self.target_length)\n",
    "        return np.array(resize(to_pil_image(image), target_size))\n",
    "\n",
    "    def apply_coords(self, coords: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array of length 2 in the final dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(\n",
    "            original_size[0], original_size[1], self.target_length\n",
    "        )\n",
    "        coords = deepcopy(coords).astype(float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def apply_boxes(self, boxes: np.ndarray, original_size: Tuple[int, ...]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Expects a numpy array shape Bx4. Requires the original image size\n",
    "        in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "\n",
    "    def apply_image_torch(self, image: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects batched images with shape BxCxHxW and float format. This\n",
    "        transformation may not exactly match apply_image. apply_image is\n",
    "        the transformation expected by the model.\n",
    "        \"\"\"\n",
    "        # Expects an image in BCHW format. May not exactly match apply_image.\n",
    "        target_size = self.get_preprocess_shape(image.shape[2], image.shape[3], self.target_length)\n",
    "        return F.interpolate(\n",
    "            image, target_size, mode=\"bilinear\", align_corners=False, antialias=True\n",
    "        )\n",
    "\n",
    "    def apply_coords_torch(\n",
    "        self, coords: torch.Tensor, original_size: Tuple[int, ...]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects a torch tensor with length 2 in the last dimension. Requires the\n",
    "        original image size in (H, W) format.\n",
    "        \"\"\"\n",
    "        old_h, old_w = original_size\n",
    "        new_h, new_w = self.get_preprocess_shape(\n",
    "            original_size[0], original_size[1], self.target_length\n",
    "        )\n",
    "        coords = deepcopy(coords).to(torch.float)\n",
    "        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n",
    "        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n",
    "        return coords\n",
    "\n",
    "    def apply_boxes_torch(\n",
    "        self, boxes: torch.Tensor, original_size: Tuple[int, ...]\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expects a torch tensor with shape Bx4. Requires the original image\n",
    "        size in (H, W) format.\n",
    "        \"\"\"\n",
    "        boxes = self.apply_coords_torch(boxes.reshape(-1, 2, 2), original_size)\n",
    "        return boxes.reshape(-1, 4)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_preprocess_shape(oldh: int, oldw: int, long_side_length: int) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Compute the output size given input size and target long side length.\n",
    "        \"\"\"\n",
    "        scale = long_side_length * 1.0 / max(oldh, oldw)\n",
    "        newh, neww = oldh * scale, oldw * scale\n",
    "        neww = int(neww + 0.5)\n",
    "        newh = int(newh + 0.5)\n",
    "        return (newh, neww)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usando Sam ajustado\n",
      "Prompt Encoder freeze!\n",
      "Mask Decoder freeze!\n",
      "LoRA applied in Mask Decoder!\n"
     ]
    }
   ],
   "source": [
    "if not USE_ORIGINAL_WEIGHTS:\n",
    "    print(\"usando Sam ajustado\")\n",
    "    # modelo ajustado\n",
    "    model = Sam.load_from_checkpoint(\n",
    "        checkpoint_path=checkpoint,\n",
    "        vit_type=vit_model,\n",
    "        num_multimask_outputs=num_classes,\n",
    "        iou_head_depth=num_classes,\n",
    "        apply_freeze=apply_freeze,\n",
    "        apply_adapter=apply_adapter,\n",
    "        train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "        val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "        test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)}\n",
    "    )\n",
    "else:\n",
    "    print(\"usando Sam ORIGINAL\")\n",
    "    # modelo com pesos originais\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(\"Using device: \", device)\n",
    "    model = Sam(\n",
    "        vit_type=vit_model,\n",
    "        checkpoint=checkpoint_path,\n",
    "        num_multimask_outputs=num_classes,\n",
    "        iou_head_depth=num_classes\n",
    "    ).to(device)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "train_img_reader = TiffReader(Path(train_path) / 'test') # Configura os dados para inferência\n",
    "train_label_reader = PNGReader(Path(annotation_path) / 'test') # Configura os dados para inferência\n",
    "\n",
    "miou_metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes) # Inicializando a métrica de mIoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(image, label, pred, diff, score, point_coords, point_labels):\n",
    "    \"\"\"\n",
    "    Plota as imagens lado a lado: imagem original, label, predição, diff.\n",
    "    Pontos acumulados são exibidos sobre as imagens.\n",
    "    \"\"\"\n",
    "    num_subplots = 4  # Número de subplots: imagem original, label, pred, diff\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(1, num_subplots, figsize=(5 * num_subplots, 5))\n",
    "\n",
    "    # Plot 1: Imagem original\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot 2: Label\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Plot 3: Predição acumulada\n",
    "    axes[2].imshow(pred, cmap='gray')\n",
    "    axes[2].set_title(f\"Pred - Score: {score}\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    # Plot 4: Diferença entre label e pred\n",
    "    axes[3].imshow(diff, cmap='gray')\n",
    "    axes[3].set_title(\"Difference (Label - Pred)\")\n",
    "    axes[3].axis('off')\n",
    "\n",
    "    # Adiciona os pontos em todas as imagens\n",
    "    for ax in axes:\n",
    "        for (x, y), label in zip(point_coords, point_labels):\n",
    "            color = 'green' if label == 1 else 'red'\n",
    "            ax.scatter(x, y, color=color, s=50, edgecolors='white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_points():\n",
    "    # Inicialize os acumuladores como arrays vazios\n",
    "    accumulated_coords = np.empty((0, 2), dtype=int)  # Nx2 array\n",
    "    accumulated_labels = np.empty((0,), dtype=int)   # Array de comprimento N\n",
    "    return accumulated_coords, accumulated_labels\n",
    "\n",
    "def calculate_center_region(\n",
    "        accumulated_coords, \n",
    "        accumulated_labels, \n",
    "        original_size, \n",
    "        region: np.array, \n",
    "        point_type: str, \n",
    "        min_distance: int = 10):\n",
    "    \"\"\"\n",
    "    Calcula o centroide da maior região de pixels brancos de uma imagem binária,\n",
    "    deslocando horizontalmente o ponto se ele estiver próximo demais dos acumulados.\n",
    "\n",
    "    Args:\n",
    "        region (np.array): Imagem binária com a região de interesse (pixels brancos).\n",
    "        point_type (str): Tipo do ponto ('positive' ou 'negative').\n",
    "        min_distance (int): Distância mínima permitida entre pontos.\n",
    "\n",
    "    Returns:\n",
    "        point_coords (np.ndarray): Array Nx2 de pontos acumulados.\n",
    "        point_labels (np.ndarray): Array N de rótulos acumulados.\n",
    "    \"\"\"\n",
    "    if not isinstance(region, np.ndarray):\n",
    "        raise TypeError(\"region needs to be a NumPy array.\")\n",
    "    \n",
    "    # Encontrar as componentes conectadas\n",
    "    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(region, connectivity=8)\n",
    "\n",
    "    if num_labels < 2:  # Apenas fundo e nenhuma região branca\n",
    "        raise ValueError(\"No connected white regions found in the binary image.\")\n",
    "    \n",
    "    # Ignorar o rótulo 0 (fundo), pegar a maior componente conectada\n",
    "    largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "    center_x, center_y = centroids[largest_label]\n",
    "    center_x, center_y = int(center_x), int(center_y)\n",
    "    new_coords = np.array([[center_x, center_y]])\n",
    "\n",
    "    # Verificar se o ponto está muito próximo dos anteriores\n",
    "    if accumulated_coords.shape[0] > 0:\n",
    "        distances = np.sqrt(np.sum((accumulated_coords - new_coords) ** 2, axis=1))\n",
    "        if np.any(distances < min_distance):\n",
    "            # print(\"Ponto muito próximo ao anterior, deslocando horizontalmente...\")\n",
    "            \n",
    "            # Tentar deslocar o ponto horizontalmente dentro da região branca\n",
    "            region_height, region_width = region.shape\n",
    "            # Tenta deslocar horizontalmente usando o min_distance\n",
    "            for delta_x in range(min_distance, region_width, min_distance):  # Incrementa em min_distance\n",
    "                candidate_x_right = center_x + delta_x\n",
    "                candidate_x_left = center_x - delta_x\n",
    "\n",
    "                # Verifica primeiro para direita, depois para esquerda\n",
    "                if candidate_x_right < region_width and region[center_y, candidate_x_right] > 0:\n",
    "                    center_x = candidate_x_right\n",
    "                    break\n",
    "                elif candidate_x_left >= 0 and region[center_y, candidate_x_left] > 0:\n",
    "                    center_x = candidate_x_left\n",
    "                    break\n",
    "\n",
    "            new_coords = np.array([[center_x, center_y]])\n",
    "\n",
    "    # Definir o rótulo (positivo ou negativo)\n",
    "    if point_type == 'positive':\n",
    "        new_labels = np.array([1])\n",
    "    elif point_type == 'negative':\n",
    "        new_labels = np.array([0])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid point_type. Must be 'positive' or 'negative'.\")\n",
    "\n",
    "    # Acumular os resultados\n",
    "    accumulated_coords = np.vstack([accumulated_coords, new_coords])\n",
    "    accumulated_labels = np.hstack([accumulated_labels, new_labels])\n",
    "\n",
    "    # convertendo para o formato do Sam()\n",
    "    transform = ResizeLongestSide(model.model.image_encoder.img_size)\n",
    "    point_coords = transform.apply_coords(accumulated_coords, original_size)\n",
    "    coords_torch = torch.as_tensor(point_coords, dtype=torch.float, device=model.device)\n",
    "    labels_torch = torch.as_tensor(accumulated_labels, dtype=torch.int, device=model.device)\n",
    "    coords_torch, labels_torch = coords_torch[None, :, :], labels_torch[None, :]\n",
    "\n",
    "    return accumulated_coords, accumulated_labels, coords_torch, labels_torch\n",
    "\n",
    "def calculate_diff_label_pred(label:np.array, pred:np.array):\n",
    "    \"\"\"\n",
    "    Calcula a diferença entre duas imagens binárias e determina se a área externa ou interna é maior.\n",
    "\n",
    "    Args:\n",
    "        label (np.array): Imagem binária de referência (label).\n",
    "        pred (np.array): Imagem binária predita (pred).\n",
    "\n",
    "    Returns:\n",
    "        diff_colored (np.array): Imagem colorida representando as diferenças.\n",
    "        point_type (str): 'negative' se a área externa for maior, 'positive' se a interna for maior.\n",
    "    \"\"\"\n",
    "    if label.shape != pred.shape:\n",
    "        raise ValueError(\"Label and Pred images have differents shapes. Check it before call calculate_dif_label_pred() function.\")\n",
    "\n",
    "    # Máscaras para regiões de diferença\n",
    "    mask_outward = (label > pred)  # Diferença para fora -> Vermelho\n",
    "    mask_inward = (label < pred)  # Diferença para dentro -> Azul\n",
    "\n",
    "    area_outward = np.sum(mask_outward)\n",
    "    area_inward = np.sum(mask_inward)\n",
    "\n",
    "    diff_binary = teste1 = teste2 = np.zeros(label.shape, dtype=np.uint8) # [H,W]\n",
    "\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    # teste1[mask_outward] = 1\n",
    "    # axes[0].imshow(teste1)\n",
    "    # axes[0].set_title('Image 1')\n",
    "    # axes[0].axis('off')\n",
    "    # teste2[mask_inward] = 1\n",
    "    # axes[1].imshow(teste2)\n",
    "    # axes[1].set_title('Image 2')\n",
    "    # axes[1].axis('off')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Comparar as áreas\n",
    "    if area_outward > area_inward:\n",
    "        diff_binary[mask_outward] = 1\n",
    "        point_type = 'positive'\n",
    "    else:\n",
    "        diff_binary[mask_inward] = 1\n",
    "        point_type = 'negative'\n",
    "    \n",
    "    return diff_binary, point_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute predict with prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_119588/1293084424.py:83: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  masks_logits = torch.tensor(outputs[0]['masks'].squeeze()).to(model.device)  # Remover a dimensão extra e mover para GPU\n",
      "/tmp/ipykernel_119588/1293084424.py:104: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, new_row], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['sample_id', 'facie_id', 'accumulated_point', 'iou', 'num_points'])\n",
    "\n",
    "# for each (image, label)...\n",
    "for idx, (image, label) in enumerate(zip(train_img_reader, train_label_reader)):\n",
    "    num_facies = np.unique(label) # num of facies\n",
    "    # print(\"facies: \", num_facies)\n",
    "\n",
    "    # DEBUG\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    # axes[0].imshow(image)\n",
    "    # axes[1].imshow(label)\n",
    "    # plt.show()\n",
    "    \n",
    "    # for each facie...\n",
    "    for i, facie in enumerate(num_facies):\n",
    "        region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "        region[label == facie] = 1\n",
    "        real_label = region\n",
    "\n",
    "        point_type = 'positive' # first point is positive\n",
    "        accumulated_coords, accumulated_labels = set_points()\n",
    "\n",
    "        # for each point...\n",
    "        for point in range(num_points):\n",
    "            # calculate center region\n",
    "            accumulated_coords, accumulated_labels, _, _ = calculate_center_region(\n",
    "                accumulated_coords, \n",
    "                accumulated_labels, \n",
    "                image.shape[:2],\n",
    "                region=region, \n",
    "                point_type=point_type)\n",
    "            \n",
    "            point_coords = torch.tensor(accumulated_coords).unsqueeze(0).to(model.device)\n",
    "            point_labels = torch.tensor(accumulated_labels).unsqueeze(0).to(model.device)\n",
    "            # print(point_coords, point_labels)\n",
    "\n",
    "            # convertendo para PNG (TODO SEM ISSO O SAM ORIGINAL NAO DÁ BONS RESULTADOS NO F3. NO PARIHAKA NAO TEM DIFERENCA)\n",
    "            if image.dtype != np.uint8:\n",
    "                tiff_image = ((image - image.min()) / (image.max() - image.min()) * 255).astype(np.uint8)\n",
    "            else:\n",
    "                tiff_image = image\n",
    "            _, png_img = cv2.imencode('.png', tiff_image)\n",
    "            decoded_image = cv2.imdecode(np.frombuffer(png_img, np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "            batch = [{\n",
    "                'image': torch.from_numpy(decoded_image).permute(2, 0, 1).float().to(model.device),\n",
    "                'label': torch.from_numpy(label).to(model.device),\n",
    "                'original_size': decoded_image.shape[:2],\n",
    "                'point_coords': point_coords,\n",
    "                'point_labels': point_labels\n",
    "            }]\n",
    "\n",
    "            # Inferência\n",
    "            outputs = model(batch, multimask_output=multimask_output)\n",
    "\n",
    "            # if multimask_output:\n",
    "            #     # stack logits 'masks_logits' and 'labels' for loss and metrics function\n",
    "            #     masks_logits = torch.stack([output['masks_logits'].squeeze(0) for output in outputs])  # [batch_size, num_classes, H, W]\n",
    "            #     labels = torch.stack([input['label'].squeeze(0) for input in batch])  # [batch_size, H, W]\n",
    "\n",
    "            #     for i in range(len(batch)):\n",
    "            #         pred = masks_logits[i].unsqueeze(0)\n",
    "            #         label = labels[i].unsqueeze(0)\n",
    "            #         pred = torch.argmax(pred, dim=1, keepdim=True).squeeze(1)\n",
    "            #         # (DEBUG) visualizar as imagens\n",
    "            #         pred = pred.squeeze(0).cpu().detach().numpy()  # Converte para numpy para plotar\n",
    "            #         label = label.squeeze(0).cpu().detach().numpy()  # Converte para numpy para plotar\n",
    "            #         fig, axs = plt.subplots(1, 2, figsize=(12, 6))  # Cria 2 colunas\n",
    "            #         axs[0].imshow(pred, cmap='gray')          # Exibe a máscara\n",
    "            #         axs[0].set_title(\"Mask Logits\")           # Título para a primeira imagem\n",
    "            #         axs[0].axis('off')                       # Remove os eixos da imagem\n",
    "            #         axs[1].imshow(label, cmap='gray')        # Exibe os labels\n",
    "            #         axs[1].set_title(\"Labels\")               # Título para a segunda imagem\n",
    "            #         axs[1].axis('off')                       # Remove os eixos da imagem\n",
    "            #         for ax in axs:\n",
    "            #             for (x, y), label in zip(accumulated_coords, accumulated_labels):\n",
    "            #                 color = 'green' if label == 1 else 'red'\n",
    "            #                 ax.scatter(x, y, color=color, s=50, edgecolors='white')\n",
    "\n",
    "            #         plt.tight_layout()\n",
    "            #         plt.show()\n",
    "            \n",
    "            masks_logits = torch.tensor(outputs[0]['masks'].squeeze()).to(model.device)  # Remover a dimensão extra e mover para GPU\n",
    "            labels = torch.tensor(real_label).to(model.device)  # Converta para tensor 2D e mova para GPU\n",
    "            \n",
    "            diff, new_point_type = calculate_diff_label_pred(label=real_label, pred=masks_logits.squeeze(0).cpu().numpy())\n",
    "\n",
    "            region = diff # [H,W], atualiza para a proxima regiao\n",
    "            point_type = new_point_type # 'positive' ou 'negative', atualiza para a proxima regiao\n",
    "\n",
    "            # Calcular o mIoU para a imagem i\n",
    "            iou_score = miou_metric.to(model.device)(masks_logits, labels)\n",
    "            # print(\"IoU: \", iou_score.item())\n",
    "\n",
    "            # salvando progresso\n",
    "            new_row = pd.DataFrame([{\n",
    "                'sample_id': idx,\n",
    "                'facie_id': facie,\n",
    "                'accumulated_point': point + 1,\n",
    "                'iou': iou_score.item(),\n",
    "                'num_points': num_points\n",
    "            }])\n",
    "\n",
    "            results = pd.concat([results, new_row], ignore_index=True)\n",
    "            \n",
    "            # plot_all(\n",
    "            #     image=image,\n",
    "            #     label=real_label,\n",
    "            #     pred=masks_logits.squeeze(0).cpu().numpy(),  # empilha as máscaras geradas\n",
    "            #     diff=diff,\n",
    "            #     score=iou_score,\n",
    "            #     point_coords=accumulated_coords,\n",
    "            #     point_labels=accumulated_labels\n",
    "            # )\n",
    "            \n",
    "            # break\n",
    "        # break\n",
    "    # break\n",
    "results.to_csv(f'{model_name_experiment}iou_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
