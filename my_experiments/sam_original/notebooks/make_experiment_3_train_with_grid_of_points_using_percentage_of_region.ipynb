{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Notebook\n",
    "- This notebook implements the experiment 3.\n",
    "- In the experiment 3, we use SAM model in your original version and:\n",
    "    - train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is promptable, i.e, we use 3 points for learn model where to segment.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 2\n",
    "num_epochs = 20\n",
    "ratio = 0.2\n",
    "batch_size = 3\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels\n",
    "            image = data_readers[0]\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords_positive, point_coords_negative = self.get_points_in_region(\n",
    "                    region=region, \n",
    "                    positive_percent=self.num_points * 4, \n",
    "                    negative_percent=self.num_points\n",
    "                )\n",
    "                self.samples.append((image, region, point_coords_positive, point_coords_negative))\n",
    "\n",
    "    def get_points_in_region(self, region, positive_percent=50, negative_percent=50):\n",
    "        \"\"\"\n",
    "        Processa a região binária e retorna os pontos positivos e negativos \n",
    "        com base na porcentagem informada.\n",
    "        \n",
    "        Parâmetros:\n",
    "            region (np.ndarray): Imagem binária (0 e 1) representando a região de interesse.\n",
    "            positive_percent (float): Porcentagem para definir a densidade da grade na região positiva (pixel 1).\n",
    "            negative_percent (float): Porcentagem para definir a densidade da grade na região negativa (pixel 0).\n",
    "            \n",
    "        Retorna:\n",
    "            tuple: (points_positive, points_negative)\n",
    "                - points_positive (np.ndarray): Array (N, 2) com as coordenadas (x, y) dos pontos positivos.\n",
    "                - points_negative (np.ndarray): Array (M, 2) com as coordenadas (x, y) dos pontos negativos.\n",
    "        \"\"\"\n",
    "        # Garantir que a matriz seja 2D\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)\n",
    "        \n",
    "        # Verifica se a região contém apenas 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not (np.array_equal(unique_values, [0, 1]) or\n",
    "                np.array_equal(unique_values, [1]) or\n",
    "                np.array_equal(unique_values, [0])):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "        \n",
    "        # Gera a grade de pontos para a região positiva (pixel 1)\n",
    "        points_positive = self._get_grid_points(region, target_value=1, percent=positive_percent)\n",
    "        \n",
    "        # Gera a grade de pontos para a região negativa (pixel 0)\n",
    "        points_negative = self._get_grid_points(region, target_value=0, percent=negative_percent)\n",
    "        \n",
    "        return points_positive, points_negative\n",
    "\n",
    "    def _get_grid_points(self, region, target_value, percent):\n",
    "        \"\"\"\n",
    "        Gera uma grade uniforme de pontos sobre o bounding box dos pixels que possuem o valor target_value,\n",
    "        com densidade definida pela porcentagem informada.\n",
    "        \n",
    "        Parâmetros:\n",
    "            region (np.ndarray): Imagem binária.\n",
    "            target_value (int): Valor alvo da região (0 ou 1).\n",
    "            percent (float): Porcentagem que define a densidade da grade (ex.: 50 para 50%).\n",
    "            \n",
    "        Retorna:\n",
    "            np.ndarray: Array (N, 2) com as coordenadas (x, y) dos pontos que pertencem à região target.\n",
    "        \"\"\"\n",
    "        # Obter as coordenadas dos pixels que correspondem ao target_value\n",
    "        coords = np.argwhere(region == target_value)\n",
    "        if coords.size == 0:\n",
    "            return np.empty((0, 2), dtype=int)\n",
    "        \n",
    "        # Determinar o bounding box dos pixels do target\n",
    "        y_min, x_min = coords.min(axis=0)\n",
    "        y_max, x_max = coords.max(axis=0)\n",
    "        \n",
    "        # Calcula o número de pontos em cada direção com base na porcentagem\n",
    "        # Garantindo que haja pelo menos 2 pontos para formar uma grade\n",
    "        n_x = max(2, int(round((x_max - x_min + 1) * (percent / 100.0))))\n",
    "        n_y = max(2, int(round((y_max - y_min + 1) * (percent / 100.0))))\n",
    "        \n",
    "        # Gera os valores de x e y de forma equidistante dentro do bounding box\n",
    "        x_vals = np.linspace(x_min, x_max, n_x)\n",
    "        y_vals = np.linspace(y_min, y_max, n_y)\n",
    "        xv, yv = np.meshgrid(x_vals, y_vals)\n",
    "        \n",
    "        # Converte para inteiros (índices de pixels) e organiza os pontos em (x, y)\n",
    "        grid_points = np.vstack([xv.ravel(), yv.ravel()]).T.astype(int)\n",
    "        \n",
    "        # Filtra para manter apenas os pontos que estão de fato na região target\n",
    "        valid_points = [pt for pt in grid_points if region[pt[1], pt[0]] == target_value]\n",
    "        \n",
    "        return np.array(valid_points)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords_positive, point_coords_negative = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels positives for add with prompt to SAM\n",
    "        points_positives = [[x, y] for (x, y) in point_coords_positive]\n",
    "        labels_positives = [1] * len(points_positives)\n",
    "\n",
    "        # preparing points and labels negative for add with prompt to SAM\n",
    "        points_negatives = [[x, y] for (x, y) in point_coords_negative]\n",
    "        labels_negatives = [0] * len(points_negatives)\n",
    "\n",
    "        # concatenate points positives and negatives\n",
    "        combined_points = points_positives + points_negatives\n",
    "        combined_labels = labels_positives + labels_negatives\n",
    "\n",
    "        # convert to tensors\n",
    "        points = torch.tensor(combined_points, dtype=torch.long).unsqueeze(0)\n",
    "        labels = torch.tensor(combined_labels, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        # points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        # labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    test_train_dataloader = get_train_dataloader(data_module)\n",
    "    print(\"Total batches: \", len(test_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_batch = next(iter(test_train_dataloader))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape} - type: {type(train_batch[0]['image'])}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape} - type: {type(train_batch[0]['label'])}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']} - type: {type(train_batch[0]['original_size'])}\")\n",
    "    print(f\"Train batch point_coords shape: {train_batch[0]['point_coords'].shape} - type: {type(train_batch[0]['point_coords'])}\")\n",
    "    print(f\"Train batch point_labels shape: {train_batch[0]['point_labels'].shape} - type: {type(train_batch[0]['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_batch = next(iter(test_train_dataloader))\n",
    "\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    print(\"shape da image: \", train_batch[0]['image'].shape)\n",
    "    print(\"intervalo da image: \", torch.min(train_batch[0]['image']), torch.max(train_batch[0]['image']))\n",
    "    print(\"shape da label: \", train_batch[0]['label'].shape)\n",
    "    # image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    # label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    points = train_batch[0]['point_coords'] # Lista de coordenadas (shape: torch.Size([1, 10, 2]))\n",
    "    # print(points.shape, points)\n",
    "    point_labels = train_batch[0]['point_labels'] # Lista de coordenadas (shape: torch.Size([1, 10]))\n",
    "    # print(point_labels.shape, point_labels)\n",
    "    image = train_batch[0]['image'].permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plotando os pontos na imagem\n",
    "    for point, point_label in zip(points, point_labels):\n",
    "        for i, coord in enumerate(point):\n",
    "            y, x = coord\n",
    "            if point_label[i] == 0:\n",
    "                axes[0].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "            else:\n",
    "                axes[0].scatter(y, x, color='green', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # plotando os pontos na label\n",
    "    for point, point_label in zip(points, point_labels):\n",
    "        for i, coord in enumerate(point):\n",
    "            y, x = coord\n",
    "            if point_label[i] == 0:\n",
    "                axes[1].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "            else:\n",
    "                axes[1].scatter(y, x, color='green', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=nn.BCEWithLogitsLoss()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30/run_2025-03-03-20-24-57db61ecf2d06647e09c5f59e867173465.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_30 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 93.7 M | train\n",
      "-----------------------------------------------------\n",
      "93.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.7 M    Total params\n",
      "374.942   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:01<00:01,  0.99it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 436/436 [05:44<00:00,  1.27it/s, v_num=30, train_loss_step=0.00974, train_mIoU_step=0.991, val_loss_step=0.0417, val_mIoU_step=0.948, val_loss_epoch=0.0492, val_mIoU_epoch=0.951, train_loss_epoch=0.0056, train_mIoU_epoch=0.991] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 436/436 [05:44<00:00,  1.27it/s, v_num=30, train_loss_step=0.00974, train_mIoU_step=0.991, val_loss_step=0.0417, val_mIoU_step=0.948, val_loss_epoch=0.0492, val_mIoU_epoch=0.951, train_loss_epoch=0.0056, train_mIoU_epoch=0.991]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30/run_2025-03-03-20-24-57db61ecf2d06647e09c5f59e867173465.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_3_{ratio}_{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30/run_2025-03-03-20-24-57db61ecf2d06647e09c5f59e867173465.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  22%|██▏       | 89/399 [00:22<01:19,  3.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 399/399 [01:42<00:00,  3.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.05715259164571762    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9385291934013367     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.05715259164571762   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9385291934013367    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30/run_2025-03-03-20-24-57db61ecf2d06647e09c5f59e867173465.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.05715259164571762,\n",
       "  'test_mIoU_epoch': 0.9385291934013367}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30/run_2025-03-03-20-24-57db61ecf2d06647e09c5f59e867173465.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   6%|▌         | 23/399 [00:06<01:42,  3.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 399/399 [01:45<00:00,  3.77it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_30/run_2025-03-03-20-24-57db61ecf2d06647e09c5f59e867173465.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
