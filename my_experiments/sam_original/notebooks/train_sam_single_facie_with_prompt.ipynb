{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Notebook\n",
    "- Task: train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "model_name = \"SAM-ViT_B_with_prompts\"\n",
    "vit_model = 'vit-b'\n",
    "height, width = 255, 701 # f3\n",
    "# height, width = 1006, 590 # parihaka\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 15\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM\n",
    "- have prompts implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            image = data_readers[0]\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_train_dataloader(data_module):\n",
    "#     data_module.setup(\"fit\")\n",
    "#     return data_module.train_dataloader()\n",
    "\n",
    "# test_train_dataloader = get_train_dataloader(data_module)\n",
    "# print(\"Total batches: \", len(test_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = next(iter(test_train_dataloader))\n",
    "# print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape} - type: {type(train_batch[0]['image'])}\")\n",
    "# print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape} - type: {type(train_batch[0]['label'])}\")\n",
    "# print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']} - type: {type(train_batch[0]['original_size'])}\")\n",
    "# print(f\"Train batch point_coords shape: {train_batch[0]['point_coords'].shape} - type: {type(train_batch[0]['point_coords'])}\")\n",
    "# print(f\"Train batch point_labels shape: {train_batch[0]['point_labels'].shape} - type: {type(train_batch[0]['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_batch = next(iter(test_train_dataloader))\n",
    "\n",
    "# # Obtendo a imagem e a label do batch\n",
    "# print(\"shape da image: \", train_batch[0]['image'].shape)\n",
    "# print(\"shape da label: \", train_batch[0]['label'].shape)\n",
    "# # image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "# # label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "# # Transformando para formato adequado para matplotlib\n",
    "# points = train_batch[0]['point_coords'] # Lista de coordenadas (x, y)\n",
    "# print(points)\n",
    "# image = train_batch[0]['image'].permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "# label = train_batch[0]['label'].squeeze(0)  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "# # Plotando a imagem e a label\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# # Imagem original\n",
    "# axes[0].imshow(image)\n",
    "# axes[0].set_title(\"Imagem\")\n",
    "# axes[0].axis('off')\n",
    "\n",
    "# # Plotando os pontos na imagem\n",
    "# for point in points:\n",
    "#     for y, x in point:\n",
    "#         axes[0].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "# # Label (provavelmente uma máscara ou rótulo binário)\n",
    "# axes[1].imshow(label, cmap='gray')\n",
    "# axes[1].set_title(\"Label\")\n",
    "# axes[1].axis('off')\n",
    "\n",
    "# # plotando os pontos na label\n",
    "# for point in points:\n",
    "#     for y, x in point:\n",
    "#         axes[1].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2343: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0/run_2025-01-21-23-36-35f23fc62342ae4d87a66671195d7a2c14.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_0 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 93.7 M | train\n",
      "-----------------------------------------------------\n",
      "93.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.7 M    Total params\n",
      "374.942   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2557/2557 [13:28<00:00,  3.16it/s, v_num=0, train_loss_step=0.000395, train_mIoU_step=0.983, val_loss_step=0.00137, val_mIoU_step=0.991, val_loss_epoch=0.00526, val_mIoU_epoch=0.990, train_loss_epoch=0.00444, train_mIoU_epoch=0.991]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 2557/2557 [13:35<00:00,  3.14it/s, v_num=0, train_loss_step=0.000395, train_mIoU_step=0.983, val_loss_step=0.00137, val_mIoU_step=0.991, val_loss_epoch=0.00526, val_mIoU_epoch=0.990, train_loss_epoch=0.00444, train_mIoU_epoch=0.991]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0/run_2025-01-21-23-36-35f23fc62342ae4d87a66671195d7a2c14.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    # monitor=\"val_loss\", # Métrica para monitorar\n",
    "    save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam-{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    # save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    # mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0/run_2025-01-21-23-36-35f23fc62342ae4d87a66671195d7a2c14.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1138/1138 [02:06<00:00,  9.02it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.10471148043870926    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9132229089736938     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.10471148043870926   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9132229089736938    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0/run_2025-01-21-23-36-35f23fc62342ae4d87a66671195d7a2c14.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.10471148043870926,\n",
       "  'test_mIoU_epoch': 0.9132229089736938}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0/run_2025-01-21-23-36-35f23fc62342ae4d87a66671195d7a2c14.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   3%|▎         | 38/1138 [00:04<02:02,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1138/1138 [02:06<00:00,  9.00it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_0/run_2025-01-21-23-36-35f23fc62342ae4d87a66671195d7a2c14.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
