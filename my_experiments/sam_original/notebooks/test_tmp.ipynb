{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.1+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "model_name = \"SAM-ViT_B_with_prompts\"\n",
    "vit_model = 'vit-b'\n",
    "height, width = 255, 701 # f3\n",
    "# height, width = 1006, 590 # parihaka\n",
    "multimask_output=False # estamos fazendo inferencia para o acerto de 1 unica mascara (no SAM, a melhor mascara possivel dado as num_classes possiveis)\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 15\n",
    "batch_size = 1 # deixa sempre como 1 de preferencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all(image, label, pred, diff, score, point_coords, point_labels):\n",
    "    \"\"\"\n",
    "    Plota as imagens lado a lado: imagem original, label, predição, diff.\n",
    "    Pontos acumulados são exibidos sobre as imagens.\n",
    "    \"\"\"\n",
    "    num_subplots = 4  # Número de subplots: imagem original, label, pred, diff\n",
    "    plt.clf()\n",
    "    fig, axes = plt.subplots(1, num_subplots, figsize=(5 * num_subplots, 5))\n",
    "\n",
    "    # Plot 1: Imagem original\n",
    "    axes[0].imshow(image, cmap='gray')\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plot 2: Label\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Plot 3: Predição acumulada\n",
    "    axes[2].imshow(pred, cmap='gray')\n",
    "    axes[2].set_title(f\"Pred - Score: {score}\")\n",
    "    axes[2].axis('off')\n",
    "\n",
    "    # Plot 4: Diferença entre label e pred\n",
    "    axes[3].imshow(diff, cmap='gray')\n",
    "    axes[3].set_title(\"Difference (Label - Pred)\")\n",
    "    axes[3].axis('off')\n",
    "\n",
    "    # Adiciona os pontos em todas as imagens\n",
    "    for ax in axes:\n",
    "        for (x, y), label in zip(point_coords, point_labels):\n",
    "            color = 'green' if label == 1 else 'red'\n",
    "            ax.scatter(x, y, color=color, s=50, edgecolors='white')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset (ignore prompts and return full label and your image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# versao que usa só o dict da imagem, label e o size da imagem\n",
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom Dataset to use properties that needed in images when send some image to SAM model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        readers: List[_Reader]\n",
    "            List of data readers. It must contain exactly 2 readers.\n",
    "            The first reader for the input data and the second reader for the\n",
    "            target data.\n",
    "        transforms: Optional[_Transform]\n",
    "            Optional data transformation pipeline.\n",
    "        transform_coords_input: Optional[dict] \n",
    "            List with transforms to apply.\n",
    "                point_coords (np.ndarray or None): A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.\n",
    "                point_labels (np.ndarray or None): A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.\n",
    "    \"\"\"\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        # assert (\n",
    "        #     len(self.readers) == len(self.transforms)\n",
    "        #     and len(self.transforms) == len(self.transform_coords_input)\n",
    "        #     and len(self.readers) == len(self.transform_coords_input)\n",
    "        # ), \"DatasetForSAM requires exactly iquals lens (readers, transforms and transform_coords_input)\"\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Load data and return data with SAM format (dict), where dict has:\n",
    "        'image' (required): The image as a torch tensor in 3xHxW format.\n",
    "        'label' (required): The label of the image.\n",
    "        'original_size' (required): The original size of the image before transformation.\n",
    "        'point_coords' (optional): (torch.Tensor) Batched point prompts for this image, with shape BxNx2. Already transformed to the input frame of the model.\n",
    "        'point_labels' (optional): (torch.Tensor) Batched labels for point prompts, with shape BxN. (0 is background, 1 is object and -1 is pad)\n",
    "        'boxes' (optional): (torch.Tensor) Batched box inputs, with shape Bx4.  Already transformed to the input frame of the model.\n",
    "        'mask_inputs' (optional): (torch.Tensor) Batched mask inputs to the model, in the form Bx1xHxW.\n",
    "        \"\"\"\n",
    "\n",
    "        data_readers = []\n",
    "        for reader, transform in zip(self.readers, self.transforms):\n",
    "            sample = reader[index]\n",
    "            if transform is not None:\n",
    "                sample = transform(sample)\n",
    "            data_readers.append(sample)\n",
    "        \n",
    "        data = {\n",
    "            'image': data_readers[0],\n",
    "            'label': data_readers[1],\n",
    "            'original_size': (int(data_readers[0].shape[1]), int(data_readers[0].shape[2]))\n",
    "        }\n",
    "    \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "        print(\"num_workers: \", self.num_workers)\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workers:  16\n"
     ]
    }
   ],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2343: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Encoder freeze!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(\"Using device: \", device)\n",
    "\n",
    "# experiment 1\n",
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    # apply_freeze=apply_freeze,\n",
    "    # apply_adapter=apply_adapter\n",
    ").to(device)\n",
    "\n",
    "# experiment 3\n",
    "# model = Sam(\n",
    "#     vit_type=vit_model,\n",
    "#     checkpoint=checkpoint_path,\n",
    "#     num_multimask_outputs=num_classes, # default: 3\n",
    "#     iou_head_depth=num_classes, # default: 3\n",
    "#     apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "#     # apply_adapter=apply_adapter,\n",
    "#     train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "#     val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "#     test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "#     # multimask_output=multimask_output,\n",
    "#     # loss_fn=DiceCELoss() # if multimask_output is false\n",
    "# ).to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AUC_calculate_v1():\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 dataloader,\n",
    "                 num_points:int=3 ,\n",
    "                 experiment_num:int=0,\n",
    "                 ):\n",
    "        self.model = model\n",
    "        self.model.eval() # seta para inferencia\n",
    "        self.dataloader = dataloader\n",
    "        self.num_points = num_points\n",
    "        self.experiment_num = experiment_num\n",
    "        self.set_points() # reseta pontos\n",
    "\n",
    "        self.miou_metric = JaccardIndex(task=\"multiclass\", num_classes=2).to(self.model.device) # binario\n",
    "    \n",
    "    def set_points(self):\n",
    "        # Inicialize os acumuladores como arrays vazios\n",
    "        self.accumulated_coords = np.empty((0, 2), dtype=int)  # Nx2 array\n",
    "        self.accumulated_labels = np.empty((0,), dtype=int)   # Array de comprimento N\n",
    "    \n",
    "    def process(self):\n",
    "        self.results = pd.DataFrame(columns=['sample_id', 'facie_id', 'accumulated_point', 'iou', 'num_points'])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # for batch_idx, batch in enumerate(self.dataloader):\n",
    "            for batch_idx, batch in enumerate(tqdm(self.dataloader, desc=\"Processando batches\")):\n",
    "                for item in batch:\n",
    "                    img = item['image']\n",
    "                    # print(f\"Intervalo original: min={img.min()}, max={img.max()}\")\n",
    "                    if img.shape[0] == 1:\n",
    "                        img = img.repeat(3, 1, 1)\n",
    "                    img = (img * 255).clamp(0, 255).to(torch.uint8)\n",
    "                    # print(f\"Intervalo após multiplicação por 255: min={img.min()}, max={img.max()}\")\n",
    "                    # _, png_img = cv2.imencode('.png', img)\n",
    "                    # decoded_image = cv2.imdecode(np.frombuffer(png_img, np.uint8), cv2.IMREAD_UNCHANGED)\n",
    "                    # decoded_image = torch.from_numpy(decoded_image).permute(2, 0, 1).float()\n",
    "                    image = img.to(self.model.device)\n",
    "                    label = item['label'].squeeze(0).numpy()#.to(self.model.device)\n",
    "                #     plt.figure(figsize=(6, 6))  # Define o tamanho da figura\n",
    "                #     plt.imshow(label, cmap='gray')  # Mostra a imagem, com escala de cinza (ou RGB se for colorida)\n",
    "                #     plt.title(\"label\")  # Adiciona o título\n",
    "                #     plt.axis('off')  # Desliga os eixos para uma visualização limpa\n",
    "                #     plt.show()\n",
    "                #     break\n",
    "                # break\n",
    "\n",
    "                    num_facies = np.unique(label) # num de facies da amostra\n",
    "                    point_type = 'positive' # inicia com ponto positivo, depois pode mudar para 'negative'\n",
    "\n",
    "                    for i, facie in enumerate(num_facies):\n",
    "                        region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                        region[label == facie] = 1\n",
    "                        real_label = region\n",
    "                        # plt.figure(figsize=(6, 6))  # Define o tamanho da figura\n",
    "                        # plt.imshow(region, cmap='gray')  # Mostra a imagem, com escala de cinza (ou RGB se for colorida)\n",
    "                        # plt.title(\"region\")  # Adiciona o título\n",
    "                        # plt.axis('off')  # Desliga os eixos para uma visualização limpa\n",
    "                        # plt.show()\n",
    "                        # break\n",
    "\n",
    "                        for point in range(self.num_points):\n",
    "                            # calculando centro da regiao e retornando sua coordenada\n",
    "                            point_coords, point_labels = self.calculate_center_region(region=region, point_type=point_type)\n",
    "                        \n",
    "                            # definindo amostra (batch) a ser inserido no modelo\n",
    "                            batch = {\n",
    "                                'image': image,\n",
    "                                'label': label,\n",
    "                                'original_size': (int(image.shape[1]), int(image.shape[2])),\n",
    "                                'point_coords': torch.tensor(point_coords, dtype=torch.long).unsqueeze(0).to(self.model.device),\n",
    "                                'point_labels': torch.tensor(point_labels, dtype=torch.long).unsqueeze(0).to(self.model.device)\n",
    "                            }\n",
    "                        \n",
    "                            # Inferência\n",
    "                            outputs = self.model([batch], multimask_output=multimask_output) # batch tem que ser uma lista de dict. multimask_output é dado no init do model\n",
    "                            \n",
    "                            # calcular IoU\n",
    "                            gt_tensor = torch.tensor(real_label).to(self.model.device)  # Converta para tensor 2D e mova para GPU\n",
    "                            pred_tensor = torch.tensor(outputs[0]['masks'].squeeze()).to(self.model.device)  # Remover a dimensão extra e mover para GPU\n",
    "                            \n",
    "                            iou_score = self.miou_metric(pred_tensor, gt_tensor)\n",
    "\n",
    "                            # print(\"real_label shape: \", real_label.shape)\n",
    "                            # print('pred shape: ', outputs[0]['masks'].squeeze().numpy().shape)\n",
    "                            diff, new_point_type = self.calculate_diff_label_pred(label=real_label, pred=outputs[0]['masks'].squeeze().cpu().numpy())\n",
    "                        \n",
    "                            # salvando progresso\n",
    "                            new_row = pd.DataFrame([{\n",
    "                                'sample_id': batch_idx,\n",
    "                                'facie_id': facie,\n",
    "                                'accumulated_point': point + 1,\n",
    "                                'iou': iou_score.item(),\n",
    "                                'num_points': self.num_points\n",
    "                            }])\n",
    "\n",
    "                            self.results = pd.concat([self.results, new_row], ignore_index=True)\n",
    "                        \n",
    "                            # plot experimental a cada 50 amostras\n",
    "                            # if idx % 50 == 0:\n",
    "                            # plot_all(\n",
    "                            #     image=image.permute(1, 2, 0).cpu(),\n",
    "                            #     label=real_label,\n",
    "                            #     pred=outputs[0]['masks'].squeeze().cpu().numpy(),\n",
    "                            #     diff=diff,\n",
    "                            #     score=iou_score,\n",
    "                            #     point_coords=self.accumulated_coords,\n",
    "                            #     point_labels=self.accumulated_labels\n",
    "                            # )\n",
    "                            region = diff # [H,W], atualiza para a proxima regiao\n",
    "                            point_type = new_point_type # 'positive' ou 'negative', atualiza para a proxima regiao\n",
    "                            # break # testa só 1 ponto\n",
    "                        point_type = 'positive' # reinicia tipo do primeiro ponto (sempre deve ser positivo o primeiro)\n",
    "                        self.set_points() # reinicia empilhamento de pontos\n",
    "                        # break # testa só 1 facie\n",
    "                    # break # testar só uma amostra\n",
    "                # break # testar só uma amostra\n",
    "            \n",
    "            os.makedirs('save_test', exist_ok=True)\n",
    "            self.results.to_csv(f'save_test/iou_results_{self.experiment_num}.csv', index=False)\n",
    "\n",
    "    def calculate_center_region(self, region: np.array, point_type: str, min_distance: int = 10):\n",
    "        \"\"\"\n",
    "        Calcula o centroide da maior região de pixels brancos de uma imagem binária,\n",
    "        deslocando horizontalmente o ponto se ele estiver próximo demais dos acumulados.\n",
    "\n",
    "        Args:\n",
    "            region (np.array): Imagem binária com a região de interesse (pixels brancos).\n",
    "            point_type (str): Tipo do ponto ('positive' ou 'negative').\n",
    "            min_distance (int): Distância mínima permitida entre pontos.\n",
    "\n",
    "        Returns:\n",
    "            point_coords (np.ndarray): Array Nx2 de pontos acumulados.\n",
    "            point_labels (np.ndarray): Array N de rótulos acumulados.\n",
    "        \"\"\"\n",
    "        if not isinstance(region, np.ndarray):\n",
    "            raise TypeError(\"region needs to be a NumPy array.\")\n",
    "        \n",
    "        # Encontrar as componentes conectadas\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(region, connectivity=8)\n",
    "\n",
    "        if num_labels < 2:  # Apenas fundo e nenhuma região branca\n",
    "            raise ValueError(\"No connected white regions found in the binary image.\")\n",
    "        \n",
    "        # Ignorar o rótulo 0 (fundo), pegar a maior componente conectada\n",
    "        largest_label = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "        center_x, center_y = centroids[largest_label]\n",
    "        center_x, center_y = int(center_x), int(center_y)\n",
    "        new_coords = np.array([[center_x, center_y]])\n",
    "\n",
    "        # Verificar se o ponto está muito próximo dos anteriores\n",
    "        if self.accumulated_coords.shape[0] > 0:\n",
    "            distances = np.sqrt(np.sum((self.accumulated_coords - new_coords) ** 2, axis=1))\n",
    "            if np.any(distances < min_distance):\n",
    "                # print(\"Ponto muito próximo ao anterior, deslocando horizontalmente...\")\n",
    "                \n",
    "                # Tentar deslocar o ponto horizontalmente dentro da região branca\n",
    "                region_height, region_width = region.shape\n",
    "                # Tenta deslocar horizontalmente usando o min_distance\n",
    "                for delta_x in range(min_distance, region_width, min_distance):  # Incrementa em min_distance\n",
    "                    candidate_x_right = center_x + delta_x\n",
    "                    candidate_x_left = center_x - delta_x\n",
    "\n",
    "                    # Verifica primeiro para direita, depois para esquerda\n",
    "                    if candidate_x_right < region_width and region[center_y, candidate_x_right] > 0:\n",
    "                        center_x = candidate_x_right\n",
    "                        break\n",
    "                    elif candidate_x_left >= 0 and region[center_y, candidate_x_left] > 0:\n",
    "                        center_x = candidate_x_left\n",
    "                        break\n",
    "\n",
    "                new_coords = np.array([[center_x, center_y]])\n",
    "\n",
    "        # Definir o rótulo (positivo ou negativo)\n",
    "        if point_type == 'positive':\n",
    "            new_labels = np.array([1])\n",
    "        elif point_type == 'negative':\n",
    "            new_labels = np.array([0])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid point_type. Must be 'positive' or 'negative'.\")\n",
    "\n",
    "        # Acumular os resultados\n",
    "        self.accumulated_coords = np.vstack([self.accumulated_coords, new_coords])\n",
    "        self.accumulated_labels = np.hstack([self.accumulated_labels, new_labels])\n",
    "\n",
    "        return self.accumulated_coords, self.accumulated_labels\n",
    "    \n",
    "    def calculate_diff_label_pred(self, label:np.array, pred:np.array):\n",
    "        \"\"\"\n",
    "        Calcula a diferença entre duas imagens binárias e determina se a área externa ou interna é maior.\n",
    "\n",
    "        Args:\n",
    "            label (np.array): Imagem binária de referência (label).\n",
    "            pred (np.array): Imagem binária predita (pred).\n",
    "\n",
    "        Returns:\n",
    "            diff_colored (np.array): Imagem colorida representando as diferenças.\n",
    "            point_type (str): 'negative' se a área externa for maior, 'positive' se a interna for maior.\n",
    "        \"\"\"\n",
    "        if label.shape != pred.shape:\n",
    "            raise ValueError(\"Label and Pred images have differents shapes. Check it before call calculate_dif_label_pred() function.\")\n",
    "\n",
    "        # Máscaras para regiões de diferença\n",
    "        mask_outward = (label > pred)  # Diferença para fora -> Vermelho\n",
    "        mask_inward = (label < pred)  # Diferença para dentro -> Azul\n",
    "\n",
    "        area_outward = np.sum(mask_outward)\n",
    "        area_inward = np.sum(mask_inward)\n",
    "\n",
    "        diff_binary = teste1 = teste2 = np.zeros(label.shape, dtype=np.uint8) # [H,W]\n",
    "\n",
    "        # fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        # teste1[mask_outward] = 1\n",
    "        # axes[0].imshow(teste1)\n",
    "        # axes[0].set_title('Image 1')\n",
    "        # axes[0].axis('off')\n",
    "        # teste2[mask_inward] = 1\n",
    "        # axes[1].imshow(teste2)\n",
    "        # axes[1].set_title('Image 2')\n",
    "        # axes[1].axis('off')\n",
    "        # plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        # Comparar as áreas\n",
    "        if area_outward > area_inward:\n",
    "            diff_binary[mask_outward] = 1\n",
    "            point_type = 'positive'\n",
    "        else:\n",
    "            diff_binary[mask_inward] = 1\n",
    "            point_type = 'negative'\n",
    "        \n",
    "        return diff_binary, point_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## exec test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup(stage='predict')  # Configura os dados para inferência\n",
    "miou_metric = JaccardIndex(task=\"multiclass\", num_classes=num_classes) # Inicializando a métrica de mIoU\n",
    "predict_dataloader = data_module.predict_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processando batches:   0%|          | 0/400 [00:00<?, ?it/s]/tmp/ipykernel_213334/408153985.py:80: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred_tensor = torch.tensor(outputs[0]['masks'].squeeze()).to(self.model.device)  # Remover a dimensão extra e mover para GPU\n",
      "/tmp/ipykernel_213334/408153985.py:97: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  self.results = pd.concat([self.results, new_row], ignore_index=True)\n",
      "Processando batches: 100%|██████████| 400/400 [21:10<00:00,  3.18s/it]\n"
     ]
    }
   ],
   "source": [
    "algorithm = AUC_calculate_v1(\n",
    "    model=model,\n",
    "    dataloader=predict_dataloader,\n",
    "    num_points=10,\n",
    ")\n",
    "\n",
    "algorithm.process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
