{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 - Notebook\n",
    "- This notebook implements the experiment 2.\n",
    "- In the experiment 2, we use SAM model in your original version and:\n",
    "    - train a model for segment one of N seismic facies (espectialist model for a single facie)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is automatic, i.e, not used prompt encoder during finetune.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "model_name = \"SAM_ViT_B_f3\"\n",
    "height, width = 255, 701 # f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "# model_name = \"SAM_ViT_B_parihaka\"\n",
    "# height, width = 1006, 590 # parihaka\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "num_classes = 3\n",
    "facie = 2 # from 0 to 5\n",
    "num_epochs = 20\n",
    "ratio = 0.1\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            select_facie:int=0\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "        self.select_facie = select_facie # pode ser: 0 (facie mais escura), a 5 (facie mais clara)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data_readers = []\n",
    "        for reader, transform in zip(self.readers, self.transforms):\n",
    "            sample = reader[index]\n",
    "            if transform is not None:\n",
    "                sample = transform(sample)\n",
    "            data_readers.append(sample)\n",
    "\n",
    "        # normalize and add 3 channels\n",
    "        image = data_readers[0]\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "        label = data_readers[1]\n",
    "\n",
    "        # Gera uma máscara binária apenas para a fácie selecionada\n",
    "        binary_mask = (label == self.select_facie).to(torch.uint8)\n",
    "\n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': binary_mask,\n",
    "            'original_size': (int(image.shape[1]), int(image.shape[2])),\n",
    "            'class_id': self.select_facie\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        select_facie:int=0,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.select_facie = select_facie\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                select_facie=self.select_facie\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                select_facie=self.select_facie\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                select_facie=self.select_facie\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    select_facie=facie,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    print(\"Total batches: \", len(get_train_dataloader(data_module)))\n",
    "\n",
    "    train_batch = next(iter(get_train_dataloader(data_module)))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']}\")\n",
    "\n",
    "    for idx, batch in enumerate(get_train_dataloader(data_module)):\n",
    "        print(f\"Batch {idx}:\")\n",
    "        print(f\"Tipo do batch: {type(batch)}\")\n",
    "        print(f\"Tamanho do batch: {len(batch)}\")  # Deve ser igual ao batch_size\n",
    "        print(\"Estrutura do primeiro item do batch:\")\n",
    "        # print(batch[0])  # Exibe o primeiro dicionário do batch\n",
    "        print(f\"Shape da imagem no primeiro item: {batch[0]['image'].shape}\")\n",
    "        \n",
    "        print(20*'-')\n",
    "\n",
    "        print(f\"Train batch image (X) shape: {batch[0]['image'].shape}\")\n",
    "        print(f\"Train batch label (Y) shape: {batch[0]['label'].shape}\")\n",
    "        print(f\"Train batch label (original_size) shape: {batch[0]['original_size']}\")\n",
    "        break  # Para após o primeiro batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = label.squeeze(0).cpu().numpy()  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Encoder freeze!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    # apply_freeze=apply_freeze,\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  (1,024)\n",
       "│    │    └─Embedding: 3-6                                   (256)\n",
       "│    │    └─Sequential: 3-7                                  (4,684)\n",
       "│    │    └─Embedding: 3-8                                   (256)\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,729,252\n",
       "Non-trainable params: 6,220\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-17-18-25-130ff95d02bdc9422b8a66566e7879e897.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_2 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 93.7 M | train\n",
      "-----------------------------------------------------\n",
      "93.7 M    Trainable params\n",
      "6.2 K     Non-trainable params\n",
      "93.7 M    Total params\n",
      "374.942   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 99/99 [00:22<00:00,  4.31it/s, v_num=2, train_loss_step=0.0157, train_mIoU_step=0.987, val_loss_step=0.0223, val_mIoU_step=0.982, val_loss_epoch=0.0261, val_mIoU_epoch=0.980, train_loss_epoch=0.0141, train_mIoU_epoch=0.988] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 99/99 [00:22<00:00,  4.31it/s, v_num=2, train_loss_step=0.0157, train_mIoU_step=0.987, val_loss_step=0.0223, val_mIoU_step=0.982, val_loss_epoch=0.0261, val_mIoU_epoch=0.980, train_loss_epoch=0.0141, train_mIoU_epoch=0.988]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-17-18-25-130ff95d02bdc9422b8a66566e7879e897.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_2_{ratio}_using_facie_{facie}-{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-17-18-25-130ff95d02bdc9422b8a66566e7879e897.yaml\n",
      "Testing DataLoader 0: 100%|██████████| 400/400 [00:22<00:00, 17.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.13088306784629822    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9336490631103516     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.13088306784629822   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9336490631103516    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-17-18-25-130ff95d02bdc9422b8a66566e7879e897.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.13088306784629822,\n",
       "  'test_mIoU_epoch': 0.9336490631103516}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-17-18-25-130ff95d02bdc9422b8a66566e7879e897.yaml\n",
      "Predicting DataLoader 0: 100%|██████████| 400/400 [00:21<00:00, 18.62it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-17-18-25-130ff95d02bdc9422b8a66566e7879e897.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[9][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 9º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 9:\n",
    "        pred_9 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_9[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
