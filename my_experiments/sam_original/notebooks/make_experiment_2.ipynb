{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2 - Notebook\n",
    "- This notebook implements the experiment 2.\n",
    "- In the experiment 2, we use SAM model in your original version and:\n",
    "    - train a model for segment one of N seismic facies (espectialist model for a single facie)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is automatic, i.e, not used prompt encoder during finetune.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "num_classes = 3\n",
    "facie = 1 # from 0 to 5\n",
    "num_epochs = 20\n",
    "ratio = 1.0\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            select_facie:int=0\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "        self.select_facie = select_facie # pode ser: 0 (facie mais escura), a 5 (facie mais clara)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        data_readers = []\n",
    "        for reader, transform in zip(self.readers, self.transforms):\n",
    "            sample = reader[index]\n",
    "            if transform is not None:\n",
    "                sample = transform(sample)\n",
    "            data_readers.append(sample)\n",
    "\n",
    "        # normalize and add 3 channels\n",
    "        image = data_readers[0]\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "        label = data_readers[1]\n",
    "\n",
    "        # Gera uma máscara binária apenas para a fácie selecionada\n",
    "        binary_mask = (label == self.select_facie).to(torch.uint8)\n",
    "\n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': binary_mask,\n",
    "            'original_size': (int(image.shape[1]), int(image.shape[2])),\n",
    "            'class_id': self.select_facie\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        select_facie:int=0,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.select_facie = select_facie\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                select_facie=self.select_facie\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                select_facie=self.select_facie\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                select_facie=self.select_facie\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    select_facie=facie,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    print(\"Total batches: \", len(get_train_dataloader(data_module)))\n",
    "\n",
    "    train_batch = next(iter(get_train_dataloader(data_module)))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']}\")\n",
    "\n",
    "    for idx, batch in enumerate(get_train_dataloader(data_module)):\n",
    "        print(f\"Batch {idx}:\")\n",
    "        print(f\"Tipo do batch: {type(batch)}\")\n",
    "        print(f\"Tamanho do batch: {len(batch)}\")  # Deve ser igual ao batch_size\n",
    "        print(\"Estrutura do primeiro item do batch:\")\n",
    "        # print(batch[0])  # Exibe o primeiro dicionário do batch\n",
    "        print(f\"Shape da imagem no primeiro item: {batch[0]['image'].shape}\")\n",
    "        \n",
    "        print(20*'-')\n",
    "\n",
    "        print(f\"Train batch image (X) shape: {batch[0]['image'].shape}\")\n",
    "        print(f\"Train batch label (Y) shape: {batch[0]['label'].shape}\")\n",
    "        print(f\"Train batch label (original_size) shape: {batch[0]['original_size']}\")\n",
    "        break  # Para após o primeiro batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = label.squeeze(0).cpu().numpy()  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Encoder freeze!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): BCEWithLogitsLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    # apply_freeze=apply_freeze,\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    multimask_output=False,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    "    loss_fn=nn.BCEWithLogitsLoss()\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─BCEWithLogitsLoss: 1-1                                     --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  (1,024)\n",
       "│    │    └─Embedding: 3-6                                   (256)\n",
       "│    │    └─Sequential: 3-7                                  (4,684)\n",
       "│    │    └─Embedding: 3-8                                   (256)\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,729,252\n",
       "Non-trainable params: 6,220\n",
       "====================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 0\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_12 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_12\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_12/run_2025-03-03-12-43-20ba9950c5e4374d39b5262e94a0025e2a.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type              | Params | Mode \n",
      "------------------------------------------------------\n",
      "0 | loss_fn | BCEWithLogitsLoss | 0      | train\n",
      "1 | model   | _SAM              | 93.7 M | train\n",
      "------------------------------------------------------\n",
      "93.7 M    Trainable params\n",
      "6.2 K     Non-trainable params\n",
      "93.7 M    Total params\n",
      "374.942   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1, 1006, 590])) must be the same as input size (torch.Size([1, 1, 1006, 590]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m\n\u001b[1;32m     15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     16\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[1;32m     17\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback],\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     23\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m SimpleLightningPipeline(\n\u001b[1;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[1;32m     26\u001b[0m     save_run_status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/pipelines/base.py:351\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(traceback\u001b[38;5;241m.\u001b[39mformat_exception(\u001b[38;5;241m*\u001b[39msys\u001b[38;5;241m.\u001b[39mexc_info()))\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_exception \u001b[38;5;241m=\u001b[39m exception\n\u001b[0;32m--> 351\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_end_time \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/pipelines/base.py:343\u001b[0m, in \u001b[0;36mPipeline.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_pipeline_info(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_dir \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 343\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_status \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINTERRUPTED\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/pipelines/lightning_pipeline.py:418\u001b[0m, in \u001b[0;36mSimpleLightningPipeline._run\u001b[0;34m(self, data, task, ckpt_path)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_test(data, ckpt_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/pipelines/lightning_pipeline.py:237\u001b[0m, in \u001b[0;36mSimpleLightningPipeline._fit\u001b[0;34m(self, data, ckpt_path)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: L\u001b[38;5;241m.\u001b[39mLightningDataModule, ckpt_path: Optional[PathLike] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    226\u001b[0m ):\n\u001b[1;32m    227\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model using the given data.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03m        The checkpoint path to be used. If None, no checkpoint will be used.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py:144\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/loops/evaluation_loop.py:433\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    427\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    432\u001b[0m )\n\u001b[0;32m--> 433\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    326\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightning/pytorch/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2823\u001b[0m, in \u001b[0;36mSam.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m   2822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m-> 2823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_single_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2794\u001b[0m, in \u001b[0;36mSam._single_step\u001b[0;34m(self, batch, batch_idx, step_name)\u001b[0m\n\u001b[1;32m   2791\u001b[0m masks_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmasks_logits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs])  \u001b[38;5;66;03m# [batch_size, num_classes, H, W]\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[38;5;28minput\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28minput\u001b[39m \u001b[38;5;129;01min\u001b[39;00m batch])  \u001b[38;5;66;03m# [batch_size, H, W]\u001b[39;00m\n\u001b[0;32m-> 2794\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2795\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_metrics(masks_logits, labels, step_name)\n\u001b[1;32m   2797\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\n\u001b[1;32m   2798\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2799\u001b[0m     loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2804\u001b[0m     sync_dist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   2805\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2861\u001b[0m, in \u001b[0;36mSam._loss\u001b[0;34m(self, masks, label)\u001b[0m\n\u001b[1;32m   2860\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, masks, label):\n\u001b[0;32m-> 2861\u001b[0m     loss_ce \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_ce\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:821\u001b[0m, in \u001b[0;36mBCEWithLogitsLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 821\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy_with_logits\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3639\u001b[0m, in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3636\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m-> 3639\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3640\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) must be the same as input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3641\u001b[0m     )\n\u001b[1;32m   3643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mbinary_cross_entropy_with_logits(\n\u001b[1;32m   3644\u001b[0m     \u001b[38;5;28minput\u001b[39m, target, weight, pos_weight, reduction_enum\n\u001b[1;32m   3645\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([1, 1006, 590])) must be the same as input size (torch.Size([1, 1, 1006, 590]))"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_2_{ratio}_using_facie_{facie}-{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-22-12-48-279ac913e40b444872820dbae4b5141f4e.yaml\n",
      "Testing DataLoader 0: 100%|██████████| 200/200 [00:17<00:00, 11.40it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7056908011436462     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.746991753578186     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7056908011436462    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.746991753578186    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-22-12-48-279ac913e40b444872820dbae4b5141f4e.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.7056908011436462, 'test_mIoU_epoch': 0.746991753578186}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-22-12-48-279ac913e40b444872820dbae4b5141f4e.yaml\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [00:16<00:00, 11.96it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-22-12-48-279ac913e40b444872820dbae4b5141f4e.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[9][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 9º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 9:\n",
    "        pred_9 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_9[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
