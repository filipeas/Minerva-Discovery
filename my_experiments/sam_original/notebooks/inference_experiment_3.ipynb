{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Notebook\n",
    "- This notebook implements the experiment 3.\n",
    "- In the experiment 3, we use SAM model in your original version and:\n",
    "    - train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is promptable, i.e, we use 3 points for learn model where to segment.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/logs/sam_vit_b_experiment_3/seam_ai/checkpoints/last.ckpt\"\n",
    "\n",
    "# checkpoints SAM\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 20\n",
    "ratio = 1.0\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels\n",
    "            image = data_readers[0]\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam.load_from_checkpoint(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint_path=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"evaluate_sam_experiment_3_{ratio}_{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    # save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    # mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-22-13-15-4286355e4e32674a08a40d5630de7d05d5.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_2 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 3/1197 [00:00<06:33,  3.03it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1197/1197 [01:50<00:00, 10.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0068507194519043     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7822622060775757     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.0068507194519043    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7822622060775757    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-22-13-15-4286355e4e32674a08a40d5630de7d05d5.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 2.0068507194519043,\n",
       "  'test_mIoU_epoch': 0.7822622060775757}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-22-13-15-4286355e4e32674a08a40d5630de7d05d5.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1197/1197 [01:47<00:00, 11.09it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_2/run_2025-02-22-13-15-4286355e4e32674a08a40d5630de7d05d5.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
