{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from minerva.models.nets.image.sam import Sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ = 'exp_3' # original, exp_1, exp_2 and exp_3\n",
    "vit_model = \"vit-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when load original weights. Applying now remaping.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'image_encoder.pos_embed'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2574\u001b[0m, in \u001b[0;36mSam._build_sam\u001b[0;34m(self, vit_type, image_encoder, prompt_encoder, mask_decoder, encoder_embed_dim, encoder_depth, encoder_num_heads, mask_threshold, encoder_global_attn_indexes, num_multimask_outputs, iou_head_depth, checkpoint)\u001b[0m\n\u001b[1;32m   2573\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2574\u001b[0m     \u001b[43msam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for _SAM:\n\tMissing key(s) in state_dict: \"image_encoder.pos_embed\", \"image_encoder.patch_embed.proj.weight\", \"image_encoder.patch_embed.proj.bias\", \"image_encoder.blocks.0.norm1.weight\", \"image_encoder.blocks.0.norm1.bias\", \"image_encoder.blocks.0.attn.rel_pos_h\", \"image_encoder.blocks.0.attn.rel_pos_w\", \"image_encoder.blocks.0.attn.qkv.weight\", \"image_encoder.blocks.0.attn.qkv.bias\", \"image_encoder.blocks.0.attn.proj.weight\", \"image_encoder.blocks.0.attn.proj.bias\", \"image_encoder.blocks.0.norm2.weight\", \"image_encoder.blocks.0.norm2.bias\", \"image_encoder.blocks.0.mlp.lin1.weight\", \"image_encoder.blocks.0.mlp.lin1.bias\", \"image_encoder.blocks.0.mlp.lin2.weight\", \"image_encoder.blocks.0.mlp.lin2.bias\", \"image_encoder.blocks.1.norm1.weight\", \"image_encoder.blocks.1.norm1.bias\", \"image_encoder.blocks.1.attn.rel_pos_h\", \"image_encoder.blocks.1.attn.rel_pos_w\", \"image_encoder.blocks.1.attn.qkv.weight\", \"image_encoder.blocks.1.attn.qkv.bias\", \"image_encoder.blocks.1.attn.proj.weight\", \"image_encoder.blocks.1.attn.proj.bias\", \"image_encoder.blocks.1.norm2.weight\", \"image_encoder.blocks.1.norm2.bias\", \"image_encoder.blocks.1.mlp.lin1.weight\", \"image_encoder.blocks.1.mlp.lin1.bias\", \"image_encoder.blocks.1.mlp.lin2.weight\", \"image_encoder.blocks.1.mlp.lin2.bias\", \"image_encoder.blocks.2.norm1.weight\", \"image_encoder.blocks.2.norm1.bias\", \"image_encoder.blocks.2.attn.rel_pos_h\", \"image_encoder.blocks.2.attn.rel_pos_w\", \"image_encoder.blocks.2.attn.qkv.weight\", \"image_encoder.blocks.2.attn.qkv.bias\", \"image_encoder.blocks.2.attn.proj.weight\", \"image_encoder.blocks.2.attn.proj.bias\", \"image_encoder.blocks.2.norm2.weight\", \"image_encoder.blocks.2.norm2.bias\", \"image_encoder.blocks.2.mlp.lin1.weight\", \"image_encoder.blocks.2.mlp.lin1.bias\", \"image_encoder.blocks.2.mlp.lin2.weight\", \"image_encoder.blocks.2.mlp.lin2.bias\", \"image_encoder.blocks.3.norm1.weight\", \"image_encoder.blocks.3.norm1.bias\", \"image_encoder.blocks.3.attn.rel_pos_h\", \"image_encoder.blocks.3.attn.rel_pos_w\", \"image_encoder.blocks.3.attn.qkv.weight\", \"image_encoder.blocks.3.attn.qkv.bias\", \"image_encoder.blocks.3.attn.proj.weight\", \"image_encoder.blocks.3.attn.proj.bias\", \"image_encoder.blocks.3.norm2.weight\", \"image_encoder.blocks.3.norm2.bias\", \"image_encoder.blocks.3.mlp.lin1.weight\", \"image_encoder.blocks.3.mlp.lin1.bias\", \"image_encoder.blocks.3.mlp.lin2.weight\", \"image_encoder.blocks.3.mlp.lin2.bias\", \"image_encoder.blocks.4.norm1.weight\", \"image_encoder.blocks.4.norm1.bias\", \"image_encoder.blocks.4.attn.rel_pos_h\", \"image_encoder.blocks.4.attn.rel_pos_w\", \"image_encoder.blocks.4.attn.qkv.weight\", \"image_encoder.blocks.4.attn.qkv.bias\", \"image_encoder.blocks.4.attn.proj.weight\", \"image_encoder.blocks.4.attn.proj.bias\", \"image_encoder.blocks.4.norm2.weight\", \"image_encoder.blocks.4.norm2.bias\", \"image_encoder.blocks.4.mlp.lin1.weight\", \"image_encoder.blocks.4.mlp.lin1.bias\", \"image_encoder.blocks.4.mlp.lin2.weight\", \"image_encoder.blocks.4.mlp.lin2.bias\", \"image_encoder.blocks.5.norm1.weight\", \"image_encoder.blocks.5.norm1.bias\", \"image_encoder.blocks.5.attn.rel_pos_h\", \"image_encoder.blocks.5.attn.rel_pos_w\", \"image_encoder.blocks.5.attn.qkv.weight\", \"image_encoder.blocks.5.attn.qkv.bias\", \"image_encoder.blocks.5.attn.proj.weight\", \"image_encoder.blocks.5.attn.proj.bias\", \"image_encoder.blocks.5.norm2.weight\", \"image_encoder.blocks.5.norm2.bias\", \"image_encoder.blocks.5.mlp.lin1.weight\", \"image_encoder.blocks.5.mlp.lin1.bias\", \"image_encoder.blocks.5.mlp.lin2.weight\", \"image_encoder.blocks.5.mlp.lin2.bias\", \"image_encoder.blocks.6.norm1.weight\", \"image_encoder.blocks.6.norm1.bias\", \"image_encoder.blocks.6.attn.rel_pos_h\", \"image_encoder.blocks.6.attn.rel_pos_w\", \"image_encoder.blocks.6.attn.qkv.weight\", \"image_encoder.blocks.6.attn.qkv.bias\", \"image_encoder.blocks.6.attn.proj.weight\", \"image_encoder.blocks.6.attn.proj.bias\", \"image_encoder.blocks.6.norm2.weight\", \"image_encoder.blocks.6.norm2.bias\", \"image_encoder.blocks.6.mlp.lin1.weight\", \"image_encoder.blocks.6.mlp.lin1.bias\", \"image_encoder.blocks.6.mlp.lin2.weight\", \"image_encoder.blocks.6.mlp.lin2.bias\", \"image_encoder.blocks.7.norm1.weight\", \"image_encoder.blocks.7.norm1.bias\", \"image_encoder.blocks.7.attn.rel_pos_h\", \"image_encoder.blocks.7.attn.rel_pos_w\", \"image_encoder.blocks.7.attn.qkv.weight\", \"image_encoder.blocks.7.attn.qkv.bias\", \"image_encoder.blocks.7.attn.proj.weight\", \"image_encoder.blocks.7.attn.proj.bias\", \"image_encoder.blocks.7.norm2.weight\", \"image_encoder.blocks.7.norm2.bias\", \"image_encoder.blocks.7.mlp.lin1.weight\", \"image_encoder.blocks.7.mlp.lin1.bias\", \"image_encoder.blocks.7.mlp.lin2.weight\", \"image_encoder.blocks.7.mlp.lin2.bias\", \"image_encoder.blocks.8.norm1.weight\", \"image_encoder.blocks.8.norm1.bias\", \"image_encoder.blocks.8.attn.rel_pos_h\", \"image_encoder.blocks.8.attn.rel_pos_w\", \"image_encoder.blocks.8.attn.qkv.weight\", \"image_encoder.blocks.8.attn.qkv.bias\", \"image_encoder.blocks.8.attn.proj.weight\", \"image_encoder.blocks.8.attn.proj.bias\", \"image_encoder.blocks.8.norm2.weight\", \"image_encoder.blocks.8.norm2.bias\", \"image_encoder.blocks.8.mlp.lin1.weight\", \"image_encoder.blocks.8.mlp.lin1.bias\", \"image_encoder.blocks.8.mlp.lin2.weight\", \"image_encoder.blocks.8.mlp.lin2.bias\", \"image_encoder.blocks.9.norm1.weight\", \"image_encoder.blocks.9.norm1.bias\", \"image_encoder.blocks.9.attn.rel_pos_h\", \"image_encoder.blocks.9.attn.rel_pos_w\", \"image_encoder.blocks.9.attn.qkv.weight\", \"image_encoder.blocks.9.attn.qkv.bias\", \"image_encoder.blocks.9.attn.proj.weight\", \"image_encoder.blocks.9.attn.proj.bias\", \"image_encoder.blocks.9.norm2.weight\", \"image_encoder.blocks.9.norm2.bias\", \"image_encoder.blocks.9.mlp.lin1.weight\", \"image_encoder.blocks.9.mlp.lin1.bias\", \"image_encoder.blocks.9.mlp.lin2.weight\", \"image_encoder.blocks.9.mlp.lin2.bias\", \"image_encoder.blocks.10.norm1.weight\", \"image_encoder.blocks.10.norm1.bias\", \"image_encoder.blocks.10.attn.rel_pos_h\", \"image_encoder.blocks.10.attn.rel_pos_w\", \"image_encoder.blocks.10.attn.qkv.weight\", \"image_encoder.blocks.10.attn.qkv.bias\", \"image_encoder.blocks.10.attn.proj.weight\", \"image_encoder.blocks.10.attn.proj.bias\", \"image_encoder.blocks.10.norm2.weight\", \"image_encoder.blocks.10.norm2.bias\", \"image_encoder.blocks.10.mlp.lin1.weight\", \"image_encoder.blocks.10.mlp.lin1.bias\", \"image_encoder.blocks.10.mlp.lin2.weight\", \"image_encoder.blocks.10.mlp.lin2.bias\", \"image_encoder.blocks.11.norm1.weight\", \"image_encoder.blocks.11.norm1.bias\", \"image_encoder.blocks.11.attn.rel_pos_h\", \"image_encoder.blocks.11.attn.rel_pos_w\", \"image_encoder.blocks.11.attn.qkv.weight\", \"image_encoder.blocks.11.attn.qkv.bias\", \"image_encoder.blocks.11.attn.proj.weight\", \"image_encoder.blocks.11.attn.proj.bias\", \"image_encoder.blocks.11.norm2.weight\", \"image_encoder.blocks.11.norm2.bias\", \"image_encoder.blocks.11.mlp.lin1.weight\", \"image_encoder.blocks.11.mlp.lin1.bias\", \"image_encoder.blocks.11.mlp.lin2.weight\", \"image_encoder.blocks.11.mlp.lin2.bias\", \"image_encoder.neck.0.weight\", \"image_encoder.neck.1.weight\", \"image_encoder.neck.1.bias\", \"image_encoder.neck.2.weight\", \"image_encoder.neck.3.weight\", \"image_encoder.neck.3.bias\", \"prompt_encoder.pe_layer.positional_encoding_gaussian_matrix\", \"prompt_encoder.point_embeddings.0.weight\", \"prompt_encoder.point_embeddings.1.weight\", \"prompt_encoder.point_embeddings.2.weight\", \"prompt_encoder.point_embeddings.3.weight\", \"prompt_encoder.not_a_point_embed.weight\", \"prompt_encoder.mask_downscaling.0.weight\", \"prompt_encoder.mask_downscaling.0.bias\", \"prompt_encoder.mask_downscaling.1.weight\", \"prompt_encoder.mask_downscaling.1.bias\", \"prompt_encoder.mask_downscaling.3.weight\", \"prompt_encoder.mask_downscaling.3.bias\", \"prompt_encoder.mask_downscaling.4.weight\", \"prompt_encoder.mask_downscaling.4.bias\", \"prompt_encoder.mask_downscaling.6.weight\", \"prompt_encoder.mask_downscaling.6.bias\", \"prompt_encoder.no_mask_embed.weight\", \"mask_decoder.transformer.layers.0.self_attn.q_proj.weight\", \"mask_decoder.transformer.layers.0.self_attn.q_proj.bias\", \"mask_decoder.transformer.layers.0.self_attn.k_proj.weight\", \"mask_decoder.transformer.layers.0.self_attn.k_proj.bias\", \"mask_decoder.transformer.layers.0.self_attn.v_proj.weight\", \"mask_decoder.transformer.layers.0.self_attn.v_proj.bias\", \"mask_decoder.transformer.layers.0.self_attn.out_proj.weight\", \"mask_decoder.transformer.layers.0.self_attn.out_proj.bias\", \"mask_decoder.transformer.layers.0.norm1.weight\", \"mask_decoder.transformer.layers.0.norm1.bias\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias\", \"mask_decoder.transformer.layers.0.norm2.weight\", \"mask_decoder.transformer.layers.0.norm2.bias\", \"mask_decoder.transformer.layers.0.mlp.lin1.weight\", \"mask_decoder.transformer.layers.0.mlp.lin1.bias\", \"mask_decoder.transformer.layers.0.mlp.lin2.weight\", \"mask_decoder.transformer.layers.0.mlp.lin2.bias\", \"mask_decoder.transformer.layers.0.norm3.weight\", \"mask_decoder.transformer.layers.0.norm3.bias\", \"mask_decoder.transformer.layers.0.norm4.weight\", \"mask_decoder.transformer.layers.0.norm4.bias\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight\", \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias\", \"mask_decoder.transformer.layers.1.self_attn.q_proj.weight\", \"mask_decoder.transformer.layers.1.self_attn.q_proj.bias\", \"mask_decoder.transformer.layers.1.self_attn.k_proj.weight\", \"mask_decoder.transformer.layers.1.self_attn.k_proj.bias\", \"mask_decoder.transformer.layers.1.self_attn.v_proj.weight\", \"mask_decoder.transformer.layers.1.self_attn.v_proj.bias\", \"mask_decoder.transformer.layers.1.self_attn.out_proj.weight\", \"mask_decoder.transformer.layers.1.self_attn.out_proj.bias\", \"mask_decoder.transformer.layers.1.norm1.weight\", \"mask_decoder.transformer.layers.1.norm1.bias\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias\", \"mask_decoder.transformer.layers.1.norm2.weight\", \"mask_decoder.transformer.layers.1.norm2.bias\", \"mask_decoder.transformer.layers.1.mlp.lin1.weight\", \"mask_decoder.transformer.layers.1.mlp.lin1.bias\", \"mask_decoder.transformer.layers.1.mlp.lin2.weight\", \"mask_decoder.transformer.layers.1.mlp.lin2.bias\", \"mask_decoder.transformer.layers.1.norm3.weight\", \"mask_decoder.transformer.layers.1.norm3.bias\", \"mask_decoder.transformer.layers.1.norm4.weight\", \"mask_decoder.transformer.layers.1.norm4.bias\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight\", \"mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias\", \"mask_decoder.transformer.final_attn_token_to_image.q_proj.weight\", \"mask_decoder.transformer.final_attn_token_to_image.q_proj.bias\", \"mask_decoder.transformer.final_attn_token_to_image.k_proj.weight\", \"mask_decoder.transformer.final_attn_token_to_image.k_proj.bias\", \"mask_decoder.transformer.final_attn_token_to_image.v_proj.weight\", \"mask_decoder.transformer.final_attn_token_to_image.v_proj.bias\", \"mask_decoder.transformer.final_attn_token_to_image.out_proj.weight\", \"mask_decoder.transformer.final_attn_token_to_image.out_proj.bias\", \"mask_decoder.transformer.norm_final_attn.weight\", \"mask_decoder.transformer.norm_final_attn.bias\", \"mask_decoder.iou_token.weight\", \"mask_decoder.mask_tokens.weight\", \"mask_decoder.output_upscaling.0.weight\", \"mask_decoder.output_upscaling.0.bias\", \"mask_decoder.output_upscaling.1.weight\", \"mask_decoder.output_upscaling.1.bias\", \"mask_decoder.output_upscaling.3.weight\", \"mask_decoder.output_upscaling.3.bias\", \"mask_decoder.output_hypernetworks_mlps.0.layers.0.weight\", \"mask_decoder.output_hypernetworks_mlps.0.layers.0.bias\", \"mask_decoder.output_hypernetworks_mlps.0.layers.1.weight\", \"mask_decoder.output_hypernetworks_mlps.0.layers.1.bias\", \"mask_decoder.output_hypernetworks_mlps.0.layers.2.weight\", \"mask_decoder.output_hypernetworks_mlps.0.layers.2.bias\", \"mask_decoder.output_hypernetworks_mlps.1.layers.0.weight\", \"mask_decoder.output_hypernetworks_mlps.1.layers.0.bias\", \"mask_decoder.output_hypernetworks_mlps.1.layers.1.weight\", \"mask_decoder.output_hypernetworks_mlps.1.layers.1.bias\", \"mask_decoder.output_hypernetworks_mlps.1.layers.2.weight\", \"mask_decoder.output_hypernetworks_mlps.1.layers.2.bias\", \"mask_decoder.output_hypernetworks_mlps.2.layers.0.weight\", \"mask_decoder.output_hypernetworks_mlps.2.layers.0.bias\", \"mask_decoder.output_hypernetworks_mlps.2.layers.1.weight\", \"mask_decoder.output_hypernetworks_mlps.2.layers.1.bias\", \"mask_decoder.output_hypernetworks_mlps.2.layers.2.weight\", \"mask_decoder.output_hypernetworks_mlps.2.layers.2.bias\", \"mask_decoder.output_hypernetworks_mlps.3.layers.0.weight\", \"mask_decoder.output_hypernetworks_mlps.3.layers.0.bias\", \"mask_decoder.output_hypernetworks_mlps.3.layers.1.weight\", \"mask_decoder.output_hypernetworks_mlps.3.layers.1.bias\", \"mask_decoder.output_hypernetworks_mlps.3.layers.2.weight\", \"mask_decoder.output_hypernetworks_mlps.3.layers.2.bias\", \"mask_decoder.iou_prediction_head.layers.0.weight\", \"mask_decoder.iou_prediction_head.layers.0.bias\", \"mask_decoder.iou_prediction_head.layers.1.weight\", \"mask_decoder.iou_prediction_head.layers.1.bias\", \"mask_decoder.iou_prediction_head.layers.2.weight\", \"mask_decoder.iou_prediction_head.layers.2.bias\". \n\tUnexpected key(s) in state_dict: \"epoch\", \"global_step\", \"pytorch-lightning_version\", \"state_dict\", \"loops\", \"callbacks\", \"optimizer_states\", \"lr_schedulers\". ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     checkpoint_exp_3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m---> 42\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvit_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_exp_3\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_multimask_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# default: 3\u001b[39;49;00m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43miou_head_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# default: 3\u001b[39;49;00m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapply_freeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt_encoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmask_decoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(model)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2378\u001b[0m, in \u001b[0;36mSam.__init__\u001b[0;34m(self, image_encoder, prompt_encoder, mask_decoder, pixel_mean, pixel_std, mask_threshold, learning_rate, vit_type, checkpoint, num_multimask_outputs, iou_head_depth, loss_fn, train_metrics, val_metrics, test_metrics, apply_freeze, apply_adapter, lora_rank, lora_alpha, multimask_output, return_prediction_only)\u001b[0m\n\u001b[1;32m   2331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_type \u001b[38;5;241m=\u001b[39m vit_type\n\u001b[1;32m   2333\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2334\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit-b\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m   2335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit-b\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2375\u001b[0m     },\n\u001b[1;32m   2376\u001b[0m }\n\u001b[0;32m-> 2378\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_sam\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2380\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_freeze(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_freeze)\n\u001b[1;32m   2381\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_adapter(\n\u001b[1;32m   2382\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_adapter, alpha\u001b[38;5;241m=\u001b[39mlora_alpha, rank\u001b[38;5;241m=\u001b[39mlora_rank\n\u001b[1;32m   2383\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2580\u001b[0m, in \u001b[0;36mSam._build_sam\u001b[0;34m(self, vit_type, image_encoder, prompt_encoder, mask_decoder, encoder_embed_dim, encoder_depth, encoder_num_heads, mask_threshold, encoder_global_attn_indexes, num_multimask_outputs, iou_head_depth, checkpoint)\u001b[0m\n\u001b[1;32m   2576\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError when load original weights. Applying now remaping.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2578\u001b[0m )\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vit_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit-b\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 2580\u001b[0m     new_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_b\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2581\u001b[0m \u001b[43m        \u001b[49m\u001b[43msam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvit_patch_size\u001b[49m\n\u001b[1;32m   2582\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# using remaping for vit_b\u001b[39;00m\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m vit_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvit-h\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   2584\u001b[0m     new_state_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_from_h(\n\u001b[1;32m   2585\u001b[0m         sam,\n\u001b[1;32m   2586\u001b[0m         state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2589\u001b[0m         encoder_global_attn_indexes,\n\u001b[1;32m   2590\u001b[0m     )  \u001b[38;5;66;03m# using remaping for vit_h\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/minerva/models/nets/image/sam.py:2629\u001b[0m, in \u001b[0;36mSam.load_from_b\u001b[0;34m(self, sam, state_dict, image_size, vit_patch_size)\u001b[0m\n\u001b[1;32m   2616\u001b[0m except_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   2617\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2618\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hypernetworks_mlps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2619\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miou_prediction_head\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2620\u001b[0m ]\n\u001b[1;32m   2621\u001b[0m new_state_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2622\u001b[0m     k: v\n\u001b[1;32m   2623\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m state_dict\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m except_keys[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[1;32m   2628\u001b[0m }\n\u001b[0;32m-> 2629\u001b[0m pos_embed \u001b[38;5;241m=\u001b[39m \u001b[43mnew_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage_encoder.pos_embed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2630\u001b[0m token_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(image_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m vit_patch_size)\n\u001b[1;32m   2631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos_embed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m token_size:\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;66;03m# resize pos embedding, which may sacrifice the performance, but I have no better idea\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'image_encoder.pos_embed'"
     ]
    }
   ],
   "source": [
    "if test_ == 'original':\n",
    "    checkpoint_original_sam = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\"\n",
    "    num_classes = 3\n",
    "\n",
    "    model = Sam(\n",
    "        vit_type=vit_model,\n",
    "        checkpoint=checkpoint_original_sam,\n",
    "        num_multimask_outputs=num_classes, # default: 3\n",
    "        iou_head_depth=num_classes, # default: 3\n",
    "        # apply_freeze={\"prompt_encoder\": False, \"image_encoder\": False, \"mask_decoder\": False}\n",
    "        # apply_freeze=apply_freeze,\n",
    "        # apply_adapter=apply_adapter\n",
    "    )\n",
    "elif test_ == 'exp_1':\n",
    "    checkpoint_exp_1 = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_1_1.0_SAM_ViT_B_parihaka_fine_tuning_&_fine_tuning_1.0-2025-02-16-epoch=04-val_loss=0.11.ckpt\"\n",
    "    num_classes = 6\n",
    "    apply_freeze={\"prompt_encoder\": True, \"image_encoder\": False, \"mask_decoder\": False}\n",
    "    apply_adapter={}\n",
    "\n",
    "    model = Sam.load_from_checkpoint(\n",
    "        checkpoint_path=checkpoint_exp_1,\n",
    "        vit_type=vit_model,\n",
    "        num_multimask_outputs=num_classes,\n",
    "        iou_head_depth=num_classes,\n",
    "        apply_freeze=apply_freeze,\n",
    "        apply_adapter=apply_adapter\n",
    "    )\n",
    "elif test_ == 'exp_2':\n",
    "    checkpoint_exp_2 = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_0.1/sam_experiment_2_0.1_using_facie_1-SAM_ViT_B_parihaka-2025-02-17-epoch=07-val_loss=0.22.ckpt\"\n",
    "    num_classes = 3\n",
    "\n",
    "    model = Sam(\n",
    "        vit_type=vit_model,\n",
    "        checkpoint=checkpoint_exp_2,\n",
    "        num_multimask_outputs=num_classes, # default: 3\n",
    "        iou_head_depth=num_classes, # default: 3\n",
    "    )\n",
    "elif test_ == \"exp_3\":\n",
    "    checkpoint_exp_3 = \"/workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints/parihaka_1.0/sam_experiment_3_1.0_SAM_ViT_B_parihaka-2025-02-16-epoch=01-val_loss=0.03.ckpt\"\n",
    "    num_classes = 3\n",
    "\n",
    "    model = Sam(\n",
    "        vit_type=vit_model,\n",
    "        checkpoint=checkpoint_exp_3,\n",
    "        num_multimask_outputs=num_classes, # default: 3\n",
    "        iou_head_depth=num_classes, # default: 3\n",
    "        apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False}\n",
    "    )\n",
    "\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
