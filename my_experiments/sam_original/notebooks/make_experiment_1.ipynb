{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - Notebook\n",
    "- This notebook implements the experiment 1.\n",
    "- In the experiment 1, we use SAM model in your original version and:\n",
    "    - segment 6 masks.\n",
    "    - all segmentation is automatic, i.e, not used prompt encoder during finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 2\n",
      "CUDA Device Name: NVIDIA RTX A6000\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=True\n",
    "num_classes = 6\n",
    "num_epochs = 1\n",
    "ratio = 0.1\n",
    "batch_size = 1\n",
    "rank = 4\n",
    "alpha = 1\n",
    "gpu_index = 0\n",
    "debug=False # if True, the cell \"Debug\" will run.\n",
    "# fine_tuning & fine_tuning\n",
    "model_name_experiment = f\"{model_name}_fine_tuning_&_fine_tuning_{ratio}\"\n",
    "apply_freeze={\"prompt_encoder\": True, \"image_encoder\": False, \"mask_decoder\": False}\n",
    "apply_adapter={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class for create dataset with SAM pattern \"\"\"\n",
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            transform_coords_input:Optional[dict]=None,\n",
    "            multimask_output:bool=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom Dataset to use properties that needed in images when send some image to SAM model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        readers: List[_Reader]\n",
    "            List of data readers. It must contain exactly 2 readers.\n",
    "            The first reader for the input data and the second reader for the\n",
    "            target data.\n",
    "        transforms: Optional[_Transform]\n",
    "            Optional data transformation pipeline.\n",
    "        transform_coords_input: Optional[dict] \n",
    "            List with transforms to apply.\n",
    "                point_coords (np.ndarray or None): A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.\n",
    "                point_labels (np.ndarray or None): A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.\n",
    "    \"\"\"\n",
    "        super().__init__(readers, transforms)\n",
    "        # self.transform_coords_input = transform_coords_input\n",
    "        self.multimask_output = multimask_output\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        # assert (\n",
    "        #     len(self.readers) == len(self.transforms)\n",
    "        #     and len(self.transforms) == len(self.transform_coords_input)\n",
    "        #     and len(self.readers) == len(self.transform_coords_input)\n",
    "        # ), \"DatasetForSAM requires exactly iquals lens (readers, transforms and transform_coords_input)\"\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Load data and return data with SAM format (dict), where dict has:\n",
    "        'image' (required): The image as a torch tensor in 3xHxW format.\n",
    "        'label' (required): The label of the image.\n",
    "        'original_size' (required): The original size of the image before transformation.\n",
    "        'point_coords' (optional): (torch.Tensor) Batched point prompts for this image, with shape BxNx2. Already transformed to the input frame of the model.\n",
    "        'point_labels' (optional): (torch.Tensor) Batched labels for point prompts, with shape BxN. (0 is background, 1 is object and -1 is pad)\n",
    "        'boxes' (optional): (torch.Tensor) Batched box inputs, with shape Bx4.  Already transformed to the input frame of the model.\n",
    "        'mask_inputs' (optional): (torch.Tensor) Batched mask inputs to the model, in the form Bx1xHxW.\n",
    "        \"\"\"\n",
    "\n",
    "        data_readers = []\n",
    "        for reader, transform in zip(self.readers, self.transforms):\n",
    "            sample = reader[index]\n",
    "            if transform is not None:\n",
    "                sample = transform(sample)\n",
    "            data_readers.append(sample)\n",
    "        \n",
    "        # normalize to rgb (0,255) and add 3 channels\n",
    "        image = data_readers[0]\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "        label = data_readers[1]\n",
    "\n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': (int(data_readers[0].shape[1]), int(data_readers[0].shape[2])),\n",
    "            'multimask_output': self.multimask_output\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class for create data module \"\"\"\n",
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        transform_coords_input: _Transform = None,\n",
    "        multimask_output:bool = True,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.transform_coords_input = transform_coords_input\n",
    "        self.multimask_output = multimask_output\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "            \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                transform_coords_input=self.transform_coords_input,\n",
    "                multimask_output=self.multimask_output\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                transform_coords_input=self.transform_coords_input,\n",
    "                multimask_output=self.multimask_output\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                transform_coords_input=self.transform_coords_input,\n",
    "                multimask_output=self.multimask_output\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width), \n",
    "    filter_type=filter_type,\n",
    "    multimask_output=multimask_output,\n",
    "    batch_size=batch_size,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    print(\"Total batches: \", len(get_train_dataloader(data_module)))\n",
    "\n",
    "    train_batch = next(iter(get_train_dataloader(data_module)))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']}\")\n",
    "    print(f\"multimask_output: {train_batch[0]['multimask_output']}\")\n",
    "\n",
    "    for idx, batch in enumerate(get_train_dataloader(data_module)):\n",
    "        print(f\"Batch {idx}:\")\n",
    "        print(f\"Tipo do batch: {type(batch)}\")\n",
    "        print(f\"Tamanho do batch: {len(batch)}\")  # Deve ser igual ao batch_size\n",
    "        print(\"Estrutura do primeiro item do batch:\")\n",
    "        # print(batch[0])  # Exibe o primeiro dicionário do batch\n",
    "        print(f\"Shape da imagem no primeiro item: {batch[0]['image'].shape}\")\n",
    "        \n",
    "        print(20*'-')\n",
    "\n",
    "        print(f\"Train batch image (X) shape: {batch[0]['image'].shape}\")\n",
    "        print(f\"Train batch label (Y) shape: {batch[0]['label'].shape}\")\n",
    "        print(f\"Train batch label (original_size) shape: {batch[0]['original_size']}\")\n",
    "        print(f\"multimask_output: {batch[0]['multimask_output']}\")\n",
    "        break  # Para após o primeiro batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = label.squeeze(0).cpu().numpy()  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when load original weights. Applying now remaping.\n",
      "Prompt Encoder freeze!\n"
     ]
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes,  # default: 3\n",
    "    iou_head_depth=num_classes,  # default: 3\n",
    "    apply_freeze=apply_freeze,\n",
    "    apply_adapter=apply_adapter,\n",
    "    lora_rank=rank,\n",
    "    lora_alpha=alpha,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(7, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-6): 7 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (5): Linear(in_features=256, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  359.93122482299805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  (1,024)\n",
       "│    │    └─Embedding: 3-6                                   (256)\n",
       "│    │    └─Sequential: 3-7                                  (4,684)\n",
       "│    │    └─Embedding: 3-8                                   (256)\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,792\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 978,656\n",
       "│    │    └─MLP: 3-14                                        330,759\n",
       "=====================================================================================\n",
       "Total params: 94,353,811\n",
       "Trainable params: 94,347,591\n",
       "Non-trainable params: 6,220\n",
       "====================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-19-14-35-31f0767bbf10bc45c98ee6ca5082ad5ada.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_1 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 94.4 M | train\n",
      "-----------------------------------------------------\n",
      "94.3 M    Trainable params\n",
      "6.2 K     Non-trainable params\n",
      "94.4 M    Total params\n",
      "377.415   Total estimated model params size (MB)\n",
      "257       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 112/112 [00:39<00:00,  2.87it/s, v_num=1, train_loss_step=0.705, train_mIoU_step=0.452, val_loss_step=0.733, val_mIoU_step=0.385, val_loss_epoch=0.740, val_mIoU_epoch=0.390, train_loss_epoch=1.020, train_mIoU_epoch=0.306]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 112/112 [01:05<00:00,  1.71it/s, v_num=1, train_loss_step=0.705, train_mIoU_step=0.452, val_loss_step=0.733, val_mIoU_step=0.385, val_loss_epoch=0.740, val_mIoU_epoch=0.390, train_loss_epoch=1.020, train_mIoU_epoch=0.306]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-19-14-35-31f0767bbf10bc45c98ee6ca5082ad5ada.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_1_{ratio}_{model_name_experiment}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-19-14-35-31f0767bbf10bc45c98ee6ca5082ad5ada.yaml\n",
      "Testing DataLoader 0: 100%|██████████| 200/200 [00:18<00:00, 10.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8612738251686096     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.33523374795913696    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8612738251686096    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.33523374795913696   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-19-14-35-31f0767bbf10bc45c98ee6ca5082ad5ada.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.8612738251686096,\n",
       "  'test_mIoU_epoch': 0.33523374795913696}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-19-14-35-31f0767bbf10bc45c98ee6ca5082ad5ada.yaml\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [00:17<00:00, 11.19it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_1/run_2025-02-19-14-35-31f0767bbf10bc45c98ee6ca5082ad5ada.yaml\n"
     ]
    }
   ],
   "source": [
    "preds = pipeline.run(data=data_module, task=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.argmax(preds[108][0]['masks_logits'], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze().numpy(), cmap=label_cmap)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
