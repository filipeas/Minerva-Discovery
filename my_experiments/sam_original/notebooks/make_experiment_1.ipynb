{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1 - Notebook\n",
    "- This notebook implements the experiment 1.\n",
    "- In the experiment 1, we use SAM model in your original version and:\n",
    "    - segment 6 masks.\n",
    "    - all segmentation is automatic, i.e, not used prompt encoder during finetune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=True\n",
    "num_classes = 6\n",
    "num_epochs = 20\n",
    "ratio = 0.1\n",
    "batch_size = 1\n",
    "rank = 4\n",
    "alpha = 1\n",
    "gpu_index = 0\n",
    "debug=False # if True, the cell \"Debug\" will run.\n",
    "# fine_tuning & fine_tuning\n",
    "model_name_experiment = f\"{model_name}_fine_tuning_&_fine_tuning_{ratio}\"\n",
    "apply_freeze={\"prompt_encoder\": True, \"image_encoder\": False, \"mask_decoder\": False}\n",
    "apply_adapter={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class for create dataset with SAM pattern \"\"\"\n",
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            transform_coords_input:Optional[dict]=None,\n",
    "            multimask_output:bool=True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Custom Dataset to use properties that needed in images when send some image to SAM model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        readers: List[_Reader]\n",
    "            List of data readers. It must contain exactly 2 readers.\n",
    "            The first reader for the input data and the second reader for the\n",
    "            target data.\n",
    "        transforms: Optional[_Transform]\n",
    "            Optional data transformation pipeline.\n",
    "        transform_coords_input: Optional[dict] \n",
    "            List with transforms to apply.\n",
    "                point_coords (np.ndarray or None): A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.\n",
    "                point_labels (np.ndarray or None): A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.\n",
    "    \"\"\"\n",
    "        super().__init__(readers, transforms)\n",
    "        # self.transform_coords_input = transform_coords_input\n",
    "        self.multimask_output = multimask_output\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        # assert (\n",
    "        #     len(self.readers) == len(self.transforms)\n",
    "        #     and len(self.transforms) == len(self.transform_coords_input)\n",
    "        #     and len(self.readers) == len(self.transform_coords_input)\n",
    "        # ), \"DatasetForSAM requires exactly iquals lens (readers, transforms and transform_coords_input)\"\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Load data and return data with SAM format (dict), where dict has:\n",
    "        'image' (required): The image as a torch tensor in 3xHxW format.\n",
    "        'label' (required): The label of the image.\n",
    "        'original_size' (required): The original size of the image before transformation.\n",
    "        'point_coords' (optional): (torch.Tensor) Batched point prompts for this image, with shape BxNx2. Already transformed to the input frame of the model.\n",
    "        'point_labels' (optional): (torch.Tensor) Batched labels for point prompts, with shape BxN. (0 is background, 1 is object and -1 is pad)\n",
    "        'boxes' (optional): (torch.Tensor) Batched box inputs, with shape Bx4.  Already transformed to the input frame of the model.\n",
    "        'mask_inputs' (optional): (torch.Tensor) Batched mask inputs to the model, in the form Bx1xHxW.\n",
    "        \"\"\"\n",
    "\n",
    "        data_readers = []\n",
    "        for reader, transform in zip(self.readers, self.transforms):\n",
    "            sample = reader[index]\n",
    "            if transform is not None:\n",
    "                sample = transform(sample)\n",
    "            data_readers.append(sample)\n",
    "        \n",
    "        # normalize to rgb (0,255) and add 3 channels\n",
    "        image = data_readers[0]\n",
    "        if image.shape[0] == 1:\n",
    "            image = image.repeat(3, 1, 1)\n",
    "        image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "        label = data_readers[1]\n",
    "\n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': (int(data_readers[0].shape[1]), int(data_readers[0].shape[2])),\n",
    "            'multimask_output': self.multimask_output\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" class for create data module \"\"\"\n",
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        transform_coords_input: _Transform = None,\n",
    "        multimask_output:bool = True,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.transform_coords_input = transform_coords_input\n",
    "        self.multimask_output = multimask_output\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "            \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                transform_coords_input=self.transform_coords_input,\n",
    "                multimask_output=self.multimask_output\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                transform_coords_input=self.transform_coords_input,\n",
    "                multimask_output=self.multimask_output\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                transform_coords_input=self.transform_coords_input,\n",
    "                multimask_output=self.multimask_output\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width), \n",
    "    filter_type=filter_type,\n",
    "    multimask_output=multimask_output,\n",
    "    batch_size=batch_size,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    print(\"Total batches: \", len(get_train_dataloader(data_module)))\n",
    "\n",
    "    train_batch = next(iter(get_train_dataloader(data_module)))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']}\")\n",
    "    print(f\"multimask_output: {train_batch[0]['multimask_output']}\")\n",
    "\n",
    "    for idx, batch in enumerate(get_train_dataloader(data_module)):\n",
    "        print(f\"Batch {idx}:\")\n",
    "        print(f\"Tipo do batch: {type(batch)}\")\n",
    "        print(f\"Tamanho do batch: {len(batch)}\")  # Deve ser igual ao batch_size\n",
    "        print(\"Estrutura do primeiro item do batch:\")\n",
    "        # print(batch[0])  # Exibe o primeiro dicionário do batch\n",
    "        print(f\"Shape da imagem no primeiro item: {batch[0]['image'].shape}\")\n",
    "        \n",
    "        print(20*'-')\n",
    "\n",
    "        print(f\"Train batch image (X) shape: {batch[0]['image'].shape}\")\n",
    "        print(f\"Train batch label (Y) shape: {batch[0]['label'].shape}\")\n",
    "        print(f\"Train batch label (original_size) shape: {batch[0]['original_size']}\")\n",
    "        print(f\"multimask_output: {batch[0]['multimask_output']}\")\n",
    "        break  # Para após o primeiro batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    image = image.permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = label.squeeze(0).cpu().numpy()  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error when load original weights. Applying now remaping.\n",
      "Prompt Encoder freeze!\n"
     ]
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes,  # default: 3\n",
    "    iou_head_depth=num_classes,  # default: 3\n",
    "    apply_freeze=apply_freeze,\n",
    "    apply_adapter=apply_adapter,\n",
    "    lora_rank=rank,\n",
    "    lora_alpha=alpha,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(7, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-6): 7 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (5): Linear(in_features=256, out_features=7, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  359.93122482299805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  (1,024)\n",
       "│    │    └─Embedding: 3-6                                   (256)\n",
       "│    │    └─Sequential: 3-7                                  (4,684)\n",
       "│    │    └─Embedding: 3-8                                   (256)\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,792\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 978,656\n",
       "│    │    └─MLP: 3-14                                        330,759\n",
       "=====================================================================================\n",
       "Total params: 94,353,811\n",
       "Trainable params: 94,347,591\n",
       "Non-trainable params: 6,220\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6/run_2025-02-17-12-17-11bcee03ce8bdd46a0a10a75ddd74a1606.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_6 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 94.4 M | train\n",
      "-----------------------------------------------------\n",
      "94.3 M    Trainable params\n",
      "6.2 K     Non-trainable params\n",
      "94.4 M    Total params\n",
      "377.415   Total estimated model params size (MB)\n",
      "257       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 112/112 [00:55<00:00,  2.03it/s, v_num=6, train_loss_step=0.062, train_mIoU_step=0.913, val_loss_step=0.276, val_mIoU_step=0.738, val_loss_epoch=0.265, val_mIoU_epoch=0.762, train_loss_epoch=0.0475, train_mIoU_epoch=0.908] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 112/112 [00:55<00:00,  2.03it/s, v_num=6, train_loss_step=0.062, train_mIoU_step=0.913, val_loss_step=0.276, val_mIoU_step=0.738, val_loss_epoch=0.265, val_mIoU_epoch=0.762, train_loss_epoch=0.0475, train_mIoU_epoch=0.908]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6/run_2025-02-17-12-17-11bcee03ce8bdd46a0a10a75ddd74a1606.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_1_{ratio}_{model_name_experiment}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6/run_2025-02-17-12-17-11bcee03ce8bdd46a0a10a75ddd74a1606.yaml\n",
      "Testing DataLoader 0:  50%|█████     | 101/200 [00:17<00:17,  5.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 200/200 [00:33<00:00,  5.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5051261782646179     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.651515007019043     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5051261782646179    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.651515007019043    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6/run_2025-02-17-12-17-11bcee03ce8bdd46a0a10a75ddd74a1606.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.5051261782646179, 'test_mIoU_epoch': 0.651515007019043}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6/run_2025-02-17-12-17-11bcee03ce8bdd46a0a10a75ddd74a1606.yaml\n",
      "Predicting DataLoader 0: 100%|██████████| 200/200 [00:30<00:00,  6.53it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_6/run_2025-02-17-12-17-11bcee03ce8bdd46a0a10a75ddd74a1606.yaml\n"
     ]
    }
   ],
   "source": [
    "preds = pipeline.run(data=data_module, task=\"predict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.argmax(preds[108][0]['masks_logits'], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(image.squeeze().numpy(), cmap=label_cmap)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
