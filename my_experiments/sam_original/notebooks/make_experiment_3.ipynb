{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Notebook\n",
    "- This notebook implements the experiment 3.\n",
    "- In the experiment 3, we use SAM model in your original version and:\n",
    "    - train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is promptable, i.e, we use 3 points for learn model where to segment.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "# model_name = \"SAM_ViT_B_f3\"\n",
    "# height, width = 255, 701 # f3\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "model_name = \"SAM_ViT_B_parihaka\"\n",
    "height, width = 1006, 590 # parihaka\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 20\n",
    "ratio = 0.1\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels\n",
    "            image = data_readers[0]\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    test_train_dataloader = get_train_dataloader(data_module)\n",
    "    print(\"Total batches: \", len(test_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_batch = next(iter(test_train_dataloader))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape} - type: {type(train_batch[0]['image'])}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape} - type: {type(train_batch[0]['label'])}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']} - type: {type(train_batch[0]['original_size'])}\")\n",
    "    print(f\"Train batch point_coords shape: {train_batch[0]['point_coords'].shape} - type: {type(train_batch[0]['point_coords'])}\")\n",
    "    print(f\"Train batch point_labels shape: {train_batch[0]['point_labels'].shape} - type: {type(train_batch[0]['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_batch = next(iter(test_train_dataloader))\n",
    "\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    print(\"shape da image: \", train_batch[0]['image'].shape)\n",
    "    print(\"intervalo da image: \", torch.min(train_batch[0]['image']), torch.max(train_batch[0]['image']))\n",
    "    print(\"shape da label: \", train_batch[0]['label'].shape)\n",
    "    # image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    # label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    points = train_batch[0]['point_coords'] # Lista de coordenadas (x, y)\n",
    "    print(points)\n",
    "    image = train_batch[0]['image'].permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plotando os pontos na imagem\n",
    "    for point in points:\n",
    "        for y, x in point:\n",
    "            axes[0].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # plotando os pontos na label\n",
    "    for point in points:\n",
    "        for y, x in point:\n",
    "            axes[1].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-17-13-34-036a64e70dfa994ec8b9e83da8986b1dbc.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_9 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 93.7 M | train\n",
      "-----------------------------------------------------\n",
      "93.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.7 M    Total params\n",
      "374.942   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 650/650 [02:06<00:00,  5.14it/s, v_num=9, train_loss_step=0.00836, train_mIoU_step=0.992, val_loss_step=0.124, val_mIoU_step=0.752, val_loss_epoch=0.0686, val_mIoU_epoch=0.875, train_loss_epoch=0.0133, train_mIoU_epoch=0.959]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 650/650 [02:06<00:00,  5.14it/s, v_num=9, train_loss_step=0.00836, train_mIoU_step=0.992, val_loss_step=0.124, val_mIoU_step=0.752, val_loss_epoch=0.0686, val_mIoU_epoch=0.875, train_loss_epoch=0.0133, train_mIoU_epoch=0.959]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-17-13-34-036a64e70dfa994ec8b9e83da8986b1dbc.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_3_{ratio}_{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-17-13-34-036a64e70dfa994ec8b9e83da8986b1dbc.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1197/1197 [01:14<00:00, 16.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.1707085371017456     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.7732944488525391     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.1707085371017456    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7732944488525391    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-17-13-34-036a64e70dfa994ec8b9e83da8986b1dbc.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.1707085371017456,\n",
       "  'test_mIoU_epoch': 0.7732944488525391}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-17-13-34-036a64e70dfa994ec8b9e83da8986b1dbc.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 1197/1197 [01:15<00:00, 15.85it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_9/run_2025-02-17-13-34-036a64e70dfa994ec8b9e83da8986b1dbc.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
