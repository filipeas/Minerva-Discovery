{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3 - Notebook\n",
    "- This notebook implements the experiment 3.\n",
    "- In the experiment 3, we use SAM model in your original version and:\n",
    "    - train a model for segment one of the seismic facies (the model learn about borders, so the user need send prompts for the model learn where apply segmentation)\n",
    "    - segment 3 masks.\n",
    "    - all segmentation is promptable, i.e, we use 3 points for learn model where to segment.\n",
    "    - we execute a preprocess in images and label to get one facie at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from typing import List, Optional, Tuple\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchinfo import summary\n",
    "from torch import nn\n",
    "\n",
    "from minerva.models.finetune_adapters import LoRA\n",
    "from minerva.models.nets.image.sam import Sam\n",
    "from minerva.data.datasets.supervised_dataset import SimpleDataset\n",
    "from minerva.data.readers.png_reader import PNGReader\n",
    "from minerva.data.readers.tiff_reader import TiffReader\n",
    "from minerva.transforms.transform import _Transform\n",
    "from minerva.data.readers.reader import _Reader\n",
    "from minerva.pipelines.lightning_pipeline import SimpleLightningPipeline\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from datetime import datetime\n",
    "from lightning.pytorch.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "CUDA Version: 12.4\n",
      "CUDA Available: True\n",
      "CUDA Device Count: 1\n",
      "CUDA Device Name: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch Version:\", torch.__version__)\n",
    "print(\"CUDA Version:\", torch.version.cuda)\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"CUDA Device Count:\", torch.cuda.device_count())\n",
    "print(\"CUDA Device Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No CUDA Device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f3\n",
    "model_name = \"SAM_ViT_B_f3\"\n",
    "height, width = 255, 701 # f3\n",
    "train_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/images\"\n",
    "annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seismic/f3_segmentation/annotations\"\n",
    "\n",
    "# parihaka\n",
    "# model_name = \"SAM_ViT_B_parihaka\"\n",
    "# height, width = 1006, 590 # parihaka\n",
    "# train_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    "# annotation_path = \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    "\n",
    "# checkpoints SAM\n",
    "checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_b_01ec64.pth\" # vit_b\n",
    "# checkpoint_path = \"/workspaces/Minerva-Discovery/shared_data/weights_sam/checkpoints_sam/sam_vit_h_4b8939.pth\" # vit_h\n",
    "\n",
    "# parameters\n",
    "vit_model = 'vit-b'\n",
    "filter_type=None # il_ to filter inlines, xl_ to filter crosslines and None to no apply filter\n",
    "multimask_output=False # if true, segment num_classes\n",
    "num_classes = 3\n",
    "num_points = 3\n",
    "num_epochs = 20\n",
    "ratio = 0.1\n",
    "batch_size = 1\n",
    "debug=False # if true, show debug in cell \"Debug\"\n",
    "gpu_index = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Padding(_Transform):\n",
    "    def __init__(self, target_h_size: int, target_w_size: int):\n",
    "        self.target_h_size = target_h_size\n",
    "        self.target_w_size = target_w_size\n",
    "\n",
    "    def __call__(self, x: np.ndarray) -> np.ndarray:\n",
    "        h, w = x.shape[:2]\n",
    "        pad_h = max(0, self.target_h_size - h)\n",
    "        pad_w = max(0, self.target_w_size - w)\n",
    "        if len(x.shape) == 2:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w)), mode=\"reflect\")\n",
    "            padded = np.expand_dims(padded, axis=2)\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "        else:\n",
    "            padded = np.pad(x, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"reflect\")\n",
    "            padded = torch.from_numpy(padded).float()\n",
    "\n",
    "        padded = np.transpose(padded, (2, 0, 1))\n",
    "        return padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset for SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetForSAM(SimpleDataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            readers: List[_Reader], \n",
    "            transforms: Optional[_Transform] = None,\n",
    "            num_points:int=3\n",
    "    ):\n",
    "        super().__init__(readers, transforms)\n",
    "\n",
    "        assert (\n",
    "            len(self.readers) == 2\n",
    "        ), \"DatasetForSAM requires exactly 2 readers (image your label)\"\n",
    "\n",
    "        self.num_points = num_points\n",
    "        self.samples = []\n",
    "        self._preprocess_data()\n",
    "    \n",
    "    def _preprocess_data(self):\n",
    "        for index in range(len(self.readers[0])):\n",
    "            data_readers = []\n",
    "            for reader, transform in zip(self.readers, self.transforms):\n",
    "                sample = reader[index]\n",
    "                if transform is not None:\n",
    "                    sample = transform(sample)\n",
    "                data_readers.append(sample)\n",
    "            \n",
    "            # normalize and add 3 channels\n",
    "            image = data_readers[0]\n",
    "            if image.shape[0] == 1:\n",
    "                image = image.repeat(3, 1, 1)\n",
    "            image = (image * 255).clamp(0, 255).to(torch.uint8)\n",
    "            label = data_readers[1]\n",
    "            \n",
    "            num_facies = np.unique(label)\n",
    "            \n",
    "            for facie in num_facies:\n",
    "                region = np.zeros_like(label, dtype=np.uint8) # [H,W]\n",
    "                region[label == facie] = 1\n",
    "\n",
    "                point_coords = self.get_points_in_region(region=region, num_points=self.num_points)\n",
    "                self.samples.append((image, region, point_coords))\n",
    "\n",
    "    def get_points_in_region(self, region, num_points=3):\n",
    "        # # Garantir que a região tem apenas valores 0 e 1\n",
    "        # region = (region > 0).astype(np.uint8)\n",
    "\n",
    "        # Garantir que a matriz tem apenas duas dimensões removendo a dimensão extra\n",
    "        if region.ndim == 3 and region.shape[0] == 1:\n",
    "            region = region.squeeze(0)  # Remove a primeira dimensão (1, H, W) -> (H, W)\n",
    "\n",
    "\n",
    "        # Verificar se a região contém apenas valores 0 e 1\n",
    "        unique_values = np.unique(region)\n",
    "        if not np.array_equal(unique_values, [0, 1]) and not np.array_equal(unique_values, [1]) and not np.array_equal(unique_values, [0]):\n",
    "            raise ValueError(f\"A matriz 'region' contém valores inesperados: {unique_values}. Esperado apenas 0 e 1.\")\n",
    "\n",
    "        # Obter todas as coordenadas (y, x) da região branca\n",
    "        y_indices, x_indices = np.where(region == 1)\n",
    "\n",
    "        # Se não houver pontos na região, retornar uma lista vazia\n",
    "        if len(y_indices) == 0:\n",
    "            return []\n",
    "\n",
    "        # Encontrar o centro vertical de cada coluna\n",
    "        unique_x = np.unique(x_indices)\n",
    "        central_y_coords = []\n",
    "\n",
    "        for x in unique_x:\n",
    "            y_in_column = y_indices[x_indices == x]\n",
    "\n",
    "            if len(y_in_column) > 0:\n",
    "                central_y = y_in_column[len(y_in_column) // 2]  # Pega um ponto real, não a média\n",
    "                central_y_coords.append((x, central_y))\n",
    "\n",
    "        # Ordenar os pontos pela coordenada x\n",
    "        central_y_coords = sorted(central_y_coords, key=lambda coord: coord[0])\n",
    "\n",
    "        # Selecionar pontos equidistantes\n",
    "        num_points = min(num_points, len(central_y_coords))\n",
    "        indices = np.linspace(0, len(central_y_coords) - 1, num_points, dtype=int)\n",
    "        \n",
    "        selected_points = [central_y_coords[i] for i in indices]\n",
    "\n",
    "        # Filtrar pontos que realmente pertencem à região branca\n",
    "        filtered_points = [(int(x), int(round(y)), 1) for x, y in selected_points if region[int(round(y)), int(x)] == 1]\n",
    "\n",
    "        return filtered_points\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        index: Tuple:\n",
    "            - (image, label, point_coords)\n",
    "        \"\"\"\n",
    "        image, label, point_coords = self.samples[index]\n",
    "        \n",
    "        # preparing points and labels for add with prompt to SAM\n",
    "        points = [[x, y] for (x, y, value) in point_coords]\n",
    "        labels = [1] * len(points)\n",
    "\n",
    "        # image = torch.tensor(image, dtype=torch.float32)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        original_size = (int(image.shape[1]), int(image.shape[2])) # torch.tensor((int(image.shape[1]), int(image.shape[2])), dtype=torch.long)\n",
    "\n",
    "        # Verificar se original_size é uma tupla com 2 elementos\n",
    "        if not isinstance(original_size, tuple) or len(original_size) != 2:\n",
    "            raise ValueError(f\"original_size is not a valid tuple: {original_size}\")\n",
    "\n",
    "        points = torch.tensor(points, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        labels = torch.tensor(labels, dtype=torch.long).unsqueeze(0)  # Adicionando uma dimensão no início\n",
    "        \n",
    "        data = {\n",
    "            'image': image,\n",
    "            'label': label,\n",
    "            'original_size': original_size,\n",
    "            'point_coords': points,\n",
    "            'point_labels': labels\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_path: str,\n",
    "        annotations_path: str,\n",
    "        transforms: _Transform = None,\n",
    "        num_points:int = 3,\n",
    "        batch_size: int = 1,\n",
    "        data_ratio: float = 1.0,\n",
    "        filter_type: str = None,\n",
    "        num_workers: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_path = Path(train_path)\n",
    "        self.annotations_path = Path(annotations_path)\n",
    "        self.transforms = transforms\n",
    "        self.num_points = num_points\n",
    "        self.batch_size = batch_size\n",
    "        self.data_ratio = data_ratio\n",
    "\n",
    "        if filter_type not in (None, \"il_\", \"xl_\"):\n",
    "            raise ValueError(f\"filter_type must be 'il_', 'xl_', or None, but got '{filter_type}'\")\n",
    "        self.filter_type = filter_type\n",
    "\n",
    "        self.num_workers = (\n",
    "            num_workers if num_workers is not None else os.cpu_count()\n",
    "        )\n",
    "\n",
    "        self.datasets = {}\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == \"fit\":\n",
    "            train_img_reader = TiffReader(self.train_path / \"train\")\n",
    "            train_label_reader = PNGReader(self.annotations_path / \"train\")\n",
    "\n",
    "            # applying filter for get only inline or crossline\n",
    "            if self.filter_type:\n",
    "                # to images\n",
    "                train_img_reader.files = [\n",
    "                    f for f in train_img_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".tiff\", \".tif\"))\n",
    "                ]\n",
    "                # to labels\n",
    "                train_label_reader.files = [\n",
    "                    f for f in train_label_reader.files \n",
    "                    if f.name.startswith(self.filter_type) and f.name.lower().endswith((\".png\"))\n",
    "                ]\n",
    "\n",
    "            # applying ratio\n",
    "            num_train_samples = int(len(train_img_reader) * self.data_ratio)\n",
    "            if num_train_samples < len(train_img_reader):\n",
    "                indices = random.sample(range(len(train_img_reader)), num_train_samples)\n",
    "                train_img_reader = [train_img_reader[i] for i in indices]\n",
    "                train_label_reader = [train_label_reader[i] for i in indices]\n",
    "                \n",
    "            train_dataset = DatasetForSAM(\n",
    "                readers=[train_img_reader, train_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            val_img_reader = TiffReader(self.train_path / \"val\")\n",
    "            val_label_reader = PNGReader(self.annotations_path / \"val\")\n",
    "            val_dataset = DatasetForSAM(\n",
    "                readers=[val_img_reader, val_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "\n",
    "            self.datasets[\"train\"] = train_dataset\n",
    "            self.datasets[\"val\"] = val_dataset\n",
    "\n",
    "        elif stage == \"test\" or stage == \"predict\":\n",
    "            test_img_reader = TiffReader(self.train_path / \"test\")\n",
    "            test_label_reader = PNGReader(self.annotations_path / \"test\")\n",
    "            test_dataset = DatasetForSAM(\n",
    "                readers=[test_img_reader, test_label_reader],\n",
    "                transforms=self.transforms,\n",
    "                num_points=self.num_points\n",
    "            )\n",
    "            self.datasets[\"test\"] = test_dataset\n",
    "            self.datasets[\"predict\"] = test_dataset\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid stage: {stage}\")\n",
    "    \n",
    "    def custom_collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Custom collate function for DataLoader to return a list of dictionaries.\n",
    "        \"\"\"\n",
    "        return batch \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"train\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"val\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"test\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.datasets[\"predict\"],\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.custom_collate_fn\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = DataModule(\n",
    "    train_path=train_path,\n",
    "    annotations_path=annotation_path,\n",
    "    transforms=Padding(height, width),\n",
    "    filter_type=filter_type,\n",
    "    batch_size=batch_size,\n",
    "    num_points=num_points,\n",
    "    data_ratio=ratio\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    def get_train_dataloader(data_module):\n",
    "        data_module.setup(\"fit\")\n",
    "        return data_module.train_dataloader()\n",
    "\n",
    "    test_train_dataloader = get_train_dataloader(data_module)\n",
    "    print(\"Total batches: \", len(test_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_batch = next(iter(test_train_dataloader))\n",
    "    print(f\"Train batch image (X) shape: {train_batch[0]['image'].shape} - type: {type(train_batch[0]['image'])}\")\n",
    "    print(f\"Train batch label (Y) shape: {train_batch[0]['label'].shape} - type: {type(train_batch[0]['label'])}\")\n",
    "    print(f\"Train batch label (original_size) shape: {train_batch[0]['original_size']} - type: {type(train_batch[0]['original_size'])}\")\n",
    "    print(f\"Train batch point_coords shape: {train_batch[0]['point_coords'].shape} - type: {type(train_batch[0]['point_coords'])}\")\n",
    "    print(f\"Train batch point_labels shape: {train_batch[0]['point_labels'].shape} - type: {type(train_batch[0]['point_labels'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    print(f\"O Batch (de tamanho {len(train_batch)}) possui: {train_batch[0]['image'].shape[0]} canais, {train_batch[0]['image'].shape[1]} altura e {train_batch[0]['image'].shape[2]} largura.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if debug:\n",
    "    train_batch = next(iter(test_train_dataloader))\n",
    "\n",
    "    # Obtendo a imagem e a label do batch\n",
    "    print(\"shape da image: \", train_batch[0]['image'].shape)\n",
    "    print(\"intervalo da image: \", torch.min(train_batch[0]['image']), torch.max(train_batch[0]['image']))\n",
    "    print(\"shape da label: \", train_batch[0]['label'].shape)\n",
    "    # image = train_batch[0]['image'].squeeze(0)  # Remover a dimensão do batch (1, 3, 1006, 590) -> (3, 1006, 590)\n",
    "    # label = train_batch[0]['label'].squeeze(0)  # Remover a dimensão do batch (1, 1, 1006, 590) -> (1, 1006, 590)\n",
    "\n",
    "    # Transformando para formato adequado para matplotlib\n",
    "    points = train_batch[0]['point_coords'] # Lista de coordenadas (x, y)\n",
    "    print(points)\n",
    "    image = train_batch[0]['image'].permute(1, 2, 0).cpu().numpy()  # (3, 1006, 590) -> (1006, 590, 3)\n",
    "    label = train_batch[0]['label'].squeeze(0)  # (1, 1006, 590) -> (1006, 590)\n",
    "\n",
    "    # Plotando a imagem e a label\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Imagem original\n",
    "    axes[0].imshow(image)\n",
    "    axes[0].set_title(\"Imagem\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Plotando os pontos na imagem\n",
    "    for point in points:\n",
    "        for y, x in point:\n",
    "            axes[0].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    # Label (provavelmente uma máscara ou rótulo binário)\n",
    "    axes[1].imshow(label, cmap='gray')\n",
    "    axes[1].set_title(\"Label\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # plotando os pontos na label\n",
    "    for point in points:\n",
    "        for y, x in point:\n",
    "            axes[1].scatter(y, x, color='red', s=50, marker='x', label='Ponto')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (loss_fn): CrossEntropyLoss()\n",
       "  (model): _SAM(\n",
       "    (image_encoder): ImageEncoderViT(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (neck): Sequential(\n",
       "        (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): LayerNorm2d()\n",
       "        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (3): LayerNorm2d()\n",
       "      )\n",
       "    )\n",
       "    (prompt_encoder): PromptEncoder(\n",
       "      (pe_layer): PositionEmbeddingRandom()\n",
       "      (point_embeddings): ModuleList(\n",
       "        (0-3): 4 x Embedding(1, 256)\n",
       "      )\n",
       "      (not_a_point_embed): Embedding(1, 256)\n",
       "      (mask_downscaling): Sequential(\n",
       "        (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): LayerNorm2d()\n",
       "        (5): GELU(approximate='none')\n",
       "        (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (no_mask_embed): Embedding(1, 256)\n",
       "    )\n",
       "    (mask_decoder): MaskDecoder(\n",
       "      (transformer): TwoWayTransformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x TwoWayAttentionBlock(\n",
       "            (self_attn): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_token_to_image): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "            (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): MLPBlock(\n",
       "              (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn_image_to_token): AttentionMaskDecoder(\n",
       "              (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "              (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_attn_token_to_image): AttentionMaskDecoder(\n",
       "          (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (iou_token): Embedding(1, 256)\n",
       "      (mask_tokens): Embedding(4, 256)\n",
       "      (output_upscaling): Sequential(\n",
       "        (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (1): LayerNorm2d()\n",
       "        (2): GELU(approximate='none')\n",
       "        (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "        (4): GELU(approximate='none')\n",
       "      )\n",
       "      (output_hypernetworks_mlps): ModuleList(\n",
       "        (0-3): 4 x MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "            (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (iou_prediction_head): MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sam(\n",
    "    vit_type=vit_model,\n",
    "    checkpoint=checkpoint_path,\n",
    "    num_multimask_outputs=num_classes, # default: 3\n",
    "    iou_head_depth=num_classes, # default: 3\n",
    "    apply_freeze={\"image_encoder\": False, \"prompt_encoder\": False, \"mask_decoder\": False},\n",
    "    # apply_adapter=apply_adapter,\n",
    "    train_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    val_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    test_metrics={\"mIoU\": JaccardIndex(task=\"multiclass\", num_classes=num_classes)},\n",
    "    # multimask_output=multimask_output,\n",
    "    # loss_fn=DiceCELoss() # if multimask_output is false\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peso em MB:  357.57244873046875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "Sam                                                          --\n",
       "├─CrossEntropyLoss: 1-1                                      --\n",
       "├─_SAM: 1-2                                                  --\n",
       "│    └─ImageEncoderViT: 2-1                                  3,145,728\n",
       "│    │    └─PatchEmbed: 3-1                                  590,592\n",
       "│    │    └─ModuleList: 3-2                                  85,147,136\n",
       "│    │    └─Sequential: 3-3                                  787,456\n",
       "│    └─PromptEncoder: 2-2                                    --\n",
       "│    │    └─PositionEmbeddingRandom: 3-4                     --\n",
       "│    │    └─ModuleList: 3-5                                  1,024\n",
       "│    │    └─Embedding: 3-6                                   256\n",
       "│    │    └─Sequential: 3-7                                  4,684\n",
       "│    │    └─Embedding: 3-8                                   256\n",
       "│    └─MaskDecoder: 2-3                                      --\n",
       "│    │    └─TwoWayTransformer: 3-9                           3,291,264\n",
       "│    │    └─Embedding: 3-10                                  256\n",
       "│    │    └─Embedding: 3-11                                  1,024\n",
       "│    │    └─Sequential: 3-12                                 73,952\n",
       "│    │    └─ModuleList: 3-13                                 559,232\n",
       "│    │    └─MLP: 3-14                                        132,612\n",
       "=====================================================================================\n",
       "Total params: 93,735,472\n",
       "Trainable params: 93,735,472\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_model_size(model: torch.nn.Module):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    size_in_bytes = total_params * 4  # 4 bytes por parâmetro\n",
    "    size_in_mb = size_in_bytes / (1024 ** 2)\n",
    "    return size_in_mb\n",
    "\n",
    "print(\"Peso em MB: \", calculate_model_size(model))\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/utilities/seed.py:42: No seed found, seed set to 0\n",
      "Seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log directory set to: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3/run_2025-02-17-18-43-44522b8909b2804440bc067527f95df436.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/fabric/loggers/csv_logs.py:268: Experiment logs directory logs/sam/version_3 exists and is not empty. Previous log files in this directory will be deleted when the new ones are saved!\n",
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/checkpoints exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type             | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0      | train\n",
      "1 | model   | _SAM             | 93.7 M | train\n",
      "-----------------------------------------------------\n",
      "93.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "93.7 M    Total params\n",
      "374.942   Total estimated model params size (MB)\n",
      "239       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 3. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 514/514 [01:59<00:00,  4.29it/s, v_num=3, train_loss_step=0.00843, train_mIoU_step=0.993, val_loss_step=0.00194, val_mIoU_step=0.989, val_loss_epoch=0.0191, val_mIoU_epoch=0.960, train_loss_epoch=0.00586, train_mIoU_epoch=0.984] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 514/514 [01:59<00:00,  4.29it/s, v_num=3, train_loss_step=0.00843, train_mIoU_step=0.993, val_loss_step=0.00194, val_mIoU_step=0.989, val_loss_epoch=0.0191, val_mIoU_epoch=0.960, train_loss_epoch=0.00586, train_mIoU_epoch=0.984]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3/run_2025-02-17-18-43-44522b8909b2804440bc067527f95df436.yaml\n"
     ]
    }
   ],
   "source": [
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Define o callback para salvar o modelo com base no menor valor da métrica de validação\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\", # Métrica para monitorar\n",
    "    # save_last=True,\n",
    "    dirpath=\"./checkpoints\", # Diretório onde os checkpoints serão salvos\n",
    "    filename=f\"sam_experiment_3_{ratio}_{model_name}-{current_date}-{{epoch:02d}}-{{val_loss:.2f}}\", # Nome do arquivo do checkpoint\n",
    "    save_top_k=1, # Quantos melhores checkpoints salvar (no caso, o melhor)\n",
    "    mode=\"min\", # Como a métrica deve ser tratada (no caso, 'min' significa que menor valor de val_loss é melhor)\n",
    ")\n",
    "\n",
    "logger = CSVLogger(\"logs\", name=\"sam\")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[gpu_index],\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "\n",
    "pipeline = SimpleLightningPipeline(\n",
    "    model=model,\n",
    "    trainer=trainer,\n",
    "    save_run_status=True\n",
    ")\n",
    "\n",
    "pipeline.run(data=data_module, task=\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3/run_2025-02-17-18-43-44522b8909b2804440bc067527f95df436.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2275/2275 [02:22<00:00, 15.98it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_loss_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.16284091770648956    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_mIoU_epoch      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.858048677444458     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_loss_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.16284091770648956   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     test_mIoU_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.858048677444458    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3/run_2025-02-17-18-43-44522b8909b2804440bc067527f95df436.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss_epoch': 0.16284091770648956,\n",
       "  'test_mIoU_epoch': 0.858048677444458}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run(data=data_module, task=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3/run_2025-02-17-18-43-44522b8909b2804440bc067527f95df436.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 2275/2275 [02:10<00:00, 17.43it/s]\n",
      "Pipeline info saved at: /workspaces/Minerva-Discovery/my_experiments/sam_original/notebooks/logs/sam/version_3/run_2025-02-17-18-43-44522b8909b2804440bc067527f95df436.yaml\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definir o cmap para a imagem segmentada\n",
    "label_cmap = ListedColormap(\n",
    "    [\n",
    "        [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "        [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "        [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "        [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "        [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "        [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Criar o subplot com duas colunas\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Primeira imagem - Predição original\n",
    "preds = pipeline.run(data=data_module, task=\"predict\")\n",
    "image1 = torch.argmax(preds[108][0]['masks_logits'], dim=1)\n",
    "axes[0].imshow(image1.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[0].set_title(\"Imagem Segmentada Predita\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "# Segunda imagem - Predição com DataLoader\n",
    "data_module.setup(\"predict\")\n",
    "pred_module = iter(data_module.predict_dataloader())\n",
    "\n",
    "# Iterando para pegar o 108º elemento\n",
    "for i, batch in enumerate(pred_module):\n",
    "    if i == 108:\n",
    "        pred_108 = batch  # Pega o batch de predição\n",
    "        break\n",
    "\n",
    "# Segunda imagem - Máscara segmentada da predição\n",
    "image2 = pred_108[0]['label']\n",
    "axes[1].imshow(image2.squeeze().numpy(), cmap=label_cmap)\n",
    "axes[1].set_title(\"Imagem Segmentada Original\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
