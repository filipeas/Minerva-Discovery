{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from common import get_data_module\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_per_sample(pred, label, num_classes = 6, class_names = (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"7\")):\n",
    "    # Initialize metrics without averaging (per class)\n",
    "    iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    specificity = torchmetrics.Specificity(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    cohen_kappa = torchmetrics.CohenKappa(task=\"multiclass\", num_classes=num_classes)\n",
    "    mcc = torchmetrics.MatthewsCorrCoef(task=\"multiclass\", num_classes=num_classes)\n",
    "    hamming_loss = torchmetrics.HammingDistance(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    # Initialize weighted metrics\n",
    "    weighted_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "    weighted_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "    weighted_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "\n",
    "    # print(\"shape pred antes: \", pred.shape)\n",
    "    # print(\"valores unicos: \", np.unique(pred))\n",
    "    # print(\"shape label antes: \", label.shape)\n",
    "    # print(\"valores unicos: \", np.unique(label))\n",
    "    # Convert numpy arrays to tensors and add batch dimension\n",
    "    pred_tensor = torch.from_numpy(pred).unsqueeze(0)\n",
    "    label_tensor = torch.from_numpy(label).unsqueeze(0)\n",
    "    # print(\"shape pred_tensor depois: \", pred_tensor.shape)\n",
    "    # print(\"valores unicos: \", torch.unique(pred_tensor))\n",
    "    # print(\"shape label_tensor depois: \", label_tensor.shape)\n",
    "    # print(\"valores unicos: \", torch.unique(label_tensor))\n",
    "\n",
    "    # Calculate per-class metrics\n",
    "    iou_per_class = iou(pred_tensor, label_tensor)\n",
    "    iou_dict = {class_names[i]: iou_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    precision_per_class = precision(pred_tensor, label_tensor)\n",
    "    precision_dict = {class_names[i]: precision_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    recall_per_class = recall(pred_tensor, label_tensor)\n",
    "    recall_dict = {class_names[i]: recall_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    f1_per_class = f1_score(pred_tensor, label_tensor)\n",
    "    f1_dict = {class_names[i]: f1_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    accuracy_per_class = accuracy(pred_tensor, label_tensor)\n",
    "    accuracy_dict = {class_names[i]: accuracy_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    specificity_per_class = specificity(pred_tensor, label_tensor)\n",
    "    specificity_dict = {class_names[i]: specificity_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    # Compute weighted metrics\n",
    "    weighted_precision_value = weighted_precision(pred_tensor, label_tensor).item()\n",
    "    weighted_recall_value = weighted_recall(pred_tensor, label_tensor).item()\n",
    "    weighted_f1_value = weighted_f1(pred_tensor, label_tensor).item()\n",
    "\n",
    "    # Compute Cohen's Kappa, MCC, and Hamming Loss\n",
    "    cohen_kappa_value = cohen_kappa(pred_tensor, label_tensor).item()\n",
    "    mcc_value = mcc(pred_tensor, label_tensor).item()\n",
    "    hamming_loss_value = hamming_loss(pred_tensor, label_tensor).item()\n",
    "\n",
    "    # Calculate the percentage of each class in the label\n",
    "    label_flat = label_tensor.flatten()\n",
    "    class_counts = torch.bincount(label_flat, minlength=num_classes).float()  # Count occurrences of each class\n",
    "    total_samples = class_counts.sum().item()\n",
    "    class_percentages = {class_names[i]: (class_counts[i].item() / total_samples * 100) if total_samples > 0 else 0.0 for i in range(num_classes)}\n",
    "\n",
    "    # Calculate weighted accuracy manually\n",
    "    weighted_accuracy_value = sum(\n",
    "        (accuracy_per_class[i] * class_counts[i]).item() for i in range(num_classes)\n",
    "    ) / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "    # Compute mean values while handling potential NaNs\n",
    "    if \"6\" in iou_dict:\n",
    "        iou_dict.pop(\"6\")\n",
    "        print(f\"6 not in iou_dict anymore: {iou_dict}\")\n",
    "    if \"6\" in precision_dict:\n",
    "        precision_dict.pop(\"6\")\n",
    "    if \"6\" in recall_dict:\n",
    "        recall_dict.pop(\"6\")\n",
    "    if \"6\" in f1_dict:\n",
    "        f1_dict.pop(\"6\")\n",
    "    if \"6\" in accuracy_dict:\n",
    "        accuracy_dict.pop(\"6\")\n",
    "    if \"6\" in specificity_dict:\n",
    "        specificity_dict.pop(\"6\")\n",
    "    \n",
    "    mean_iou = np.nanmean(list(iou_dict.values()))\n",
    "    mean_precision = np.nanmean(list(precision_dict.values()))\n",
    "    mean_recall = np.nanmean(list(recall_dict.values()))\n",
    "    mean_f1 = np.nanmean(list(f1_dict.values()))\n",
    "    mean_accuracy = np.nanmean(list(accuracy_dict.values()))\n",
    "    mean_specificity = np.nanmean(list(specificity_dict.values()))\n",
    "    \n",
    "    \n",
    "    summary = {\n",
    "        \"Mean IoU\": mean_iou,\n",
    "        \"Mean F1 Score\": mean_f1,\n",
    "        \"Mean Recall\": mean_recall,\n",
    "        \"Mean Precision\": mean_precision,\n",
    "        \"Mean Accuracy\": mean_accuracy,\n",
    "        \"Mean Specificity\": mean_specificity,\n",
    "        \"Weighted Precision\": weighted_precision_value,\n",
    "        \"Weighted Recall\": weighted_recall_value,\n",
    "        \"Weighted F1 Score\": weighted_f1_value,\n",
    "        \"Weighted Accuracy\": weighted_accuracy_value,\n",
    "        \"Cohen's Kappa\": cohen_kappa_value,\n",
    "        \"Matthews Correlation Coefficient\": mcc_value,\n",
    "        \"Hamming Loss\": hamming_loss_value,\n",
    "    }\n",
    "\n",
    "\n",
    "    for m_name, m_dict in [\n",
    "        (\"IoU\", iou_dict),\n",
    "        (\"Precision\", precision_dict),\n",
    "        (\"Recall\", recall_dict),\n",
    "        (\"F1 Score\", f1_dict),\n",
    "        (\"Accuracy\", accuracy_dict),\n",
    "        (\"Specificity\", specificity_dict),\n",
    "        (\"Class percentages\", class_percentages)\n",
    "    ]:\n",
    "        for class_name in sorted(class_names):\n",
    "            if class_name in m_dict:\n",
    "                summary[f\"{m_name} {class_name}\"] = m_dict[class_name]\n",
    "            else:\n",
    "                summary[f\"{m_name} {class_name}\"] = 0\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root_predictions_path = Path(\"/workspaces/HIAAC-KR-Dev-Container/Minerva-Dev/docs/notebooks/examples/seismic/facies_classification/parihaka/predictions\")\n",
    "root_predictions_path = Path(\"/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_3\")\n",
    "root_data_dir = Path(\n",
    "    \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\"\n",
    ")\n",
    "root_annotation_dir = Path(\n",
    "    \"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\"\n",
    ")\n",
    "\n",
    "data_module = get_data_module(\n",
    "    root_data_dir=root_data_dir,\n",
    "    root_annotation_dir=root_annotation_dir,\n",
    "    img_size=None,\n",
    "    batch_size=1,\n",
    "    single_channel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over predictions\n",
    "\n",
    "The cell below will loop over the predictions (npy files), calculcates several metrics for each prediction (using Parihaka's test set), and stores the results in a csv file, with same name as the prediction file, but with the `.csv` extension. The csv is stored in the same directory as the predictions.\n",
    "\n",
    "**NOTE**: No metrics are calculated if the csv file already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from /workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_3/sam_vit_b_experiment_3.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset sam_vit_b_experiment_3: 100%|██████████| 200/200 [01:07<00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to /workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_3/sam_vit_b_experiment_3.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def summary_dataset(dataset, predictions, dname):\n",
    "    results = []\n",
    "    \n",
    "    n_classes = 7\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(dataset)), total=len(dataset), desc=f\"Processing dataset {dname}\"):\n",
    "        img, label = dataset[i]\n",
    "        img = img.squeeze(0)\n",
    "        pred = predictions[i]\n",
    "        for j in range(pred.shape[0]):\n",
    "            single_pred = pred[j]\n",
    "            # pred = pred.argmax(axis=0)\n",
    "            result = compute_metrics_per_sample(single_pred, label, num_classes=n_classes)\n",
    "            results.append(result)\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "for pred_path in sorted(root_predictions_path.rglob(\"*.npy\")):\n",
    "    csv_path = pred_path.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        print(f\"Skipping {pred_path} as {csv_path.name} already exists\")\n",
    "        continue\n",
    "    \n",
    "    data_module.setup(\"predict\")\n",
    "    predict_dataset = data_module.datasets[\"predict\"]\n",
    "    print(f\"Loading predictions from {pred_path}\")\n",
    "    predictions = np.load(pred_path) # (200, 10, 1006, 590)\n",
    "\n",
    "    df = summary_dataset(predict_dataset, predictions, pred_path.stem)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved results to {csv_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dim view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1006, 590)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.setup(\"predict\")\n",
    "predict_dataset = data_module.datasets[\"predict\"]\n",
    "labels = np.array([predict_dataset[i][1] for i in range(len(predict_dataset))])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10, 1006, 590)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_1/sam_vit_b_experiment_1.npy'),\n",
       " (200, 10, 1006, 590))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_path = Path(\"/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_1/sam_vit_b_experiment_1.npy\")\n",
    "predictions = np.load(pred_path)\n",
    "print(predictions.shape)\n",
    "# predictions = predictions[:, 0:6, :, :]\n",
    "# predictions = predictions.argmax(axis=1)\n",
    "\n",
    "pred_path, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "\n",
    "def compute_mean_iou_plane(predictions, labels, axis):\n",
    "    \"\"\"\n",
    "    Compute the mean IoU along a given axis.\n",
    "    \n",
    "    Args:\n",
    "        predictions (numpy.ndarray): The predicted labels, shape (I, J, K)\n",
    "        labels (numpy.ndarray): The ground truth labels, shape (I, J, K)\n",
    "        axis (int): Axis to iterate over (0 for JxK, 1 for IxK, 2 for IxJ)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D plane with mean IoU values\n",
    "    \"\"\"\n",
    "\n",
    "    # Define IoU metric\n",
    "    iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=6, average=None)\n",
    "\n",
    "    # Get shape\n",
    "    I, J, K = predictions.shape\n",
    "\n",
    "    if axis == 0:  # Compute JxK (Iterate over i)\n",
    "        result_shape = (J, K)\n",
    "        iou_map = np.zeros(result_shape)\n",
    "\n",
    "        for j in range(J):\n",
    "            for k in range(K):\n",
    "                pred_trace = predictions[:, j, k]\n",
    "                label_trace = labels[:, j, k]\n",
    "\n",
    "                pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "                label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "                iou_per_class = iou(pred_tensor, label_tensor)\n",
    "                iou_map[j, k] = torch.mean(iou_per_class).item()\n",
    "\n",
    "    elif axis == 1:  # Compute IxK (Iterate over j)\n",
    "        result_shape = (I, K)\n",
    "        iou_map = np.zeros(result_shape)\n",
    "\n",
    "        for i in range(I):\n",
    "            for k in range(K):\n",
    "                pred_trace = predictions[i, :, k]\n",
    "                label_trace = labels[i, :, k]\n",
    "\n",
    "                pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "                label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "                iou_per_class = iou(pred_tensor, label_tensor)\n",
    "                iou_map[i, k] = torch.mean(iou_per_class).item()\n",
    "\n",
    "    elif axis == 2:  # Compute IxJ (Iterate over k)\n",
    "        result_shape = (I, J)\n",
    "        iou_map = np.zeros(result_shape)\n",
    "\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "                pred_trace = predictions[i, j, :]\n",
    "                label_trace = labels[i, j, :]\n",
    "\n",
    "                pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "                label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "                iou_per_class = iou(pred_tensor, label_tensor)\n",
    "                iou_map[i, j] = torch.mean(iou_per_class).item()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid axis. Choose 0 (JxK), 1 (IxK), or 2 (IxJ).\")\n",
    "\n",
    "    return iou_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_iou_map(\n",
    "    iou_map,\n",
    "    title=\"IoU Map\",\n",
    "    cmap=\"viridis\",\n",
    "    xlabel=\"Dimension 2\",\n",
    "    ylabel=\"Dimension 1\",\n",
    "    figsize=(8, 6),\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the IoU map using Matplotlib with full customization.\n",
    "\n",
    "    Args:\n",
    "        iou_map (numpy.ndarray): 2D array of IoU values.\n",
    "        title (str): Title of the plot.\n",
    "        cmap (str): Colormap to use (e.g., 'viridis', 'plasma', 'jet', etc.).\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        figsize (tuple): Size of the figure (width, height).\n",
    "        save_path (str or None): If provided, saves the plot to the given file path.\n",
    "        show (bool): Whether to display the figure.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The generated figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.imshow(iou_map, cmap=cmap, interpolation=\"nearest\")\n",
    "    fig.colorbar(cax, label=\"Mean IoU\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)  # Prevents displaying if show=False\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i range(len(predictions))\n",
    "plane_jk = compute_mean_iou_plane(predictions, labels, axis=0)\n",
    "print(plane_jk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_iou_map(plane_jk, title=\"Mean IoU (JxK)\", xlabel=\"K\", ylabel=\"J\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ik = compute_mean_iou_plane(predictions, labels, axis=1)\n",
    "print(plane_ik.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_iou_map(plane_ik, title=\"Mean IoU (IxK)\", xlabel=\"K\", ylabel=\"I\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ij = compute_mean_iou_plane(predictions, labels, axis=2)\n",
    "print(plane_ij.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_iou_map(plane_ij, title=\"Mean IoU (IxJ)\", xlabel=\"J\", ylabel=\"I\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "\n",
    "# Define IoU metric\n",
    "iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=6, average=None)\n",
    "\n",
    "# Assuming predictions and labels are numpy arrays of shape (I, J, K)\n",
    "I, J, K = predictions.shape\n",
    "\n",
    "# Storage for mean IoU values for each (i, j)\n",
    "iou_map = np.zeros((I, J))\n",
    "\n",
    "# Iterate over each (i, j) trace\n",
    "for i in range(I):\n",
    "    for j in range(J):\n",
    "        # Get the trace along k-axis\n",
    "        pred_trace = predictions[i, j, :]\n",
    "        label_trace = labels[i, j, :]\n",
    "\n",
    "        # Convert to PyTorch tensors and add batch dimension\n",
    "        pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "        label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "        # Compute per-class IoU and take mean\n",
    "        iou_per_class = iou(pred_tensor, label_tensor)\n",
    "        mean_iou = torch.mean(iou_per_class).item()\n",
    "\n",
    "        # Store result\n",
    "        iou_map[i, j] = mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(iou_map, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSVs\n",
    "\n",
    "The cell below will load all the csv files and create a dictionary, where the key is the name of the model, and the value is another dictionary, where the key is the run id and the value is the dataframe with the metrics.\n",
    "\n",
    "The metrics for multiple runs are grouped into a single dataframe, by taking the mean and standard deviation of the metrics. Thus, the final result is a dictionary where key is the model name and value is a dataframe with the mean and standard deviation of the metrics across the multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(\n",
    "    dfs_dict: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Utilitary function to merge multiple DataFrames into a single one.\"\"\"\n",
    "    merged_results = {}\n",
    "\n",
    "    for model_name, runs in dfs_dict.items():\n",
    "        # List to store DataFrames for this model\n",
    "        df_list = [\n",
    "            df.assign(\n",
    "                sample=list(df.index.tolist())\n",
    "            )  # Add a column to track runs (optional)\n",
    "            for i, df in enumerate(runs.values())\n",
    "        ]\n",
    "\n",
    "        # Combine all runs into a single DataFrame\n",
    "        combined_df = pd.concat(df_list)\n",
    "\n",
    "        # Compute mean and std for each column\n",
    "        # (index column is the respective inline/crossline. They are ordered)\n",
    "        mean_df = combined_df.groupby(\"sample\").mean().fillna(0)\n",
    "        std_df = (\n",
    "            combined_df.groupby(\"sample\").std().fillna(0)\n",
    "        )  # Fill NaNs with 0\n",
    "\n",
    "        # Rename standard deviation columns\n",
    "        std_df = std_df.rename(\n",
    "            columns={col: f\"{col} (std)\" for col in std_df.columns}\n",
    "        )\n",
    "\n",
    "        # Merge mean and std dataframes\n",
    "        final_df = pd.concat([mean_df, std_df], axis=1)\n",
    "\n",
    "        # Store in result dictionary\n",
    "        merged_results[model_name] = final_df.reset_index()\n",
    "\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for p in sorted(root_predictions_path.rglob(\"*.csv\")):\n",
    "    model_name = p.stem\n",
    "    if model_name not in dfs:\n",
    "        dfs[model_name] = {}\n",
    "    dfs[model_name][p.parent.stem] = pd.read_csv(p)\n",
    "\n",
    "\n",
    "dfs = merge_dfs(dfs)\n",
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = {}\n",
    "for i in range(6):\n",
    "    class_percentage = dfs[\"simclr\"][f\"Class percentages {i}\"].mean()\n",
    "    percentages[f\"Class {i}\"] = class_percentage\n",
    "    \n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_df(dfs: Dict[str, pd.DataFrame], metric_name) -> pd.DataFrame:\n",
    "    metrics = {}\n",
    "    for model_name, df in dfs.items():\n",
    "        results = {\n",
    "            f\"Mean {metric_name}\": df[f\"Mean {metric_name}\"].mean(),\n",
    "            f\"{metric_name} 0\": df[f\"{metric_name} 0\"].mean(),\n",
    "            f\"{metric_name} 1\": df[f\"{metric_name} 1\"].mean(),\n",
    "            f\"{metric_name} 2\": df[f\"{metric_name} 2\"].mean(),\n",
    "            f\"{metric_name} 3\": df[f\"{metric_name} 3\"].mean(),\n",
    "            f\"{metric_name} 4\": df[f\"{metric_name} 4\"].mean(),\n",
    "            f\"{metric_name} 5\": df[f\"{metric_name} 5\"].mean(),\n",
    "            \n",
    "            f\"Mean {metric_name} (std)\": df[f\"Mean {metric_name} (std)\"].mean(),\n",
    "            f\"{metric_name} 0 (std)\": df[f\"{metric_name} 0 (std)\"].mean(),\n",
    "            f\"{metric_name} 1 (std)\": df[f\"{metric_name} 1 (std)\"].mean(),\n",
    "            f\"{metric_name} 2 (std)\": df[f\"{metric_name} 2 (std)\"].mean(),\n",
    "            f\"{metric_name} 3 (std)\": df[f\"{metric_name} 3 (std)\"].mean(),\n",
    "            f\"{metric_name} 4 (std)\": df[f\"{metric_name} 4 (std)\"].mean(),\n",
    "            f\"{metric_name} 5 (std)\": df[f\"{metric_name} 5 (std)\"].mean(),\n",
    "        }\n",
    "        \n",
    "        metrics[model_name] = results\n",
    "        \n",
    "    metrics_df = pd.DataFrame(metrics).T.sort_index()\n",
    "    metrics_df = metrics_df.sort_values(f\"Mean {metric_name}\", ascending=False) * 100\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_df(metrics_df):\n",
    "    non_std_cols = [col for col in metrics_df.columns if \"std\" not in col]\n",
    "    new_metrics_df = metrics_df[non_std_cols].copy()\n",
    "    for col in non_std_cols:\n",
    "        new_metrics_df[col] = metrics_df.apply(\n",
    "            lambda x: f\"{x[col]:.2f}<br>(±{x[col + ' (std)']:.2f})\", axis=1\n",
    "        )\n",
    "        \n",
    "    return new_metrics_df\n",
    "\n",
    "\n",
    "def get_df_without_std(metrics_df):\n",
    "    non_std_cols = [col for col in metrics_df.columns if \"std\" not in col]\n",
    "    return metrics_df[non_std_cols]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = {\n",
    "    \"byol\": \"BYOL<br>(ResNet50)\",\n",
    "    \"deeplabv3\": \"DeepLabV3<br>(ResNet50)\",\n",
    "    \"tribyol\": \"TriBYOL<br>(ResNet50)\",\n",
    "    \"sam\": \"SAM<br>(SAM-ViT-B)\",\n",
    "    \"kenshodense\": \"KenShoDense<br>(ResNet50)\",\n",
    "    \"fastsiam\": \"FastSiam<br>(ResNet50)\",\n",
    "    \"simclr\": \"SimCLR<br>(ResNet50)\",\n",
    "    \"lfr\": \"LFR<br>(ResNet50)\",\n",
    "    \"dinov2_mla\": \"DINOv2-MLA<br>(DinoViT)\",\n",
    "    \"dinov2_mla_reflect_6_classes\": \"DINOv2-MLA - reflect<br>(DinoViT)\",\n",
    "    \"dinov2_mla_interpolate_6_classes\": \"DINOv2-MLA - interpolate<br>(DinoViT)\",\n",
    "    \"dinov2_mla_constant_7_classes\": \"DINOv2-MLA - constant<br>(DinoViT)\",\n",
    "    \"dinov2_dpt\": \"DINOv2-DPT<br>(DinoViT)\",\n",
    "    \"dinov2_pup\": \"DINOx2-PUP<br>(DinoViT)\",\n",
    "    \"sfm_base_patch16\": \"SFM<br>(SFM-ViT-B)\",\n",
    "    \"setr_pup\": \"SETR-PUP<br>(ViT-L)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_heat_map(metrics_df, text_df, colorbar_title: str = None, height: int = 1200, width: int = 800, index_map: dict = None):\n",
    "    df = metrics_df.copy()\n",
    "    if index_map is not None:\n",
    "        df = df.rename(index=index_map)\n",
    "    \n",
    "    fig = go.Figure(\n",
    "        go.Heatmap(\n",
    "            x=df.columns,\n",
    "            y=df.index[::-1],  # Reverse order of y-axis\n",
    "            z=df.values[::-1],  # Reverse order of values\n",
    "            text=text_df.values[::-1],  # Reverse order of text\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"family\": \"Times New Roman\", \"size\": 15},\n",
    "            colorscale=\"Blues\",\n",
    "            colorbar_title=colorbar_title,  # Changed legend title to \"IoU\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        width=width,\n",
    "        xaxis=dict(\n",
    "            showticklabels=True,  # Keep tick labels but remove title\n",
    "            title=None,  # Remove x-axis title\n",
    "            tickfont=dict(family=\"Times New Roman\", size=16),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=10, b=10),\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_iou = get_metrics_df(dfs, \"IoU\")\n",
    "metrics_df = get_df_without_std(metrics_df_iou)\n",
    "text_df = get_text_df(metrics_df_iou)\n",
    "\n",
    "metrics_df = metrics_df.drop(index=\"setr_pup\")\n",
    "metrics_df = metrics_df.drop(index=\"dinov2_mla\")\n",
    "text_df = text_df.drop(index=\"setr_pup\")\n",
    "text_df = text_df.drop(index=\"dinov2_mla\")\n",
    "\n",
    "\n",
    "fig = plot_heat_map(metrics_df, text_df, colorbar_title=\"IoU (%)\", height=1000, width=700, index_map=index_map)\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(root_predictions_path / \"iou_heatmap.png\")\n",
    "print(f\"Saved heatmap to {root_predictions_path / 'iou_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_iou = get_metrics_df(dfs, \"F1 Score\")\n",
    "metrics_df = get_df_without_std(metrics_df_iou)\n",
    "text_df = get_text_df(metrics_df_iou)\n",
    "\n",
    "metrics_df = metrics_df.drop(index=\"setr_pup\")\n",
    "metrics_df = metrics_df.drop(index=\"dinov2_mla\")\n",
    "text_df = text_df.drop(index=\"setr_pup\")\n",
    "text_df = text_df.drop(index=\"dinov2_mla\")\n",
    "\n",
    "\n",
    "\n",
    "fig = plot_heat_map(metrics_df, text_df, colorbar_title=\"F1-Score (%)\", height=1000, width=700, index_map=index_map)\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(root_predictions_path / \"f1_heatmap.png\")\n",
    "print(f\"Saved heatmap to {root_predictions_path / 'f1_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_value_heatmap(\n",
    "    values,\n",
    "    colorscale=\"Blues\",\n",
    "    title=\"Heatmap of Values\",\n",
    "    show_colorbar=True,\n",
    "    width=1000,\n",
    "    height=200,\n",
    "    metric_name=\"IoU\",\n",
    "    filename=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the given values using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    - values (list or np.array): List of values to visualize.\n",
    "    - colorscale (str): Color scheme for the heatmap (e.g., 'Viridis', 'Plasma', 'Jet').\n",
    "    - title (str): Title of the heatmap.\n",
    "    - show_colorbar (bool): Whether to display the color bar.\n",
    "    - width (int): Width of the plot.\n",
    "    - height (int): Height of the plot.\n",
    "    \"\"\"\n",
    "    # Reshape values for a single-row heatmap\n",
    "    heatmap_values = np.array([values])  # Convert to 2D array\n",
    "\n",
    "    # Create heatmap figure\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=heatmap_values,\n",
    "            colorscale=colorscale,\n",
    "            showscale=show_colorbar,\n",
    "            zmin=0,\n",
    "            zmax=1,\n",
    "            colorbar=dict(\n",
    "                title=metric_name,\n",
    "                tickvals=np.arange(0, 1.2, 0.2),\n",
    "                ticktext=[f\"{i:.1f}\" for i in np.arange(0, 1.2, 0.2)]\n",
    "            ) if show_colorbar else None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout settings\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickvals=list(range(len(values))),\n",
    "            showticklabels=False,  # Hide tick labels for cleaner look\n",
    "            title=\"Crossline index\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showticklabels=False,  # Hide y-axis labels as it's a single row\n",
    "            title=\"\",\n",
    "        ),\n",
    "        margin=dict(l=10, r=10, t=40, b=10),\n",
    "        title=title,\n",
    "        height=height,\n",
    "        width=width,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    if filename:\n",
    "        fig.write_image(filename)\n",
    "        print(f\"Saved heatmap to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in sorted(dfs.items(), key=lambda x: x[0]):\n",
    "    values = df[\"Mean IoU\"].values\n",
    "    plot_value_heatmap(\n",
    "        values,\n",
    "        colorscale=\"Plasma\",\n",
    "        title=f\"IoU for model: {index_map[key].replace('<br>', ' ')} (min: {values.min() * 100:.2f}%, average: {values.mean() * 100:.2f}%, max: {values.max() * 100:.2f}%)\",\n",
    "        width=1000,\n",
    "        height=250,\n",
    "        metric_name=\"IoU\",\n",
    "        filename=root_predictions_path / f\"{key}_iou_heatmap.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in sorted(dfs.items(), key=lambda x: x[0]):\n",
    "    values = df[\"Mean F1 Score\"].values\n",
    "    plot_value_heatmap(\n",
    "        values,\n",
    "        colorscale=\"Plasma\",\n",
    "        title=f\"F1-Score for model: {index_map[key].replace('<br>', ' ')} (min: {values.min() * 100:.2f}%, average: {values.mean() * 100:.2f}%, max: {values.max() * 100:.2f}%)\",\n",
    "        width=1000,\n",
    "        height=250,\n",
    "        metric_name=\"F1-Score\",\n",
    "        filename=root_predictions_path / f\"{key}_f1_heatmap.png\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
