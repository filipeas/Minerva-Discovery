{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/_distutils_hack/__init__.py:53: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from common import get_data_module\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.patches as mpatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_predictions_path = Path(\"/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/methodology_1/sam_vit_b_experiment_1\")\n",
    "root_data_dir = Path(\"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/images\")\n",
    "root_annotation_dir = Path(\"/workspaces/Minerva-Discovery/shared_data/seam_ai_datasets/seam_ai/annotations\")\n",
    "\n",
    "data_module = get_data_module(\n",
    "    root_data_dir=root_data_dir,\n",
    "    root_annotation_dir=root_annotation_dir,\n",
    "    img_size=None,\n",
    "    batch_size=1,\n",
    "    single_channel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looping over predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_per_sample(pred, label, num_classes = 6, class_names = (\"0\", \"1\", \"2\", \"3\", \"4\", \"5\")):\n",
    "    # Passo 1: Filtrar pixels onde a previsão é 7 (classe \"não classificada\")\n",
    "    # Cria uma máscara que seleciona somente os pixels com classes válidas (0 a 5)\n",
    "    mask = (pred != 7)\n",
    "\n",
    "    # Se não houver nenhum pixel válido, retorna métricas zeradas\n",
    "    if np.sum(mask) == 0:\n",
    "        return { \n",
    "            \"Mean IoU\": 0.0,\n",
    "            \"Mean F1 Score\": 0.0,\n",
    "            \"Mean Recall\": 0.0,\n",
    "            \"Mean Precision\": 0.0,\n",
    "            \"Mean Accuracy\": 0.0,\n",
    "            \"Mean Specificity\": 0.0,\n",
    "            \"Weighted Precision\": 0.0,\n",
    "            \"Weighted Recall\": 0.0,\n",
    "            \"Weighted F1 Score\": 0.0,\n",
    "            \"Weighted Accuracy\": 0.0,\n",
    "            \"Cohen's Kappa\": 0.0,\n",
    "            \"Matthews Correlation Coefficient\": 0.0,\n",
    "            \"Hamming Loss\": 0.0,\n",
    "            # Adiciona métricas por classe (0 a 5)\n",
    "            **{f\"IoU {cn}\": 0.0 for cn in class_names},\n",
    "            **{f\"Precision {cn}\": 0.0 for cn in class_names},\n",
    "            **{f\"Recall {cn}\": 0.0 for cn in class_names},\n",
    "            **{f\"F1 Score {cn}\": 0.0 for cn in class_names},\n",
    "            **{f\"Accuracy {cn}\": 0.0 for cn in class_names},\n",
    "            **{f\"Specificity {cn}\": 0.0 for cn in class_names},\n",
    "            **{f\"Class percentages {cn}\": 0.0 for cn in class_names},\n",
    "        }\n",
    "    \n",
    "    # Aplica o filtro tanto na previsão quanto no label\n",
    "    pred_filtered = pred[mask]\n",
    "    label_filtered = label[mask]\n",
    "\n",
    "    # Passo 2: Converter para tensores e adicionar dimensão de batch\n",
    "    pred_tensor = torch.from_numpy(pred_filtered).unsqueeze(0)\n",
    "    label_tensor = torch.from_numpy(label_filtered).unsqueeze(0)\n",
    "\n",
    "    # Passo 3: Inicializar os módulos de métrica do torchmetrics para os 6 classes\n",
    "    iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    specificity = torchmetrics.Specificity(task=\"multiclass\", num_classes=num_classes, average=None)\n",
    "    \n",
    "    cohen_kappa = torchmetrics.CohenKappa(task=\"multiclass\", num_classes=num_classes)\n",
    "    mcc = torchmetrics.MatthewsCorrCoef(task=\"multiclass\", num_classes=num_classes)\n",
    "    hamming_loss = torchmetrics.HammingDistance(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    # Métricas ponderadas\n",
    "    weighted_precision = torchmetrics.Precision(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "    weighted_recall = torchmetrics.Recall(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "    weighted_f1 = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"weighted\")\n",
    "\n",
    "    # Calcular métricas por classe\n",
    "    iou_per_class = iou(pred_tensor, label_tensor)\n",
    "    precision_per_class = precision(pred_tensor, label_tensor)\n",
    "    recall_per_class = recall(pred_tensor, label_tensor)\n",
    "    f1_per_class = f1_score(pred_tensor, label_tensor)\n",
    "    accuracy_per_class = accuracy(pred_tensor, label_tensor)\n",
    "    specificity_per_class = specificity(pred_tensor, label_tensor)\n",
    "\n",
    "    # Montar dicionários com as métricas de cada classe (usando os nomes das classes)\n",
    "    iou_dict = {class_names[i]: iou_per_class[i].item() for i in range(num_classes)}\n",
    "    precision_dict = {class_names[i]: precision_per_class[i].item() for i in range(num_classes)}\n",
    "    recall_dict = {class_names[i]: recall_per_class[i].item() for i in range(num_classes)}\n",
    "    f1_dict = {class_names[i]: f1_per_class[i].item() for i in range(num_classes)}\n",
    "    accuracy_dict = {class_names[i]: accuracy_per_class[i].item() for i in range(num_classes)}\n",
    "    specificity_dict = {class_names[i]: specificity_per_class[i].item() for i in range(num_classes)}\n",
    "\n",
    "    # Calcular métricas ponderadas\n",
    "    weighted_precision_value = weighted_precision(pred_tensor, label_tensor).item()\n",
    "    weighted_recall_value = weighted_recall(pred_tensor, label_tensor).item()\n",
    "    weighted_f1_value = weighted_f1(pred_tensor, label_tensor).item()\n",
    "\n",
    "    # Calcular Cohen's Kappa, MCC e Hamming Loss\n",
    "    cohen_kappa_value = cohen_kappa(pred_tensor, label_tensor).item()\n",
    "    mcc_value = mcc(pred_tensor, label_tensor).item()\n",
    "    hamming_loss_value = hamming_loss(pred_tensor, label_tensor).item()\n",
    "\n",
    "    # Calcular a porcentagem de cada classe no label (apenas dos pixels válidos)\n",
    "    label_flat = label_tensor.flatten()\n",
    "    class_counts = torch.bincount(label_flat, minlength=num_classes).float()  # Conta ocorrências de cada classe\n",
    "    total_samples = class_counts.sum().item()\n",
    "    class_percentages = {\n",
    "        class_names[i]: (class_counts[i].item() / total_samples * 100) if total_samples > 0 else 0.0 \n",
    "        for i in range(num_classes)\n",
    "    }\n",
    "\n",
    "    # Calcular a acurácia ponderada manualmente\n",
    "    weighted_accuracy_value = (\n",
    "        sum(accuracy_per_class[i] * class_counts[i] for i in range(num_classes)).item() / total_samples \n",
    "        if total_samples > 0 else 0.0\n",
    "    )\n",
    "\n",
    "    # Passo 4: Calcular as médias ignorando eventuais NaNs (mas garantindo 6 classes)\n",
    "    mean_iou = np.nanmean(list(iou_dict.values()))\n",
    "    mean_precision = np.nanmean(list(precision_dict.values()))\n",
    "    mean_recall = np.nanmean(list(recall_dict.values()))\n",
    "    mean_f1 = np.nanmean(list(f1_dict.values()))\n",
    "    mean_accuracy = np.nanmean(list(accuracy_dict.values()))\n",
    "    mean_specificity = np.nanmean(list(specificity_dict.values()))\n",
    "\n",
    "    # Montar o resumo das métricas\n",
    "    summary = {\n",
    "        \"Mean IoU\": mean_iou,\n",
    "        \"Mean F1 Score\": mean_f1,\n",
    "        \"Mean Recall\": mean_recall,\n",
    "        \"Mean Precision\": mean_precision,\n",
    "        \"Mean Accuracy\": mean_accuracy,\n",
    "        \"Mean Specificity\": mean_specificity,\n",
    "        \"Weighted Precision\": weighted_precision_value,\n",
    "        \"Weighted Recall\": weighted_recall_value,\n",
    "        \"Weighted F1 Score\": weighted_f1_value,\n",
    "        \"Weighted Accuracy\": weighted_accuracy_value,\n",
    "        \"Cohen's Kappa\": cohen_kappa_value,\n",
    "        \"Matthews Correlation Coefficient\": mcc_value,\n",
    "        \"Hamming Loss\": hamming_loss_value,\n",
    "    }\n",
    "\n",
    "    # Adicionar as métricas por classe e as porcentagens no resumo\n",
    "    for m_name, m_dict in [\n",
    "        (\"IoU\", iou_dict),\n",
    "        (\"Precision\", precision_dict),\n",
    "        (\"Recall\", recall_dict),\n",
    "        (\"F1 Score\", f1_dict),\n",
    "        (\"Accuracy\", accuracy_dict),\n",
    "        (\"Specificity\", specificity_dict),\n",
    "        (\"Class percentages\", class_percentages)\n",
    "    ]:\n",
    "        for class_name in class_names:\n",
    "            summary[f\"{m_name} {class_name}\"] = m_dict[class_name]\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample(label, pred, metrics, save_path):\n",
    "    miou = metrics.get(\"Mean IoU\", 0.0)\n",
    "    iou0 = metrics.get(\"IoU 0\", 0.0)\n",
    "    iou1 = metrics.get(\"IoU 1\", 0.0)\n",
    "    iou2 = metrics.get(\"IoU 2\", 0.0)\n",
    "    iou3 = metrics.get(\"IoU 3\", 0.0)\n",
    "    iou4 = metrics.get(\"IoU 4\", 0.0)\n",
    "    iou5 = metrics.get(\"IoU 5\", 0.0)\n",
    "\n",
    "    label_cmap_label = ListedColormap(\n",
    "        [\n",
    "            [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "            [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "            [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "            [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "            [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "            [0.8470588235294118, 0.1568627450980392, 0.1411764705882353]\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    label_cmap_pred = ListedColormap(\n",
    "        [\n",
    "            [0.29411764705882354, 0.4392156862745098, 0.7333333333333333],\n",
    "            [0.5882352941176471, 0.7607843137254902, 0.8666666666666667],\n",
    "            [0.8901960784313725, 0.9647058823529412, 0.9764705882352941],\n",
    "            [0.9803921568627451, 0.8745098039215686, 0.4666666666666667],\n",
    "            [0.9607843137254902, 0.47058823529411764, 0.29411764705882354],\n",
    "            [0.8470588235294118, 0.1568627450980392, 0.1411764705882353],\n",
    "            [1.0, 0.7529411764705882, 0.796078431372549], #background\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Cria uma figura com dois subplots: um para label e outro para predição\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    # Exibe o label\n",
    "    axs[0].imshow(label, cmap=label_cmap_label, vmin=0, vmax=5)  # ou use um cmap adequado para a segmentação\n",
    "    axs[0].set_title('Label')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Cria os \"patches\" para a legenda, associando cada cor à sua classe\n",
    "    patches = []\n",
    "    for i, color in enumerate(label_cmap_label.colors):\n",
    "        patches.append(mpatches.Patch(color=color, label=f'Classe {i}'))\n",
    "    # Adiciona a legenda no canto superior direito do subplot do label\n",
    "    axs[0].legend(handles=patches, loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "    \n",
    "    # Exibe a predição\n",
    "    axs[1].imshow(pred, cmap=label_cmap_pred, vmin=0, vmax=7)\n",
    "    axs[1].set_title(f'Predição (mIoU: {miou:.3f})\\n(0: {iou0:.3f} | 1: {iou1:.3f} | 2: {iou2:.3f} | 3: {iou3:.3f} | 4: {iou4:.3f} | 5: {iou5:.3f})')\n",
    "    axs[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading predictions from /workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/methodology_1/sam_vit_b_experiment_1/sam_vit_b_experiment_1.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing dataset sam_vit_b_experiment_1: 100%|██████████| 200/200 [08:30<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved results to /workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/methodology_1/sam_vit_b_experiment_1/sam_vit_b_experiment_1.csv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def summary_dataset(dataset, predictions, dname, visualizations_folder):\n",
    "    results = []\n",
    "    \n",
    "    n_classes = 6\n",
    "    \n",
    "    for i in tqdm.tqdm(range(len(dataset)), total=len(dataset), desc=f\"Processing dataset {dname}\"):\n",
    "        if i >= len(predictions):\n",
    "            break\n",
    "\n",
    "        img, label = dataset[i]\n",
    "        img = img.squeeze(0)\n",
    "\n",
    "        # loop over predictions (each batch has num_points images)\n",
    "        pred = predictions[i]\n",
    "        for j in range(pred.shape[0]):\n",
    "            single_pred = pred[j]\n",
    "            metrics = compute_metrics_per_sample(single_pred, label, num_classes=n_classes)\n",
    "            save_path = visualizations_folder / f\"{pred_path.stem}_sample{i}_pred{j}.png\"\n",
    "            visualize_sample(label, single_pred, metrics, save_path)\n",
    "            results.append(metrics)\n",
    "        \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "for pred_path in sorted(root_predictions_path.rglob(\"*.npy\")):\n",
    "    csv_path = pred_path.with_suffix(\".csv\")\n",
    "    if csv_path.exists():\n",
    "        print(f\"Skipping {pred_path} as {csv_path.name} already exists\")\n",
    "        continue\n",
    "    \n",
    "    data_module.setup(\"predict\")\n",
    "    predict_dataset = data_module.datasets[\"predict\"]\n",
    "    print(f\"Loading predictions from {pred_path}\")\n",
    "    predictions = np.load(pred_path) # (200, 10, 1006, 590)\n",
    "\n",
    "    visualizations_folder = root_predictions_path / \"visualizations\"\n",
    "    visualizations_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = summary_dataset(predict_dataset, predictions, pred_path.stem, visualizations_folder)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    print(f\"Saved results to {csv_path}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dim view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 1006, 590)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.setup(\"predict\")\n",
    "predict_dataset = data_module.datasets[\"predict\"]\n",
    "labels = np.array([predict_dataset[i][1] for i in range(len(predict_dataset))])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 10, 1006, 590)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(PosixPath('/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_1/sam_vit_b_experiment_1.npy'),\n",
       " (200, 10, 1006, 590))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_path = Path(\"/workspaces/Minerva-Discovery/my_experiments/sam_original/evaluate_experiments/parihaka/tmp/predictions/sam_vit_b_experiment_1/sam_vit_b_experiment_1.npy\")\n",
    "predictions = np.load(pred_path)\n",
    "print(predictions.shape)\n",
    "# predictions = predictions[:, 0:6, :, :]\n",
    "# predictions = predictions.argmax(axis=1)\n",
    "\n",
    "pred_path, predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "\n",
    "def compute_mean_iou_plane(predictions, labels, axis):\n",
    "    \"\"\"\n",
    "    Compute the mean IoU along a given axis.\n",
    "    \n",
    "    Args:\n",
    "        predictions (numpy.ndarray): The predicted labels, shape (I, J, K)\n",
    "        labels (numpy.ndarray): The ground truth labels, shape (I, J, K)\n",
    "        axis (int): Axis to iterate over (0 for JxK, 1 for IxK, 2 for IxJ)\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D plane with mean IoU values\n",
    "    \"\"\"\n",
    "\n",
    "    # Define IoU metric\n",
    "    iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=6, average=None)\n",
    "\n",
    "    # Get shape\n",
    "    I, J, K = predictions.shape\n",
    "\n",
    "    if axis == 0:  # Compute JxK (Iterate over i)\n",
    "        result_shape = (J, K)\n",
    "        iou_map = np.zeros(result_shape)\n",
    "\n",
    "        for j in range(J):\n",
    "            for k in range(K):\n",
    "                pred_trace = predictions[:, j, k]\n",
    "                label_trace = labels[:, j, k]\n",
    "\n",
    "                pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "                label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "                iou_per_class = iou(pred_tensor, label_tensor)\n",
    "                iou_map[j, k] = torch.mean(iou_per_class).item()\n",
    "\n",
    "    elif axis == 1:  # Compute IxK (Iterate over j)\n",
    "        result_shape = (I, K)\n",
    "        iou_map = np.zeros(result_shape)\n",
    "\n",
    "        for i in range(I):\n",
    "            for k in range(K):\n",
    "                pred_trace = predictions[i, :, k]\n",
    "                label_trace = labels[i, :, k]\n",
    "\n",
    "                pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "                label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "                iou_per_class = iou(pred_tensor, label_tensor)\n",
    "                iou_map[i, k] = torch.mean(iou_per_class).item()\n",
    "\n",
    "    elif axis == 2:  # Compute IxJ (Iterate over k)\n",
    "        result_shape = (I, J)\n",
    "        iou_map = np.zeros(result_shape)\n",
    "\n",
    "        for i in range(I):\n",
    "            for j in range(J):\n",
    "                pred_trace = predictions[i, j, :]\n",
    "                label_trace = labels[i, j, :]\n",
    "\n",
    "                pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "                label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "                iou_per_class = iou(pred_tensor, label_tensor)\n",
    "                iou_map[i, j] = torch.mean(iou_per_class).item()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid axis. Choose 0 (JxK), 1 (IxK), or 2 (IxJ).\")\n",
    "\n",
    "    return iou_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_iou_map(\n",
    "    iou_map,\n",
    "    title=\"IoU Map\",\n",
    "    cmap=\"viridis\",\n",
    "    xlabel=\"Dimension 2\",\n",
    "    ylabel=\"Dimension 1\",\n",
    "    figsize=(8, 6),\n",
    "    save_path=None,\n",
    "    show=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the IoU map using Matplotlib with full customization.\n",
    "\n",
    "    Args:\n",
    "        iou_map (numpy.ndarray): 2D array of IoU values.\n",
    "        title (str): Title of the plot.\n",
    "        cmap (str): Colormap to use (e.g., 'viridis', 'plasma', 'jet', etc.).\n",
    "        xlabel (str): Label for the x-axis.\n",
    "        ylabel (str): Label for the y-axis.\n",
    "        figsize (tuple): Size of the figure (width, height).\n",
    "        save_path (str or None): If provided, saves the plot to the given file path.\n",
    "        show (bool): Whether to display the figure.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The generated figure.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    cax = ax.imshow(iou_map, cmap=cmap, interpolation=\"nearest\")\n",
    "    fig.colorbar(cax, label=\"Mean IoU\")\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)  # Prevents displaying if show=False\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i range(len(predictions))\n",
    "plane_jk = compute_mean_iou_plane(predictions, labels, axis=0)\n",
    "print(plane_jk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_iou_map(plane_jk, title=\"Mean IoU (JxK)\", xlabel=\"K\", ylabel=\"J\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ik = compute_mean_iou_plane(predictions, labels, axis=1)\n",
    "print(plane_ik.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_iou_map(plane_ik, title=\"Mean IoU (IxK)\", xlabel=\"K\", ylabel=\"I\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plane_ij = compute_mean_iou_plane(predictions, labels, axis=2)\n",
    "print(plane_ij.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_iou_map(plane_ij, title=\"Mean IoU (IxJ)\", xlabel=\"J\", ylabel=\"I\", cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchmetrics\n",
    "\n",
    "# Define IoU metric\n",
    "iou = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=6, average=None)\n",
    "\n",
    "# Assuming predictions and labels are numpy arrays of shape (I, J, K)\n",
    "I, J, K = predictions.shape\n",
    "\n",
    "# Storage for mean IoU values for each (i, j)\n",
    "iou_map = np.zeros((I, J))\n",
    "\n",
    "# Iterate over each (i, j) trace\n",
    "for i in range(I):\n",
    "    for j in range(J):\n",
    "        # Get the trace along k-axis\n",
    "        pred_trace = predictions[i, j, :]\n",
    "        label_trace = labels[i, j, :]\n",
    "\n",
    "        # Convert to PyTorch tensors and add batch dimension\n",
    "        pred_tensor = torch.from_numpy(pred_trace).unsqueeze(0)\n",
    "        label_tensor = torch.from_numpy(label_trace).unsqueeze(0)\n",
    "\n",
    "        # Compute per-class IoU and take mean\n",
    "        iou_per_class = iou(pred_tensor, label_tensor)\n",
    "        mean_iou = torch.mean(iou_per_class).item()\n",
    "\n",
    "        # Store result\n",
    "        iou_map[i, j] = mean_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.imshow(iou_map, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSVs\n",
    "\n",
    "The cell below will load all the csv files and create a dictionary, where the key is the name of the model, and the value is another dictionary, where the key is the run id and the value is the dataframe with the metrics.\n",
    "\n",
    "The metrics for multiple runs are grouped into a single dataframe, by taking the mean and standard deviation of the metrics. Thus, the final result is a dictionary where key is the model name and value is a dataframe with the mean and standard deviation of the metrics across the multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dfs(\n",
    "    dfs_dict: Dict[str, Dict[str, pd.DataFrame]]\n",
    ") -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Utilitary function to merge multiple DataFrames into a single one.\"\"\"\n",
    "    merged_results = {}\n",
    "\n",
    "    for model_name, runs in dfs_dict.items():\n",
    "        # List to store DataFrames for this model\n",
    "        df_list = [\n",
    "            df.assign(\n",
    "                sample=list(df.index.tolist())\n",
    "            )  # Add a column to track runs (optional)\n",
    "            for i, df in enumerate(runs.values())\n",
    "        ]\n",
    "\n",
    "        # Combine all runs into a single DataFrame\n",
    "        combined_df = pd.concat(df_list)\n",
    "\n",
    "        # Compute mean and std for each column\n",
    "        # (index column is the respective inline/crossline. They are ordered)\n",
    "        mean_df = combined_df.groupby(\"sample\").mean().fillna(0)\n",
    "        std_df = (\n",
    "            combined_df.groupby(\"sample\").std().fillna(0)\n",
    "        )  # Fill NaNs with 0\n",
    "\n",
    "        # Rename standard deviation columns\n",
    "        std_df = std_df.rename(\n",
    "            columns={col: f\"{col} (std)\" for col in std_df.columns}\n",
    "        )\n",
    "\n",
    "        # Merge mean and std dataframes\n",
    "        final_df = pd.concat([mean_df, std_df], axis=1)\n",
    "\n",
    "        # Store in result dictionary\n",
    "        merged_results[model_name] = final_df.reset_index()\n",
    "\n",
    "    return merged_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for p in sorted(root_predictions_path.rglob(\"*.csv\")):\n",
    "    model_name = p.stem\n",
    "    if model_name not in dfs:\n",
    "        dfs[model_name] = {}\n",
    "    dfs[model_name][p.parent.stem] = pd.read_csv(p)\n",
    "\n",
    "\n",
    "dfs = merge_dfs(dfs)\n",
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = {}\n",
    "for i in range(6):\n",
    "    class_percentage = dfs[\"simclr\"][f\"Class percentages {i}\"].mean()\n",
    "    percentages[f\"Class {i}\"] = class_percentage\n",
    "    \n",
    "percentages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_df(dfs: Dict[str, pd.DataFrame], metric_name) -> pd.DataFrame:\n",
    "    metrics = {}\n",
    "    for model_name, df in dfs.items():\n",
    "        results = {\n",
    "            f\"Mean {metric_name}\": df[f\"Mean {metric_name}\"].mean(),\n",
    "            f\"{metric_name} 0\": df[f\"{metric_name} 0\"].mean(),\n",
    "            f\"{metric_name} 1\": df[f\"{metric_name} 1\"].mean(),\n",
    "            f\"{metric_name} 2\": df[f\"{metric_name} 2\"].mean(),\n",
    "            f\"{metric_name} 3\": df[f\"{metric_name} 3\"].mean(),\n",
    "            f\"{metric_name} 4\": df[f\"{metric_name} 4\"].mean(),\n",
    "            f\"{metric_name} 5\": df[f\"{metric_name} 5\"].mean(),\n",
    "            \n",
    "            f\"Mean {metric_name} (std)\": df[f\"Mean {metric_name} (std)\"].mean(),\n",
    "            f\"{metric_name} 0 (std)\": df[f\"{metric_name} 0 (std)\"].mean(),\n",
    "            f\"{metric_name} 1 (std)\": df[f\"{metric_name} 1 (std)\"].mean(),\n",
    "            f\"{metric_name} 2 (std)\": df[f\"{metric_name} 2 (std)\"].mean(),\n",
    "            f\"{metric_name} 3 (std)\": df[f\"{metric_name} 3 (std)\"].mean(),\n",
    "            f\"{metric_name} 4 (std)\": df[f\"{metric_name} 4 (std)\"].mean(),\n",
    "            f\"{metric_name} 5 (std)\": df[f\"{metric_name} 5 (std)\"].mean(),\n",
    "        }\n",
    "        \n",
    "        metrics[model_name] = results\n",
    "        \n",
    "    metrics_df = pd.DataFrame(metrics).T.sort_index()\n",
    "    metrics_df = metrics_df.sort_values(f\"Mean {metric_name}\", ascending=False) * 100\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_df(metrics_df):\n",
    "    non_std_cols = [col for col in metrics_df.columns if \"std\" not in col]\n",
    "    new_metrics_df = metrics_df[non_std_cols].copy()\n",
    "    for col in non_std_cols:\n",
    "        new_metrics_df[col] = metrics_df.apply(\n",
    "            lambda x: f\"{x[col]:.2f}<br>(±{x[col + ' (std)']:.2f})\", axis=1\n",
    "        )\n",
    "        \n",
    "    return new_metrics_df\n",
    "\n",
    "\n",
    "def get_df_without_std(metrics_df):\n",
    "    non_std_cols = [col for col in metrics_df.columns if \"std\" not in col]\n",
    "    return metrics_df[non_std_cols]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = {\n",
    "    \"byol\": \"BYOL<br>(ResNet50)\",\n",
    "    \"deeplabv3\": \"DeepLabV3<br>(ResNet50)\",\n",
    "    \"tribyol\": \"TriBYOL<br>(ResNet50)\",\n",
    "    \"sam\": \"SAM<br>(SAM-ViT-B)\",\n",
    "    \"kenshodense\": \"KenShoDense<br>(ResNet50)\",\n",
    "    \"fastsiam\": \"FastSiam<br>(ResNet50)\",\n",
    "    \"simclr\": \"SimCLR<br>(ResNet50)\",\n",
    "    \"lfr\": \"LFR<br>(ResNet50)\",\n",
    "    \"dinov2_mla\": \"DINOv2-MLA<br>(DinoViT)\",\n",
    "    \"dinov2_mla_reflect_6_classes\": \"DINOv2-MLA - reflect<br>(DinoViT)\",\n",
    "    \"dinov2_mla_interpolate_6_classes\": \"DINOv2-MLA - interpolate<br>(DinoViT)\",\n",
    "    \"dinov2_mla_constant_7_classes\": \"DINOv2-MLA - constant<br>(DinoViT)\",\n",
    "    \"dinov2_dpt\": \"DINOv2-DPT<br>(DinoViT)\",\n",
    "    \"dinov2_pup\": \"DINOx2-PUP<br>(DinoViT)\",\n",
    "    \"sfm_base_patch16\": \"SFM<br>(SFM-ViT-B)\",\n",
    "    \"setr_pup\": \"SETR-PUP<br>(ViT-L)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_heat_map(metrics_df, text_df, colorbar_title: str = None, height: int = 1200, width: int = 800, index_map: dict = None):\n",
    "    df = metrics_df.copy()\n",
    "    if index_map is not None:\n",
    "        df = df.rename(index=index_map)\n",
    "    \n",
    "    fig = go.Figure(\n",
    "        go.Heatmap(\n",
    "            x=df.columns,\n",
    "            y=df.index[::-1],  # Reverse order of y-axis\n",
    "            z=df.values[::-1],  # Reverse order of values\n",
    "            text=text_df.values[::-1],  # Reverse order of text\n",
    "            texttemplate=\"%{text}\",\n",
    "            textfont={\"family\": \"Times New Roman\", \"size\": 15},\n",
    "            colorscale=\"Blues\",\n",
    "            colorbar_title=colorbar_title,  # Changed legend title to \"IoU\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        width=width,\n",
    "        xaxis=dict(\n",
    "            showticklabels=True,  # Keep tick labels but remove title\n",
    "            title=None,  # Remove x-axis title\n",
    "            tickfont=dict(family=\"Times New Roman\", size=16),\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=10, b=10),\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_iou = get_metrics_df(dfs, \"IoU\")\n",
    "metrics_df = get_df_without_std(metrics_df_iou)\n",
    "text_df = get_text_df(metrics_df_iou)\n",
    "\n",
    "metrics_df = metrics_df.drop(index=\"setr_pup\")\n",
    "metrics_df = metrics_df.drop(index=\"dinov2_mla\")\n",
    "text_df = text_df.drop(index=\"setr_pup\")\n",
    "text_df = text_df.drop(index=\"dinov2_mla\")\n",
    "\n",
    "\n",
    "fig = plot_heat_map(metrics_df, text_df, colorbar_title=\"IoU (%)\", height=1000, width=700, index_map=index_map)\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(root_predictions_path / \"iou_heatmap.png\")\n",
    "print(f\"Saved heatmap to {root_predictions_path / 'iou_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df_iou = get_metrics_df(dfs, \"F1 Score\")\n",
    "metrics_df = get_df_without_std(metrics_df_iou)\n",
    "text_df = get_text_df(metrics_df_iou)\n",
    "\n",
    "metrics_df = metrics_df.drop(index=\"setr_pup\")\n",
    "metrics_df = metrics_df.drop(index=\"dinov2_mla\")\n",
    "text_df = text_df.drop(index=\"setr_pup\")\n",
    "text_df = text_df.drop(index=\"dinov2_mla\")\n",
    "\n",
    "\n",
    "\n",
    "fig = plot_heat_map(metrics_df, text_df, colorbar_title=\"F1-Score (%)\", height=1000, width=700, index_map=index_map)\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(root_predictions_path / \"f1_heatmap.png\")\n",
    "print(f\"Saved heatmap to {root_predictions_path / 'f1_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Prediction Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_value_heatmap(\n",
    "    values,\n",
    "    colorscale=\"Blues\",\n",
    "    title=\"Heatmap of Values\",\n",
    "    show_colorbar=True,\n",
    "    width=1000,\n",
    "    height=200,\n",
    "    metric_name=\"IoU\",\n",
    "    filename=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the given values using Plotly.\n",
    "\n",
    "    Parameters:\n",
    "    - values (list or np.array): List of values to visualize.\n",
    "    - colorscale (str): Color scheme for the heatmap (e.g., 'Viridis', 'Plasma', 'Jet').\n",
    "    - title (str): Title of the heatmap.\n",
    "    - show_colorbar (bool): Whether to display the color bar.\n",
    "    - width (int): Width of the plot.\n",
    "    - height (int): Height of the plot.\n",
    "    \"\"\"\n",
    "    # Reshape values for a single-row heatmap\n",
    "    heatmap_values = np.array([values])  # Convert to 2D array\n",
    "\n",
    "    # Create heatmap figure\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=heatmap_values,\n",
    "            colorscale=colorscale,\n",
    "            showscale=show_colorbar,\n",
    "            zmin=0,\n",
    "            zmax=1,\n",
    "            colorbar=dict(\n",
    "                title=metric_name,\n",
    "                tickvals=np.arange(0, 1.2, 0.2),\n",
    "                ticktext=[f\"{i:.1f}\" for i in np.arange(0, 1.2, 0.2)]\n",
    "            ) if show_colorbar else None,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout settings\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "            tickvals=list(range(len(values))),\n",
    "            showticklabels=False,  # Hide tick labels for cleaner look\n",
    "            title=\"Crossline index\",\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            showticklabels=False,  # Hide y-axis labels as it's a single row\n",
    "            title=\"\",\n",
    "        ),\n",
    "        margin=dict(l=10, r=10, t=40, b=10),\n",
    "        title=title,\n",
    "        height=height,\n",
    "        width=width,\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "    if filename:\n",
    "        fig.write_image(filename)\n",
    "        print(f\"Saved heatmap to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in sorted(dfs.items(), key=lambda x: x[0]):\n",
    "    values = df[\"Mean IoU\"].values\n",
    "    plot_value_heatmap(\n",
    "        values,\n",
    "        colorscale=\"Plasma\",\n",
    "        title=f\"IoU for model: {index_map[key].replace('<br>', ' ')} (min: {values.min() * 100:.2f}%, average: {values.mean() * 100:.2f}%, max: {values.max() * 100:.2f}%)\",\n",
    "        width=1000,\n",
    "        height=250,\n",
    "        metric_name=\"IoU\",\n",
    "        filename=root_predictions_path / f\"{key}_iou_heatmap.png\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in sorted(dfs.items(), key=lambda x: x[0]):\n",
    "    values = df[\"Mean F1 Score\"].values\n",
    "    plot_value_heatmap(\n",
    "        values,\n",
    "        colorscale=\"Plasma\",\n",
    "        title=f\"F1-Score for model: {index_map[key].replace('<br>', ' ')} (min: {values.min() * 100:.2f}%, average: {values.mean() * 100:.2f}%, max: {values.max() * 100:.2f}%)\",\n",
    "        width=1000,\n",
    "        height=250,\n",
    "        metric_name=\"F1-Score\",\n",
    "        filename=root_predictions_path / f\"{key}_f1_heatmap.png\",\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
